id	title	slug	time_modified	date	excerpt	status	content
2	About	about	2008-06-17 08:52:28	2007-09-11 11:30:09		publish	<i>Inductio Ex Machina</i> is [Mark Reid][me]'s machine learning research blog.\n\nI'm a [research fellow][me_anu] with the <a href="http://csl.rsise.anu.edu.au/sml" title="SML Group">Statistical Machine Learning</a> group at the [Australian National University](http://anu.edu.au) in Canberra.\n\nCurrently, I'm investigating representations of learning problems and looking at connections between them. The aim is to (eventually) build a conceptual map of machine learning in its many and varied flavours. That's still a long way off yet - I'm still orienting myself and scanning for landmarks. More details can be found at my [academic site](http://rsise.anu.edu.au/~mreid/). \n\nIf you want to keep up with what I'm reading you can check out my [CiteULike](http://www.citeulike.org/user/mdreid) profile or your may want to join the [Statistical Machine Learning](http://www.citeulike.org/groupfunc/3808/home) group I started there.\n\nYou can find out more about me at my [eponymous website](http://mark.reid.name).\n\n[me]: http://mark.reid.name\n[me_anu]: http://users.rsise.anu.edu.au/~mreid
12	Introducing <i>Inductio Ex Machina</i>  	introducing-inductio-ex-machina	2007-09-24 05:06:51	2007-09-22 07:45:48		publish	Welcome to my machine learning research blog.\n\nI'm a newly minted Ph.D. graduate who has taken up a post-doctoral research fellow with the\n[Statistical Machine Learning][sml] group within the [Computer Sciences Laboratory][csl] at\nthe Australian National University.\n\n[sml]: http://csl.rsise.anu.edu.au/sml\n[csl]: http://csl.cecs.anu.edu.au/\n\nThe plan here is to present for discussion any papers, questions or ideas related to machine\nlearning. In particular, I'll probably concentrate on my past and present interests, inclduing:\nstatistical learning theory, dimensional reduction, transfer learning, rule learning and the\nphilosophy of induction.\n\nAs well as being an introduction, this post also plays a second role, which is to claim this blog\nas part of my <a href="http://technorati.com/claim/it4v7jtukb" rel="me">Technorati Profile</a>\nthrough the inclusion of that previous link.
15	Anti-Learning	anti-learning	2007-10-03 03:41:54	2007-10-03 03:03:53		publish	Last week I saw an interesting PhD monitoring [presentation][] by [Justin Bedo][] on the\ncounter-intuitive phenomenon of "anti-learning". For certain datasets, learning a classifier from a small number of samples and inverting its predictions performs much better than the original classifier. Most of the theoretical results Justin mentioned about are discussed in a recent [paper][] and [video lecture][] by [Adam Kowalczyk][]. These build on [earlier work][] presented at ALT 2005. As John notes in his [blog post][] from a couple of years ago, the strangeness of anti-learning is due to our assumption that proximity implies similarity.\n\nThis anti-learning effect has been observed in naturally occurring esophageal adenocarcinoma data: a binary classification problem with nearly 10,000 features. In his talk, Justin presented evidence that the effect was real (by constructing a null hypotheses through repeated shuffling of the data labels) and relatively invariant to choice of learning algorithm.\n\nLike any good scientist, Justin and his colleagues replicated the phenomena in a simpler,\nsynthetic model in order to better understand what might be happening. The model proposed is one that modeling competition between features: if one feature has a large value the others are small but in the opposite direction and examples from different classes have distinct large features pointing in opposite directions. This results in examples from opposite classes being more similar (_i.e._, they have a larger positive inner product) than examples from the same class. At a stretch, this model is also biologically plausible if features are expressions of competing entities in a cell.\n\nThe algorithm proposed to deal with anti-learning uses some of the data available at training\ntime to test whether has anti-learning characteristics and, if so, inverts the resulting\nclassifier. This "burns" some of the information in the training data but can dramatically\nimprove performance when anti-learning is correctly identified.\n\nIt's an interesting example of a trade-off that can be made between data and background\nknowledge. With relatively few examples and the knowledge that you are in an anti-learning situation, you can flip classifications and do very well. As the amount of data available increases, the learning algorithm will converge to a good classifier, the assumption is less valuable and flipping classifications is costly.\n\n[adam kowalczyk]: http://users.rsise.anu.edu.au/~akowalczyk/\n[paper]: http://adamk.antilearning.googlepages.com/ecml07.pdf\n[video lecture]: http://videolectures.net/mlss06au_kowalczyk_al/\n[justin bedo]: http://holly.ath.cx/\n[blog post]: http://hunch.net/?p=35\n[presentation]: http://cecs.anu.edu.au/seminars/showone.pl?SID=523\n[earlier work]: http://www.springerlink.com/content/e3ey7r6yxu68fye6/
37	Visualising ROC and Cost Curve Duality	visualising-roc-and-cost-curve-duality	2008-04-21 05:57:42	2008-04-21 05:57:42	Discussion of the point-line duality between Drummond and Holte's cost curves and ROC curves. An applet is provided to help visualise this relationship. 	publish	I've been looking into the relationships between losses, divergences and other measures of predictors and problems recently and came across a 2006 paper by Drummond and Holte entitled <a href="http://www.cs.ualberta.ca/~holte/Publications/mlj2006.pdf" class="pdf">Cost Curves: An improved method for visualizing classifier performance</a>. This paper describes a representation of classifier performance that is very closely related to the usual ROC curve. However, unlike ROC plots of (False Positive Rate, True Positive Rate)-points for various operating conditions of the classifier cost curves show (cost, risk)-points. That is, for each cost plotted on the x axis, the y co-ordinate shows the cost-weighted loss for the classifier.\n\nAs explained in Drummond and Holte's paper, there is a simple point-line duality between ROC space and Cost-Loss space based on the definition of cost-weighted loss. If [tex](FP,TP)[/tex] is a point in ROC space then the cost-loss relationship [tex](c, L)[/tex] is linear and satisfies\n<center>\n[tex] \\displaymath L = (1-\\pi) c FP + \\pi (1-c) (1 - TP) [/tex] \n</center> \nwhere [tex]c[/tex] is the cost of a false positive and [tex]\\pi[/tex] the prior probability of the positive class[^1]. \n\nGiven a specific [tex]\\pi[/tex] this relationship is completely invertible. A point [tex](c,L)[/tex] in cost-loss space corresponds to the following line in ROC space\n<center>\n[tex]\\displaymath TP = \\frac{(1-\\pi) c}{\\pi(1-c)} FP + \\frac{(1-\\pi) c - L}{\\pi(1-c)}.[/tex]\n</center>\n\nMy ability to intuitively grasp this duality relationship was not that great so I hacked together the following applet to help. On the right is a black curve in ROC space representing five (False Positive, True Positive) rates for some imaginary classifier. The points are (0,0), (0.1, 0.5), (0.3, 0.8), (0.7, 0.95) and (1,1). The diagonal grey line on the ROC plot represents the performance of random classifiers - each increase in True Positive rate is countered by an equivalent decrease in False Positive rate.\n\nThe left plot, entitled "Cost Space" shows the (cost,loss) duals of both the black and grey curves from the right-hand plot. The grey diagonal on the right corresponds to a "tent" on the left that represents the best performance of a classifier that constantly predicts a single class.\n\n<object classid="java:siroc.class" class="applet" style="border: 1px solid gray; margin-left: -50px; padding: 1ex 0;"\n           type="application/x-java-applet" \narchive="/downloads/siroc/siroc.jar,/downloads/siroc/controlP5.jar,/downloads/siroc/batikfont.jar,/downloads/siroc/geomerative.jar,/downloads/siroc/geovex.jar,/downloads/siroc/core.jar" \n           width="700" height="400" standby="Loading Processing software...">\n  <param name="archive" value="/downloads/siroc/siroc.jar,/downloads/siroc/controlP5.jar,/downloads/siroc/batikfont.jar,/downloads/siroc/geomerative.jar,/downloads/siroc/geovex.jar,/downloads/siroc/core.jar" />\n  <param name="mayscript" value="true" />\n  <param name="scriptable" value="true" />\n  <param name="image" value="/downloads/siroc/loading.gif" />\n  <param name="boxmessage" value="Loading Processing software..." />\n  <param name="boxbgcolor" value="#FFFFFF" />\n  <param name="test_string" value="outer" />\n<p>\n<strong>This browser does not have a Java Plug-in.<br />\n<a href="http://java.sun.com/products/plugin/downloads/index.html" title="Download Java Plug-in">Get the latest Java Plug-in here.</a></strong>\n</p>\n</object>\n\nIf you click in the applet area you can get a feel for the relationship between these two representations. When you move your mouse over ROC space you will see the corresponding line in cost space. Conversely, when you move your mouse over the cost space plot you will see the dual line in ROC space.\n\nThe bar at the bottom of the two plots controls the prior probability [tex]\\pi[/tex]. You can see how the dual curve in cost space changes as this parameter is modified.\n\nThe code for this applet is available through [GitHub](http://github.com). The visualisation aspects are written in [Processing](http://processing.org) and are [available here](http://github.com/mreid/siroc/). This relies on some [Java code](http://github.com/mreid/geovex/) I also wrote that does the point-line conversions.\n\n[Chris Drummond](http://www.site.uottawa.ca/~cdrummon/) has also created an [applet](http://www.site.uottawa.ca/~cdrummon/rocconversion.html) to do the same kind of conversion. The one here can be seen as complementary since his version allows the user to add data points and construct curves whereas mine just aims to make the key relationship interactive.   \n\n[^1]: My description here differs slightly from Drummond and Holte's in that I am keeping priors and costs separate and not normalising the loss. 
16	Antihubrisines	antihubrisines	2007-10-03 05:39:36	2007-10-03 05:39:36		publish	In keeping with the "Anti-" theme from my [last post][] I thought I'd share something I found in the treasure trove of [rants][] that [J. Michael Steele][]'s has put on the web for our edification.\n\nAntihubrisines, according to John W. Tukey in his 1986 paper, [Sunset Salvo][], are little pearls of wisdom to keep in mind if you suspect you are being afflicted by hubris. They are to "suffering philosophy" what antihistamines are to suffering sinuses:\n> To statisticians, hubris should mean the kind of pride that fosters \n> and inflated idea of one's powers and thereby keeps one from being \n> more than marginally helpful to others. ... The feeling of "Give me\n> (or more likely even, give my assistant) the data, and I will tell\n> you what the real answer is!" is one we must all fight against again\n> and again, and yet again.\n\nIncluded in Tukey's prescription are number of strains of advice, both qualitative and quantitative. Among my favourites is this very bracing tonic that should be administered whenever you plan to start number crunching:\n> The data may not contain the answer. The combination of some data \n> and an aching desire for an answer does not ensure that a \n> reasonable answer can be extracted from a given body of data.\n\n[last post]: http://conflate.net/inductio/theory/anti-learning\n[rants]: http://www-stat.wharton.upenn.edu/~steele/Rants.htm\n[j. michael steele]: http://www-stat.wharton.upenn.edu/~steele/\n[sunset salvo]: http://www-stat.wharton.upenn.edu/~steele/HoldingPen/SunsetSalvo.pdf
18	The Mathematical Grue	the-mathematical-grue	2007-10-19 22:54:58	2007-10-19 06:57:15		publish	A [discussion over at God Plays Dice][discussion] had me nodding in agreement: proving a theorem is like playing an adventure game. As Isabel puts it\n\n> You are in a maze of twisty little equations, all alike\n\nalluding to a particularly fiendish puzzle in the text adventure [Colossal Cave][].\n\nHaving recently grappled with some tricky proofs I was wondering how they might play out as a piece of interactive fiction...\n\n    You are sitting before a particularly thorny conjecture. \n    Possible proofs lead away from here in several directions.\n    \n    > inventory\n    \n    You are carrying the following items:\n       A ream of blank paper\n       A pencil\n       The Cauchy-Schwarz inequality\n       Some half-remembered undergraduate mathematics\n    \n    > look conjecture\n    \n    You stare blankly at the conjecture. You think it might \n    have something to do with convexity.\n    \n    > w\n\n    You surf over to Wikipedia and read up on sub-tangents. \n    The notation makes you confused.\n    \n    There is a lemma here.\n        \n    > take lemma\n    \n    Taken.\n    \n    > e\n    \n    You wander off to go get a bite to eat and some coffee.\n    \n    You see a colleague here.\n    \n    > talk colleague\n    \n    After explaining your conjecture your colleague mutters \n    that it was probably proven in the 50s by a Russian.\n     \n    > s\n    \n    You sit back down at your desk and spend half an hour \n    reading pages linked to from reddit.\n\n    You see an unproved conjecture here.\n    \n    > use lemma\n    [on the conjecture] \n    \n    With a bit of manipulation you turn the equation into one \n    involving the expectation of a product.\n    \n    > use Cauchy-Schwarz\n    [on the conjecture]\n    \n    Hooray! You now have a tight bound on a key quantity, \n    proving your conjecture.\n    \n    > generalise assumptions\n\n    Your theorem was eaten by a Grue.\n\n[discussion]: http://godplaysdice.blogspot.com/2007/10/you-are-in-maze-of-twisty-little.html\n[colossal cave]: http://en.wikipedia.org/wiki/Colossal_Cave_Adventure#Maze_of_twisty_little_passages
47	Scoring Rules and Prediction Markets		2008-08-11 11:59:12	0000-00-00 00:00:00		draft	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as "proper scoring rules" was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert <i>et al.</i>[^4]\n\nFor a single binary event, a scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function of a "report" [tex]r = [r_0, r_1][/tex] of the probabilities for that event. If you report [tex]r[/tex] and the event occurs you are paid [tex]s_1(r)[/tex]. If the event does not occur you are paid [tex]s_0(r)[/tex]. \n\nA convenient shorthand is to let [tex]w_1[/tex] be a random variable that is 1 if the event occurs and 0 otherwise. Then the payment from the scoring rule for a given report [tex]r[/tex] is the inner product [tex]\\langle s(r), w \\rangle[/tex] where [tex]w = [1-w_1, w_1][/tex]. This is because if [tex]w_1 = 1[/tex] then [tex]w = [0,1][/tex] and so [tex]\\langle s(r), w \\rangle = s_1(r)[/tex] and similarly the inner product is [tex]s_0(r)[/tex] if [tex]w_1 = 0[/tex].\n\nIf you know the scoring rule I use in advance then the game of gradually increasing the cost of the contracts as you buy more can be simplified. Now you just report the probabilities you believe will maximise what I will pay you using the scoring rule. \n\nIn order to ensure you report what you really believe to be the true probabilities I need to construct the scoring rule in such a way that your expected payoff is maximised when you report truthfully. That is, if [tex]p = [1-p_1, p_1][/tex] is the true probability distribution for the event then\n<center>\n[tex]\n\\displaystyle\n\\max_{r} \\mathbb{E}_p  \\langle s(r), w \\rangle  = \\mathbb{E}_p  \\langle s(p), w \\rangle .\n[/tex]\n</center>\nScoring rules that meet this criteria are described as "proper" or "Fisher consistent".\n\nThe reason the inner product notation is a useful shorthand is that, thanks to its linearity, we can now pull the expectation inside it to show that \n<center>\n[tex]\n\\displaystyle\n\\mathbb{E}_p  \\langle s(r), w \\rangle = \\langle s(r), \\mathbb{E}_p  w \\rangle = \\langle s(r), p \\rangle\n[/tex]\n</center>\nsince [tex]\\mathbb{E}_p w = p[/tex]. If everything is suitably differentiable the Fisher consistency (or "properness") condition requires that the derivatives of the scoring rule satisfy, for all [tex]p[/tex],\n<center>\n[tex]\n\\displaystyle\n\\langle \\frac{\\partial}{\\partial r_i} s(p), p \\rangle = 0.\n[/tex]\n</center>\nThat means the derivatives of the scoring rule must be orthogonal to [tex]p[/tex]. \n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
20	Principles of Learning Problem Design	principles-of-learning-problem-design	2007-11-20 05:32:36	2007-11-20 05:32:36		publish	Things have been a little quite around here of late, mainly because I've been working on a submission for the NIPS 2007 Workshop on [Principles of Learning Problem Design][nipsws] in early December.\n\nI'm pleased to say that I'll be presenting some recent results that [Bob][] and [I][me] have been working on under the heading of "Representations in Learning Task Design". The focus is on finding *primitives* and *combinators* for describing learning tasks (Aside: "problems" are what you are trying to solve, "tasks" are what you give to computers to solve them). \n\nUnsurprisingly, [cost-sensitive losses][csl] are one such primitive and when combined using weighted integration they can represent a variety of losses for a range of learning tasks including classification, regression and class probability estimation. \n\nSince this is a workshop paper, most of the results are still fairly preliminary and build on a lot of work by others. That said, I think it's a good approach as several previously known results are subsumed and simplified. I'll post our paper and slides once they are completed.\n\nLet me know if you are attending NIPS and we can try to catch up. Hope to see you at [Whistler][]. \n\n[nipsws]: http://hunch.net/~learning-problem-design/ \n[bob]: http://users.rsise.anu.edu.au/~williams/\n[me]:http://users.rsise.anu.edu.au/~mreid/\n[csl]: http://www-cse.ucsd.edu/users/elkan/rescale.pdf\n[whistler]: http://nips.cc/Conferences/2007/Program/schedule.php?Session=Workshops
21	A Crash Course in Convex Analysis	a-crash-course-in-convex-analysis	2007-12-18 23:20:43	2007-12-18 23:20:43		publish	I've been attempting to read an interesting [NIPS 2007][] paper entitled [Estimating divergence functionals and the likelihood ratio by convex risk minimzation][Nguyen et al 2007] and realised my knowledge of convex analysis was sketchy at best.\n\nFortunately, [Wikipedia][] pointed me to an excellent [summary of the Legendre-Fenchel transformation][LF transform] by [Hugo Touchette][]. A bit more digging around Hugo's site led me to a great [cheat sheet][] for convex analysis, covering many of the concepts that were causing me trouble.\n\nGreat stuff!\n\n[Nguyen et al 2007]: http://books.nips.cc/papers/files/nips20/NIPS2007_0782.pdf\n[Wikipedia]: http://en.wikipedia.org/wiki/Convex_conjugate \n[LF transform]: https://www.maths.qmul.ac.uk/~ht/archive/lfth2.pdf\n[Hugo Touchette]: http://www.maths.qmul.ac.uk/~ht/index.html\n[cheat sheet]: http://www.maths.qmul.ac.uk/~ht/archive/convex1.pdf\n[NIPS 2007]: http://books.nips.cc/nips20.html
22	NIPS 2007 Highlights	nips-2007-highlights	2008-07-17 23:41:43	2007-12-21 14:50:03		private	I attended my first NIPS conference this month and had a great time and was\nable to put faces to many names I've encountered on papers recently.\n\nThe quantity and quality of work was a somewhat overwhelming but I hear from\nmore seasoned NIPS attendees that this is par for the course. What follows are a\nfew topics and presentations that caught my attention during the conference and\nworkshop sessions this year.\n\n\n<!--more-->\n\nThe Conference\n--------------\n\n\nThe Workshops\n-------------\nI attended two of the 19 workshops: \n[Principles of Learning Problem Design][plpd] (where I gave a talk on some of \nthe [representations][] work [Bob][] and I have been working on) and \n[Representations and Inference on Probability Distributions][ripd].\n\n[Bob]: http://axiom.anu.edu.au/~williams/\n[representations]: http://users.rsise.anu.edu.au/~mreid/research/\n[plpd]: http://hunch.net/~learning-problem-design/\n[ripd]: http://nips2007.kyb.tuebingen.mpg.de/\n\nThere was a wide variety of work in the first of these workshops but the one \nthat I found most interesting was the poster that [Steve Hanneke][] talked me \nthrough on [Asymptotic Active Learning][aal]. Previous results show that active \nlearning is not any more powerful than standard supervised learning in terms of\nthe number of labelled samples required to beat a particular error rate with \nconfidence. The new result here is that a slight weakening of the usual PAC\nmodel - only requiring the learner find a good hypothesis rather than requiring \nthat the learner also "know" it is a good hypothesis - leads to sample \ncomplexities that are polylogarithmic (rather than linear) in the reciprocal of\nthe error rate.\n\n[Steve Hanneke]: http://www.cs.cmu.edu/~shanneke/\n[aal]: http://hunch.net/~learning-problem-design/gamma.pdf\n\nIn the Representations and Inference on Probability Distributions workshop there\nwere several talks and posters I found interesting. I really enjoyed both the\ncontent and presentation of [Andrew McGregor][]'s talk on \n[Sketching and Streaming for Distributions][ssfd]\n\n[Andrew McGregor]: http://talk.ucsd.edu/andrewm/\n[ssfd]: http://talk.ucsd.edu/andrewm/slides/07-nipsslides.pdf\n\n* Conjugate Projection Limits: \n  [Peter Orbanz][] has some work looking at how to extend distribution updates \n  followed by projections in infinite dimensional spaces.\n\n[Peter Orbanz]: https://www.ml.inf.ethz.ch/people/phdstudents/porbanz\n\n* Fourier decompositions of distributions over permutations:\n  Very cool group theory-based work to define how you can represent \n  distributions over permutations in terms of group characters.\n\n* Efficiently estimating divergences between distributions:\n  Andrew McGregor gives some results on "sketching" distributions and uses \n  results from communication theory to show that only L1 and L2 divergences\n  can be (space) efficiently learnt from data streams (no revisiting data).\n\n* Frechet derivatives(?):\n  Used in Maya Gupta's talk on extending divergences beyond vectors.\n\nInsights\n--------\nMaya Gupta and colleagues' presentation started from the observation that the\nmean of a set of points is the least square minimiser and others' results \nshowing that Bregman divergences can be seen as the quantity to be minimised\nwhen other summary statistics are used. Using a nice observation that the \ndefinition of a Bregman divergence for F, $d_F(x,y)$ can be seen as the tail of\na Taylor series expansion \n\\[\n\tF(x) = F(y) + \\del F(y)(y - x) + d_F(x,y)\n\\]\nthere are able to generalise divergences to objects other than vectors \n(e.g., functions).\n\nLe Song's presentation on the use of RKHSs to estimate expectations of functions\nfrom samples was neat. The trick here was that the mean of the samples in the\nHilbert space can "encode" information about the higher order moments of the\nsamples in the original space. The expectation of a function with respect to the\nsamples can be written as the inner product of the function and the mean in the\nHilbert space. They show that approximating well the sample means by an estimate \nof the density function in the Hilbert space will give good approximations in the\noriginal space of the expectation (w.r.t. to the sample distribution) of any \nfunction. This differs from the use of moment generating functions in that \ninfinitely many moments of the sample distribution can be estimated \nsimultaneously.
23	A Cute Convexity Result	a-cute-convexity-result	2008-02-04 22:35:54	2008-02-04 04:06:18		publish	Just when I thought I was starting to get my head around the multitudinous uses of convexity in statistics I was thrown by the following definition:\n\n> A function f over the interval (a,b) is convex if, for all choices of {x,y,z} \n> satisfying a < x < y < z < b the determinant \n>\n> <center>[tex] \\displaystyle \\left| \\begin{array}{ccc} 1 & 1 & 1 \\\\ x & y & z \\\\ f(x) & f(y) & f(z) \\end{array}\\right|[/tex]</center>\n>\n> is non-negative.\n\nAfter expanding the determinant and some algebraic twiddling I realised that this is just a very compact way of requiring that\n<center>\n[tex]\\displaystyle\\frac{z-y}{z-x} f(x) + \\frac{y-x}{z-x}f(z) \\geq f(y)[/tex]\n</center> \nwhich, after noticing that (z-y) + (y-x) = (z-x), of course is the more traditional way of saying a function is convex.\n\nWhat's neat about this determinant representation is that it extends nicely to what are known as k<sup>th</sup>-order convex functions (ones whose derivatives up to order k are convex). Specifically, f is k-convex whenever [tex]\\{x_i\\}_{i=0}^{k+1}[/tex] satisfy [tex]a < x_0 < \\ldots < x_{k+1} < b [/tex] and \n<center>\n[tex] \\displaystyle \\left| \n       \\begin{array}{ccc} \n          1    & \\cdots & 1 \\\\ \n          x_0 & \\cdots & x_{k+1} \\\\ \n          x_0^2 & \\cdots & x_{k+1}^2 \\\\\n          \\vdots & \\ddots & \\vdots \\\\\n          x_0^k & \\cdots & x_{k+1}^k \\\\\n          f(x_1) &  \\cdots & f(x_{k+1}) \n     \\end{array} \\right| \\geq 0.[/tex]\n</center>\n\nWhile it is arguably less transparent than explicitly writing out all the convexity inequalities for each of the derivatives of f it certainly makes up for it with compactness.
24	Staying Organised with CiteULike and BibDesk	staying-organised-with-citeulike-and-bibdesk	2008-02-07 06:42:05	2008-02-07 06:42:05		publish	I recently started using [CiteULike][] to keep track of papers I read. For those not familiar with it, it deems itself to be "a free online service to organise your academic papers". In contrast to my offline bibliography organising tool, [BibDesk][], a service like this has at least three main advantages:\n\n* Reduced data entry: If someone else has already entered the article's details or CiteULike can scrape them of a journal's web page I don't have to enter them myself.\n\n* Easy access: The bibliography is stored on the web making access away from my machine or by others straightforward. It's also possible to upload copies of papers for personal use in case you're not able to get to your university's reverse proxy. \n\n* Social networks: When I see that someone else has read a paper I'm interested in I can easily see what else that person has read. I can also look at all the other papers that people have associated with a particular tag. \n\nLike Yaroslav, who also uses CiteULike as part of his [larger strategy][yaroslav] for staying organised, I have started using the Notes field for entries to keep track of important theorems, equations and cross-references of papers that I go over more than once. \n\nOf course, once you've collected a bunch of papers you can also export your bibliography as BibTeX or RIS so your can include citations in your papers. This is especially convenient with BibDesk. All I do is open a "New External File Group" in BibDesk and point it to the URL for my CiteULike account: `http://www.citeulike.org/bibtex/user/mdreid`. BibDesk keeps track of which external entries have or haven't been imported into your offline BibTeX file making it easy to quickly build a conference specific bibliography.\n\nI find this BibDesk and CiteULike combination the best of both worlds as it reduces the amount of data entry I need to do while still making it easy to push citations to [TextMate][] or [LyX][] when I'm writing.\n\n[citeulike]: http://citeulike.org/user/mdreid\n[bibdesk]: http://bibdesk.sourceforge.net/\n[yaroslav]: http://yaroslavvb.blogspot.com/2008/02/strategies-for-organizing-literature.html\n[textmate]: http://macromates.com/\n[lyx]: http://www.lyx.org/
25	Clarity and Mathematics	clarity-and-mathematics	2008-03-06 23:06:21	2008-02-19 10:36:34		publish	John Langford has diagnosed a [complexity illness][] that afflicts research in academia. One of its symptoms is what he calls "Math++": the use of unnecessary and obfuscatory mathematics to improve the author's chance of publication. \n\nHaving recently ploughed through a large number of math-heavy articles during the preparation of a [COLT][] paper I have started to worry whether the illness is contagious. At present there is a rash of awkward notation breaking out in some sections of my draft. While I don't think I can completely get rid of it I'm hoping that I can at least tidy it up and turn it into something presentable.\n\nWanting to tidy up awkward mathematical expression is definitely not the same as wanting removing it completely. To switch [analogies][], maths is akin to a communications channel. The aim of the encoder is to cram information down the line so it can be decoded a the other end. Good mathematical notation encodes frequently occurring concepts with short, memorable terms and takes advantage of uses relationships between concepts. Using a side-channel -- e.g., the English text of the paper -- to ease the burden of decoding is also a good strategy.\n\nJohn also suggests treating Math++ (and other forms of complexity) with education. This doesn't necessarily mean give a lecture on your research but any attempt at communication. I've found that attempting to describe what I'm working on over lunch - and without a whiteboard - can be a good way to focus on the story of your research rather than the technicalities. I find technical details of a paper much easier to understand when I understand  their motivation.\n\nEven if I don't completely cure my paper of Math++, I take some solace from Fernano Pereira who [points out][pereira] that research is a form of dialogue and that dialogue is inherently messy which is sometimes the reason mathematical exposition is less than perfect. It's only through repeated attempts to communicate ideas that one is able to figure out what is important.\n\n[pereira]: http://earningmyturns.blogspot.com/2008/02/complexity-illness.html\n[complexity illness]: http://hunch.net/?p=316\n[COLT]: http://www.learningtheory.org/\n[analogies]: http://apperceptual.wordpress.com/2007/12/20/readings-in-analogy-making/\n 
26	A Meta-index of Data Sets	a-meta-index-of-data-sets	2008-02-22 04:43:37	2008-02-22 04:40:00		publish	I had to go hunting around for some data to try some new ideas on recently.\nAs [handy][google results] as Google is, there's still a fair bit of \nchaff from which to sort the wheat.  \n\n[google results]: http://google.com/search?q=machine+learning+data+sets\n\nFortunately, there is a lot of good stuff out there including well-organised\nindexes of data sets for various purposes. For my future reference (and for\nanyone else that may be interested) here are some of the better data set lists\nI found.\n\n*\t**UCI Repositories**:\n\tNo list of lists would be complete without this perennial [collection][uci]\n\tof machine learning data sets hosted by the University of California, \n\tIrvine. They also have a [repository of large data sets][kdd] for \n\tknowledge discovery in databases (KDD).\n\n[kdd]: http://kdd.ics.uci.edu/\n[uci]: http://archive.ics.uci.edu/ml/\n\n*\t**The Info**: \n\tThis [site][theinfo] "for people with large data sets" has a community\n\teditable [list of data sets][theinfo data] organised by topic. The \n\tcollection here has a web/text focus.\n\n[theinfo]: http://theinfo.org\n[theinfo data]: http://theinfo.org/get/data\n\n*\t**Text Retrieval**:\n\tThis [list][trec] kept by NIST has data sets for each of the various\n\ttracks at the Text Retrieval Conference, including data sets for \n\t[spam detection](http://trec.nist.gov/data/spam.html),\n\t[genomics](http://trec.nist.gov/data/genomics.html),\n\tand a [terabyte](http://trec.nist.gov/data/terabyte.html) track\n\t(although the data sets aren't quite up to a terabyte yet).\n\n[trec]: http://trec.nist.gov/data.html\n\n*\t**Time Series Data Library**:\n\tThis [collection][tsdl] has a large number of time varying data sets from\n\tfinance, demography, physics, sport and ecology. \n\n[tsdl]: http://www-personal.buseco.monash.edu.au/~hyndman/TSDL/\n\n*\t**DMOZ Directory of Data Sets**:\n\tThis is a good [starting point][dmoz] for more lists of data sets for \n\tmachine learning.\n\t\n\tParts of DMOZ itself are [available in RDF][dmoz data] as a data set for \n\tresearchers. There is also a [processed version][dmoz processed] made\n\tavailable as part of the PASCAL [Ontology Learning Challenge][].\n\n[dmoz]: http://www.dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Datasets/\n[dmoz data]: http://rdf.dmoz.org/\n[dmoz processed]: http://olc.ijs.si/dmozReadme.html\n[Ontology Learning Challenge]: http://olc.ijs.si/\n\n*\t**Royal Statistical Society**:\n\tThis [collection][rss data] contains data sets used in research published in \n\tthe [journal of the Royal Statistical Society][rss]. This is an admirable\n\tidea that I wish more journals would take up.\n\n[rss data]: http://www.blackwellpublishing.com/rss/ \n[rss]: http://www.rss.org.uk/ \n\nAs well as the above institution or community organised lists, I also came \nacross some maintained by individuals.\n\n*\t**Daniel Lemire**: \n\tDaniel Lemire's "[Data for Database Research][lemire]" is organised by \n\tapplication areas, including data for earthquakes, weather, finance, climate \n\tand blogs.\n\n[lemire]:  http://www.daniel-lemire.com/blog/data-for-data-mining/\n\n*\t**Peter Skomoroch**:\n\tThe [list of data sets][skomoroch] over at [Data Wrangling][] is similar\n\tin spirit to the one here.\n\n[skomoroch]: http://www.datawrangling.com/some-datasets-available-on-the-web.html\n[Data Wrangling]: http://www.datawrangling.com/\n\nA few specific data sets caught my eye, some new, and some I just hadn't seen \nbefore.\n\n*\t**Freebase Wikipedia Extraction**:\n\tThe [Wikipedia WEX][wex] data set is \n\tessentially a large (57 GB) graph of articles from wikipedia. \n\n[wex]: http://download.freebase.com/wex/\n\t\n*\t**Enron Email**:\n\tThis [collection of email][enron] (400 Mb compressed) between Enron staff \n\tcontains about half a million messages organised into folders. It was\n\treleased publicly as part of the investigation into Enron and has been\n\tused by William Cohen and others as part of the CALO project.\n\n[enron]: http://www.cs.cmu.edu/~enron/\n\n*\t**Freeway Traffic Analysis**:\n\tThis fairly [large data set][freeway] is a record of traffic flow on\n\tseveral lanes of the I-880 freeway in California in order to study the\n\teffect of roving tow-trucks on dealing with decongesting traffic \n\tincidents.\n\n[freeway]: http://ipa.eecs.berkeley.edu/~pettyk/FSP/\n\nIf all else fails and you still cannot find a suitable data set for your \nresearch, you can always invoke the social web and trawl through bookmarks\non services like [del.icio.us](http://del.icio.us). The global \n[data set tag][global tag] can throw up some interesting hits occasionally but\nthere might be a higher wheat to chaff ratio in particular user's bookmarks,\nsuch as [Peter Skomoroch][skomoroch tag]. [Mine][] is not nearly as \ncomprehensive yet.\n\n[global tag]: http://del.icio.us/tag/dataset\n[skomoroch tag]: http://del.icio.us/pskomoroch/dataset\n[mine]: http://del.icio.us/mreid/dataset\n\nIt would be interesting to do a meta-analysis of all these data sets to see how\nour ability as a discipline to deal with larger and more complex data sets has\nincreased over time. As Daniel Lemire pointed out with some surprise recently,\n[processing a terabyte of data][small terabyte] isn't that uncommon.\n\n[small terabyte]: http://www.daniel-lemire.com/blog/archives/2008/02/21/when-a-terabyte-is-small/
27	JMLR Discussion On Boosting	jmlr-discussion-on-boosting	2008-03-03 06:50:47	2008-03-03 06:50:47		publish	The upcoming [Volume 9][v9] of the [Journal of Machine Learning Research][jmlr] is dedicated a chunk of its pages to a paper entitled "[Evidence Contrary to the Statistical View of Boosting][mease08a]" by David Mease and Abraham Wyner. Following this is a number of responses by heavyweights including [boosting][]'s earliest proponents, Freund and Schapire, as well as Mease and Wyner's [rejoinder][mease08b] to the responses. The whole conversation is also available in a [single PDF][].\n\nI've seen this format of argument, response and rejoinder a couple of times before in the statistical literature and I think it works really well. It brings the wealth of expert views that are usually found only at workshop or conference panel discussions but adds the benefits of written expression: careful thinking, less time pressure and access to reference material. \n\nI'm familiar with [AdaBoost][] but haven't really kept up with the recent research surrounding it. It seems that the crux of the discussion is regarding some of the widely held beliefs about the statistical interpretation of boosting (stumps are better than small trees as weak learners, LogitBoost is better than AdaBoost on noisy data). Simple experiments are described which, often surprisingly, contradict the prevailing wisdom. \n\nAlthough I have only had time to skim the entire discussion, one thing I've found really impressive about the contrary evidence Mease and Wyner provide is that all the R code for the experiments is [available][r code]. As can be seen in the subsequent discussion, this provides the responders with concrete reference points and several use them to refine or debate some of the interpretations. This is a perfect example of putting science back into Herbert Simon's [Science of the Artificial][sota], in which he argues that\n> Even when the programs themselves are only moderately large and intricate ... \n> too little is known about their task environments to permit accurate prediction of\n> how well they will perform. ... Here again theoretical analysis must be \n> accompanied by large amounts of \n> experimental work.\n\nNow that I'm back in the world of academic research, it's high time I revisited some of these foundational algorithms in machine learning. I'm hoping that by reading this discussion on boosting and playing with the experiments I can quickly get up to speed with the area.\n\n[sota]: http://www.librarything.com/work/253126\n[r code]: http://www.davemease.com/contraryevidence/\n[boosting]: http://www.boosting.org/\n[v9]: http://jmlr.csail.mit.edu/papers/v9/\n[jmlr]: http://jmlr.org/\n[mease08a]: http://www.jmlr.org/papers/volume9/mease08a/mease08a.pdf\n[mease08b]: http://www.jmlr.org/papers/volume9/mease08b/mease08b.pdf\n[single pdf]:http://www.jmlr.org/papers/volume9/mease08a/mease08a_with_discussion.pdf\n[adaboost]: http://en.wikipedia.org/wiki/AdaBoost
28	Feed Bag: A Simple RSS Archiver	feed-bag-a-simple-rss-archiver	2008-03-13 05:33:07	2008-03-13 05:33:07		publish	One thing my [recent survey of freely available data sets][data] did not uncover was a collection of archived RSS feeds. This surprised me a little since I would imagine aggregators like [Bloglines](http://bloglines.com/), [Google Reader](http://google.com/reader) and [AideRSS](http://aiderss.com/) must have large databases of hundreds of thousands of RSS feeds.\n\n[data]: http://conflate.net/inductio/application/a-meta-index-of-data-sets/\n\nHaving seen how easy it is to [create an RSS aggregator in ruby][igvita], I figured it should be just as easy to collect feeds in the same way and write them to a database via one of the many ORM (Object-Relational Mapping) layers available in ruby. The excellent [FeedNormalizer][] library makes the first part trivial and avoids having to worry whether a feed is RSS1, RSS2, Atom, etc. For the second part I thought I'd try something new and give the ORM library [Sequel][] a go and, in the interests of simplicity, have it talk to an [SQLite][] database.\n\n[igvita]: http://www.igvita.com/2007/03/22/agile-rss-aggregator-in-ruby/\n[feednormalizer]: http://code.google.com/p/feed-normalizer/\n[sequel]: http://code.google.com/p/ruby-sequel/\n[sqlite]: http://www.sqlite.org/\n\nThe part I liked most was how easy Sequel makes setting up database schema. This is the executable ruby code that defines the two tables I use in Feed Bag:\n\n    class Feed < Sequel::Model(:feeds)\n      set_schema do\n        primary_key   :id\n        text          :name\n        text          :url\n        time          :last_checked\n        time          :created\n      end\n    end\n    \n    class Entry < Sequel::Model(:entries)\n      set_schema do\n        primary_key   :id\n        text          :url\n        text          :title\n        text          :content\n        text          :description\n        time          :time\n    \n        foreign_key   :feed_id, :table => :feeds\n        index         :url\n      end\n    end\n\nUsing it is just as easy. From the ruby-side, if you have a feed `f` you get its associated entries using `f.entries` and once you have an entry `e` you can get its URL or title using `e.url` or `e.title`. Given how easy that is, there's little reason to resort to flat text file formats such as CSV when dealing with this sort of data.\n\nI've called the resulting ruby script "Feed Bag" and have [made it available][feedbag] on my academic website along with instructions for using it. Without comments, the scripts weighs in at about 130 lines of code and only took a few hours to write and debug, most of which was learning how to use FeedNormalizer and Sequel. \n\nI've been running Feed Bag on my machine since mid-January, collecting international news feeds from the BBC, New York Times, Washington Post, and 7 others without any trouble. So far it's collected over 25,000 feed items and stashed them in a 38Mb SQLite database. If any one is interested, I've made a bzip2 compressed version of an SQL dump of the database available for [download][] (3.4Mb). \n\nPlease let me know if you use the data for anything, or if you use Feed Bag to collect your own data set.\n\n[feedbag]: http://users.rsise.anu.edu.au/~mreid/code/feed_bag.html \n[download]: http://users.rsise.anu.edu.au/~mreid/files/data/IntlNews.sql.bz2
29	ROC Curves for Machine Learning	roc-curves-for-ml	2008-04-08 11:30:43	0000-00-00 00:00:00		draft	I've recently been attempting to understand Receiver Operating Characteristic (ROC) curves and their relationship to loss, information and divergences. I've decided that the best way to understand this stuff is to attempt to explain it. So, in the spirit of [John Armstrong][]'s expository posts on category theory and integration, as well as Mark Chu-Carroll's "[Basics][]" series at Good Math, Bad Math, I plan to write a series of post explaining some  facts about ROC curves.\n\n[John Armstrong]: http://unapologetic.wordpress.com/\n[Basics]: http://scienceblogs.com/goodmath/goodmath/basics/\n\nThere are already plenty of good introductions and tutorials on ROC curves on the web but they tend to be from a medical diagnosis perspective. I'll try to focus on the material that is relevant to machine learning including the use of ROC analysis in classification, probability estimation and ranking. My aim is to provide a reasonably self-contained set of posts that emphasise some of the more important and recent properties and relationships regarding ROC curves. \n\nPostmodern Classification\n-----------------------------\nSuppose you wanted some way of deciding whether a particular book was [postmodern][] or not. \nWhat you're after is some procedure that takes in a whole lot of details about a particular text and returns a "yes" or "no" answer to the question "Is this text postmodern?"\n\nWe can think about this type of procedure abstractly as a function [tex]r[/tex] from a set [tex]\\mathcal{X}[/tex] of _observations_ about books to the set [tex]\\{0,1\\}[/tex] of _labels_ where 1 indicates membership in the positive class (i.e., the book is postmodern) and 0 indicates non-membership. \n\nROC graphs give us a way of visually assessing sets of binary _classifiers_. These are functions that assign one of two labels to each observations [tex]x[/tex] in the set [tex]\\mathcal{X}[/tex]. We'll use 1 for the positive label and 0 for the negative label so that a classifier is a function\n[tex]\n   r : \\mathcal{X} \\to \\{0,1\\}.\n[/tex]\nFor example, each observation in [tex]\\mathcal{X}[/tex] provides details of a particular text (book, film, TV show, _etc_.) and the classifier returns a 1 to indicate the text is [postmodern][] and returns 0 otherwise.\n\n[postmodern]: http://www-stat.wharton.upenn.edu/~steele/Resources/Writing%20Projects/PostmodernStatistics.htm\n\n\n
30	Algorithms, Programs and Similarity		2008-04-20 08:09:43	0000-00-00 00:00:00		draft	Via [God Plays Dice](http://godplaysdice.blogspot.com/2008/04/smales-problems.html): In Smale's discussion of the Poincaré conjecture, after pointing out that a big part of the importance of the Poincaré conjecture is that it helped make manifolds respectable objects to study in their own right,he states:\n\n> I hold the conviction that there is a comparable phenomenon today in the notion of a "polynomial\n> time algorithm". Algorithms are becoming worthy of analysis in their own right, not merely as a\n> means to solve other problems. Thus I am suggesting that as the study of the set of solutions of an\n> equation (e.g. a manifold) played such an important role in 20th century mathematics, the study of\n> finding the solutions (e.g. an algorithm) may play an equally important role in the next century.\n\nIn light of [this paper][equal algs], it seems there are difficulties in defining what "finding the solution" might mean. The authors restrict their attention to non-interactive, small-step algorithms and argue that even in this restricted setting it is difficult to define what "equivalent" might mean.\n\nAn interesting example (Example 8) from that paper concerns the composition of two functions _f_ and _g_, implemented by programs _P_ and _Q_. Both programs take strings as input and output strings in linear time and logarithmic space. One definition of composition (call Q then pass its output to P as input) takes linear space (since the output of Q is created and stored) and logarithmic time, while another definition (essentially a type of lazy evaluation) takes logarithmic space (only compute the output of Q a character at a time and don't store it) and quadratic time.\n\nI don't think this is as big an issue as the authors make it: it's well known that time and space complexity can be traded off so defining equivalence of programs with respect to input-output behaviour *and* complexity is too fine grained. \n\n[equal algs]: http://research.microsoft.com/research/pubs/view.aspx?type=Technical%20Report&id=1434\n\nThe authors finish with a few analogies: one saying that clustering without a decision function is similarly ill-defined, another that "equivalence of ideas" is an even more difficult beast to grasp but is somehow part of the answer to the question "when are two algorithms different?". When they express different ideas?\n\nThe question is a good one but there does not seem to be any easy resolution.\n\nWhat does this have to do with induction? As Goodman argued, "similarity without respects is a quack, an impostor and a fraud" since you cannot reasonable define similarity without providing some accepted terms of reference or _respects_. Certain "natural" types of similarity are natural only because of convention or human perception (colours, faces, shapes). I don't hold out hope for there being a "natural", universal sense of similarity for something as abstract as algorithms. That's not to say we can't build consensus piece by piece.\n\nPeter Turney's readings in analogy: http://apperceptual.wordpress.com/2007/12/20/readings-in-analogy-making/\n\nImplications for patents on algorithms...
31	Private	private	2008-04-08 00:43:31	2008-04-08 00:43:31		private	This page is a collection of notes and sub-pages for things I'm not yet ready to show the world.\n\n\n
32	Questions	questions	2008-04-08 01:28:55	2008-04-08 00:51:47		private	Some research questions to expand on:\n\n### ROC and Families of Tasks\n\nWe know that a class probability estimation task is equivalent to a family of cost-sensitive classification tasks. How is this related to ROC curves? Given a probability estimator, applying a threshold gives a family of classifiers and an ROC curve.\n\n### Maximal AUC and Divergences\n\nThe maximal AUC obtainable for a given learning problem measures the separation of the label-conditional distributions, much like an f-divergence. However, it seems that maximal AUC and f-divergences are different in the sense that there is no _f_ such that maximal AUC is an f-divergence for that _f_. \n\nWhat kind of divergence is maximal AUC? Is it "special" in some way or is there a whole class of them? Fawcett's "[ROC Graphs with Instance-Varying Costs][rociv]" (2006) and Xie and Priebe's "[A Weighted Generalization of the Mann-Whitney-Wilcoxon Statistic][gmww]" both suggest (equivalent, I think) families based on transforming the axes of the ROC graph.\n\n[rociv]: http://www.citeulike.org/user/mdreid/article/2614937\n[gmww]: http://www.citeulike.org/user/mdreid/article/2639710\n\n### Clustering\n\nClustering seems to be a more primitive problem than classification or class probability estimation. In some sense, clustering takes a single distribution over observations and transforms it into a collection of label-conditional distributions. In this sense, clustering is considering the collection of functions _F_ from observations to some discrete set of finite cardinality.\n\nPapadimitriou argues in his "[Algorithms, Games and the Internet][agi]" (STOC 2001), clustering is not really well-defined:\n\n> There are far too many criteria for the goodness of a clustering . . . and far too little guidance\n> about choosing among them. . . . The criterion for choosing the best clustering scheme cannot be\n> determined unless the decision-making framework that drives it is made explicit\n\nThis would suggest that understanding the collection _F_ must be done in concert with some other family _G_ from labels output by functions in _F_ to predictions in some other set. The structure of _G_ and the type of loss incurred will determine how best to choose an element from _F_, (cf. Baxter's "Learning to Learn" and the choice of bias).\n\n[agi]: http://www.citeulike.org/user/mdreid/article/326513
33	The Earth Is Round (p < 0.05)	the-earth-is-round	2008-04-09 06:36:13	2008-04-09 06:36:13		publish	I love finding old essays on statistics. The philosophical and methodological wars that rage within that discipline make for fun reading. Particularly enjoyable are those essays - inevitably written by older, well-respected researchers - who make a strong point with beautiful rhetorical flourish and no small amount of barbed humour.\n\nThe title of a [journal article][teir] ([PDF][teirpdf]) by Jacob Cohen (and this post) is a classic example. As you may have guessed, it's main aim is to rant against the misuse of p-values for null hypothesis significance testing (NHST). As well as including some extremely amusing quotes by Cohen and others, the paper does a fantastic job of curing the reader of any doubt regarding the correct interpretation of p-values. \n\n[teir]: http://www.citeulike.org/user/mdreid/article/2643653\n[teirpdf]: http://web.math.umt.edu/wilson/Math444/Handouts/Cohen94_earth%20is%20round.pdf\n\nRepeat after me: "the p-value is NOT the probability the null hypothesis is true given the observed data". Or, as Cohen puts it:\n> What's wrong with NHST? Well, among many other things, it does not tell us what we want to\n> know, and we so much want to know what we want to know that, out of desperation, we\n> nevertheless believe that it does! What we want to know is "Given these data, what is the \n> probability that H<sub>0</sub> is true?" But as most of us know, what it tells us is "Given \n> that H<sub>0</sub> is true, what is the probability of these (or more extreme) data?" \n> These are not the same...\n\nMany people make this mistake as it's so easy to erroneously reason about conditional probabilities. The particular fallacy that occurs when p-values are interpreted as the probability the null hypothesis is true is assuming that Prob(H<sub>0</sub>|Data) = Prob(Data|H<sub>0</sub>). Cohen argues that we are confused by the intuitive appeal of reasoning with rare events as though they were impossible events. He highlights why this intuition can led us astray with a wonderful example. A low p-value is erroneously reasoned with as follows:\n> If the null hypothesis is correct, then these data are highly unlikely.    \n>     These data have occurred.    \n>     Therefore, the null hypothesis is highly unlikely.    \n\nThis seems, at first glance, to be analogous to the non-probabilistic syllogism (namely _modus tollens_):\n> If a person is a Martian, then he is not a member of Congress.    \n> This person is a member of Congress.     \n> Therefore, he is not a Martian.     \n\nAbsolutely nothing wrong with that. It's watertight. Now see what happens when the first line becomes a statement with very high probability instead of strictly true:\n> If a person is an American, then he is probably not a member of Congress. (TRUE, RIGHT?)     \n> This person is a member of Congress.     \n> Therefore, he is probably not an American.\n\nOuch! That last deduction should have made your eyes water. This is exactly what is going wrong when people misinterpret p-values. It's what you get for using Bayes' rule without knowing something more _unconditionally_ about the probability of being a member of Congress. This is a rare event and its rarity must be factored in when doing the probabilistic equivalent of implication. Similarly, without knowing anything about the prior probability of the null hypothesis you cannot say anything about its posterior probability.\n\nCohen nicely sums up the danger of treating deduction and induction analogously with a quote attributed to Morris Raphael Cohen:\n> All logic texts are divided into two parts. In the first part, on deductive logic, the fallacies are explained; in the second part, on inductive logic, they are committed.\n\n\n
36	Archives	archives	2008-04-10 03:43:11	2008-04-10 03:39:14		publish	Posts placed here for posterity. Sliced and diced for your convenience.
38	Research-Changing Books	research-changing-books	2008-06-14 13:25:29	2008-05-26 06:45:44	In response to a post by Peter Turney, I list the books I feel shaped my research career.	publish	A [recent post by Peter Turney][turney] lists the books that have influenced his research. As well as compiling a great list of books that are now on my mental "must read one day" list, he makes a crucial point for compiling such a list:\n> If a reader cannot point to some tangible outcome from reading a book, \n> then the reader may be overestimating the personal impact of the book. \n\n[turney]: http://apperceptual.wordpress.com/2008/05/25/the-book-that-changed-my-life/\n\nWith that in mind I tried to think of which books had a substantial impact on my research career.\n\nAlthough I can barely remember any of it now, the [manual][vic20] that came with the Commodore Vic 20 computer I read when I was around seven got me hooked on programming. In primary and secondary school it was that book and the subsequent Commodore 64 and Amiga manuals that set me on the road to studying computer science and maths.\n\n[vic20]: http://www.geocities.com/rmelick/prg.txt\n\nIn my second year at university I had the great fortune of being recommended Hofstadter's "[Gödel, Escher, Bach][geb]" by a fellow student. It is centrally responsible for getting me to start thinking about thinking and subsequently doing a PhD in machine learning. The fanciful but extremely well written detours into everything from genetics to Zen Buddhism also broadened my horizons immensely.\n\n[geb]: http://www.librarything.com/work/5619/book/12512722\n\nI. J. Good's "[The Estimation of Probabilities][good]" was the tiny 1965 monograph I bought second-hand for $2 that made my thesis take a huge change in direction by giving it a Bayesian flavour. I now realise that a lot of that work had since been superseded by much more sophisticated Bayesian methods but sometimes finding a theory before it has been over-polished means that there is much more expository writing to aid intuition. It also helps that Good is a fabulous technical writer. \n\n[good]: http://www.librarything.com/work/2542774/book/12420041\n\nPhilosophically, Nelson Goodman's "[Fact, Fiction and Forecast][fff]" also shaped my thinking about induction quite a lot. His ideas on the "virtuous circle" of basing current induction on the successes and failures of the past provided me with a philosophical basis for the transfer learning aspects of my research. I found his views a refreshing alternative to Popper's (also personally influential) take on induction in "[The Logic of Scientific Discovery][losd]". Whereas Popper beautifully characterises the line between metaphysical and scientific theories, Goodman tries to give an explanation of *how* we might practically come up with new theories in the first place given that there will be, in general, countless that adequately fit the available data. In a nutshell, his theory of "entrenchment" says that we accrete a network of terms by induction and use these terms as features for future induction depending on how successful they were when used in past inductive leaps. This is a view of induction inline with Hume's "habits of the mind" and one I find quite satisfying.\n\n[fff]: http://www.librarything.com/work/70761/book/12419989\n[losd]: http://www.librarything.com/work/68144/book/31001290\n\nWhile not directly related to machine learning or computer science, there are a few other books that helped me form opinions on the process of research in general. I read Scott's "[Write to the Point][wttp]" over a decade ago now but it still makes me stop, look at my writing and simplify it. My attitude to presenting technical ideas was also greatly influenced by reading Feynman's "[QED][]" lectures. They are a perfect example of communicating extremely deep and difficult ideas to a non-technical audience without condescension and misrepresentation. Finally, I read Kennedy's "[Academic Duty][ad]" just as I started my current post-doc and found it immensely insightful. I plan to reread it as I (hopefully) hit various milestone's in my academic career.\n\n[wttp]: http://www.librarything.com/work/1093218/book/31001976\n[qed]: http://www.librarything.com/work/27937/book/12512712\n[ad]: http://www.librarything.com/work/252530/book/20392830 \n\nOf course, like Peter, there are innumerable other books, papers and web pages that have shaped my thinking but the ones above are the ones that leap to mind when I think about how my research interests have developed over time.
39	Mark Reid	me_shorthairdark_bw	2008-06-02 07:55:56	2008-06-02 07:55:56	Your Host 	inherit	Photo of yours truly. Someone should really get better lighting in here...
40	Visualising 19th Century Reading in Australia	visualising-reading	2008-12-09 12:05:57	2008-06-17 03:10:34	A description of a visualisation of some 19th century Australian borrowing records from the Australian Common Readers Project.	publish	****\n_Update - 9 Dec 2008_: Julieanne and I presented a much improved version of this visualisation at the [Resourceful Reading][] conference held at the University of Sydney on the 5th of December. Those looking for the application I presented there: stay tuned, I will post the updated version here shortly.\n****\n[Resourceful Reading]: http://conferences.arts.usyd.edu.au/index.php?cf=20\n\nI've recently spent a bit of time collaborating with my wife on a research project. Research collaboration by couples is not new but given that Julieanne is a [lecturer in the English program][j] and I'm part of the [computer sciences laboratory][csl], this piece of joint research is a little unusual. \n\nThe rest of this post describes the intersection of our interests --- data from the Australian Common Reader Project --- and the visualisation tool I wrote to explore it. The tool itself is based on a simple application of linear Principal Component Analysis (PCA). I'll attempt to explain it here in such a way that readers who have not studied this technique might still be able to make use of the tool.\n\n[j]: http://cass.anu.edu.au/humanities/school_sites/staff.php\n[csl]: http://csl.cecs.anu.edu.au/\n\nThe Australian Common Reader Project\n--------------------------------------------\nOne of Julieanne's research interests is the Australian audience of the late 19th and early 20th centuries. As part of her PhD, she made use of an amazing database that is part of the [Australian Common Reader Project][acrp] --- a project that has collected and entered library borrowing records from Australian libraries along with annotations about when books were borrowed, their genres, borrower occupations, author information, <i>etc</i>. This sort of information makes it possible for Australian literature and cultural studies academics to ask empirical questions about Australian readers' relationship with books and periodicals. \n\n[acrp]: http://www.api-network.com/hosted/acrp/\n\nEver on the lookout for [interesting data sets][meta-index], I suggested that we apply some basic data analysis tools to the database to see what kind of relationships between books and borrowers we might find. When asked if we could have access to the database, [Tim Dolin][] graciously agreed and enlisted [Jason Ensor][] to help with our technical questions. \n\n[meta-index]: http://conflate.net/inductio/2008/02/a-meta-index-of-data-sets/\n[tim dolin]: http://www.humanities.curtin.edu.au/staff.cfm/t.dolin\n[jason ensor]: http://www.humanities.curtin.edu.au/staff.cfm/j.ensor \n\nBooks and Borrowers\n------------------------\nAfter an initial inspection, my first thought was to try to visualise the similarity of the books in the database as measured by the number of borrowers they have in common. \nThe full database contains 99,692 loans of 7,078 different books from 11 libraries by one of the 2,642 people. To make this more manageable, I focused on books that had at least 20 different borrowers and only considered people who had borrowed one of these books.\nThis distilled the database down to a simple table with each row representing one of 1,616 books and each column representing one of 2,473 people. \n\n<table>\n<caption>Table 1: A portion of the book and borrower table. A 1 indicates that the borrower (column)\nborrowed the book (row) at least once. A 0 indicates that the borrower never borrowed the book.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="4" class="title">Borrower ID</th></tr>\n<tr><th>1</th><th>2</th><th>...</th><th>2,473</th></tr>\n<tr><th>1</th><td>1</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>2</th><td>1</td><td>1</td><td>...</td><td>0</td></tr>\n<tr><th>3</th><td>0</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>1</td><td>1</td><td>...</td><td>1</td></tr>\n</table>\n\nConceptually, each cell in the table contains a 1 if the person associated with the cell's column borrowed the book associated with the cell's row. If there was no such loan between a given book and borrower the corresponding cell contains a 0. For example, Table 1 shows that book 2 was borrowed (at least once) by borrower 1 but never by borrower 2,473.\n\nBook Similarity\n-----------------\nThe table view of the books and their borrowers does not readily lend itself to insight. The approach we took to get a better picture of this information was to plot each book as a point on a graph so that similar books are placed closer together than dissimilar books. To do this a notion of what "similar books" is required.\n\nMathematically, row [tex]i[/tex] of Table 1 can be represented as a vector [tex]\\mathbf{b}_i[/tex] of 1s and 0s. The value of the cell in the [tex]j[/tex]<sup>th</sup> column of that row will be denoted [tex]b_{i,j}[/tex]. For example, the 2<sup>nd</sup> row in the table can be written as the vector [tex]\\mathbf{b}_2 = (1,1,\\ldots,0)[/tex] and the value in its first column is [tex]b_{2,1} = 1[/tex].\n\nA crude measure of the similarity between book 1 and book 2 can be computed from this table by counting how many borrowers they have in common. That is, the number of columns that have a `1` in the row for book 1 and the row for book 2. \n \nIn terms of the vector representation, this similarity measure is simply the "[inner product][]" between [tex]\\mathbf{b}_1[/tex] and [tex]\\mathbf{b}_2[/tex] and is written [tex]\\left<\\mathbf{b}_1,\\mathbf{b}_2\\right> = b_{1,1}b_{2,1} + \\cdots + b_{1,N}b_{2,N}[/tex] where N = 2,473 is the total number of borrowers.\n\n[inner product]: http://en.wikipedia.org/wiki/Inner_product_space\n\nIt turns out that simply counting the number of borrowers two books is not a great measure of similarity. The problem is that two very popular books, each with 100 borrowers, that only share 10% of their borrowers would be considered as similar as two books, each with 10 readers, that share all of their borrowers. An easy way to correct this is to "normalise" the borrower counts by making sure the similarity of a book with itself is always equal to 1. A common way of doing this is by dividing the inner product of two books by the "size" of each of the vectors for those books. \n\nMathematically, we will denote the size of a book vector [tex]\\mathbf{b}_i[/tex] as [tex]\\|\\mathbf{b}_i\\| = \\sqrt{\\left<\\mathbf{b}_i,\\mathbf{b}_i\\right>}[/tex]. The similarity between two books then becomes:\n\n<center>\n[tex]\\displaystyle\n    \\text{sim}(\\mathbf{b}_i,\\mathbf{b}_j) \n     = \\frac{\\left<\\mathbf{b}_i,\\mathbf{b}_j\\right>}{\\|\\mathbf{b}_i\\|\\|\\mathbf{b}_j\\|}\n[/tex]\n</center>\n\nPrincipal Component Analysis\n---------------------------------\nNow that we have a similarity measure between books the idea is to create a plot of points -- one per book -- so that similar books are placed close together and dissimilar books are kept far apart. \n\nA standard technique for doing this is [Principal Component Analysis][pca]. Intuitively, this technique aims to find a way of reducing the number of coordinates in each book vector  in such a way that when the similarity between two books is computed using these smaller vectors it is as close as possible to the original similarity. That is, PCA creates a new table that represents books in terms of only two columns.\n\n[pca]: http://en.wikipedia.org/wiki/Principal_components_analysis\n\n<table>\n<caption>Table 2: A portion of the book table after PCA. The values in the two new columns (PCA IDs) can be used to plot the books.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="2" class="title">PCA ID</th></tr>\n<tr>                 <th>1</th><th>2</th></tr>\n<tr><th>1</th><td>-8.2</td><td>2.3</td></tr>\n<tr><th>2</th><td>0.4</td><td>-4.3</td></tr>\n<tr><th>3</th><td>-1.3</td><td>-3.7</td></tr>\n<tr><th>...</th><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>2.2</td><td>-5.6</td></tr>\n</table>\n\nTable 2 gives an example of the book table after PCA that reduces the book vectors (rows) from 2,473 to two entries. The PCA columns cannot be as easily interpreted as the borrowers columns in Table 1 but the values in the columns are such that the similarity of the books in Table 2 are roughly as similar as if the values in Table 1 were used. That is, if [tex]\\mathbf{c}_1 = (-8.2,2.3)[/tex] and [tex]\\mathbf{c}_2=(0.4,-4.3)[/tex] are the vectors\nfor the first two rows of Table 2 then [tex]\\text{sim}(\\mathbf{c}_1,\\mathbf{c}_2)[/tex]\nwould be close to [tex]\\text{sim}(\\mathbf{b}_1,\\mathbf{b}_2)[/tex], the similarity of the\nfirst two rows in Table 1.[^1]\n\n[^1]: Technically, the guarantee of the "closeness" of the similarity measures only holds on average, that is, over all possible pairs of books. There is no guarantee any particular pair's\nsimilarity is estimated well.\n\nVisualising the Data\n----------------------\nFigure 1 shows a plot of the PCA reduced book data. Each circle represents one of the 1,616 books, plotted according to the coordinates in a table like Table 2. The size of each circle indicates how many borrowers each book had and its colour indicates which library the book belongs to.[^2]\n\n[^2]: A book can belong to more than one library. In this case one library is chosen at random to determine a circle's colour.\n\n<div class="image">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/06/all_libraries.png" alt="Plot of the books across all libraries in the ACRP database" width="550" class="aligncenter wp-image-43" />\n<p>Figure 1: A PCA plot of all the books in the ACRP database coloured according to which library they belong to. The size of each circle indicates the number of borrowers of the corresponding book.\n</div>\n\nOne immediate observation is that books are clustered according to which library they belong to. This is not too surprising since the books in a library limit what borrowers from that library can read. This means it is likely that two voracious readers that frequent the same library will read the same books. This, in turn, will mean the similarity of two books from a library will be higher than books from different libraries as there are very few borrowers that use more than one library.\n\nDrilling Down and Interacting\n---------------------------------\nTo get a better picture of the data, we decided to focus on books from a single library to avoid this clustering. The library we focused on was the [Lambton][] Miners' and Mechanics' Institute in New South Wales. This library had the largest number of loans (20,253) and so was most likely to have interesting similarity data.\n\n[lambton]: http://en.wikipedia.org/wiki/Lambton,_New_South_Wales\n\nThere are a total of 789 books in the Lambton institute and 469 borrowers of those books. A separate PCA reduction was performed on this restricted part of the database to create a plot of only the Lambton books.\n\nTo make it easier to explore this data, I wrote a simple tool that allows a viewer to interact with the PCA plot. A screenshot from this tool is shown in Figure 2. Once again, larger circles represent books with a larger number of borrowers. \n\nClicking on the figure will open a new window and, after a short delay, the tool will run. The same page can also be accessed from [this link][applet]. \n\n[applet]: /inductio/wp-content/public/acrp/\n\n<div class="image">\n<a href='http://conflate.net/inductio/wp-content/public/acrp/' target="_"><img src="http://conflate.net/inductio/wp-content/uploads/2008/06/acrp.png" alt="Click to open visualisation applet" width="550" class="aligncenter wp-image-41" /></a>\n<p>Figure 2: A screenshot of the ACRP visualisation tool showing books from the Lambton Institute. Click the image to run the tool in a new window.</p>\n</div>\n\nInstructions describing how to use the tool can be found below it. \nIn a nutshell: hovering over a circle will reveal the title of the book corresponding to that circle; clicking on a circle will draw lines to its most similar neighbours; altering the "Borrowers" bar will only show books with at least that many borrowers; and altering the "Similarity" bar will only draw lines to books with at least that proportion of books in common.\n\nFuture Work and Distant Reading\n-------------------------------------\nJulieanne and I are still at the early stages of our research using the ACRP database. The use of PCA for visualisation was a first step in our pursuit of what [Franco Moretti][] calls "distant reading" -- looking at books as objects and how they are read rather than the "close reading" of the text of individual books. \n\n[Franco Moretti]: http://en.wikipedia.org/wiki/Franco_Moretti \n\nNow that we have this tool, we are able to quickly explore relationships between these books based on the reading habits of Australians at the turn of the century. Of course, there are many caveats that apply to any patterns we might see in these plots. For instance, the similarity between books is only based on habits of a small number of readers and will be influenced by the peculiarities of the libraries and the books they choose to buy. For this reason, these plots are not intended to provide conclusive answers to questions we might. \n\nInstead we hope that exploring the ACRP database in this way will lead us to interesting questions about particular pairs or groups of books that can be followed up by a more thorough analysis of their readers, their text as well as other historical and cultural factors about them.\n\nData and Code\n----------------\nFor the technically minded, I have made the code I used to do the visualisation is available on [GitHub][]. It is a combination of [SQL][] for data preprocessing, [R][] for the PCA reduction and [Processing][] for creating the visualisation tool. You will also find a number of images and some notes at the same location.\n\n[github]: http://github.com/mreid/acrp/tree/master \n[SQL]: http://en.wikipedia.org/wiki/SQL\n[R]: http://www.r-project.org/\n[Processing]: http://processing.org/\n\nAccess to the data that the code acts upon is not mine to give, so the code is primarily to show how I did the visualisation rather than a way to let others analyse the data. If the founders of the [ACRP][] project decide to release the data to the public at a later date I will link to it from here.\n\n
41	ACRP Visualisation	acrp	2008-06-10 11:13:43	2008-06-10 11:13:43	Applet showing neighbours of a book	inherit	A screen grab of the applet showing the neighbours of a selected book.
42	Constructive and Classical Mathematics	constructive-and-classical-mathematics	2008-06-16 05:46:56	2008-06-12 02:08:12		publish	I have a (very) amateur interest in the philosophy of mathematics. My interest was recently piqued again after finishing the very readable "[Introducing Philosophy of Mathematics][ipom]" by Michèle Friend. Since then, I've been a lot more aware of terms like "constructivist", "realist", and "formalist" as they apply to mathematics.\n\nToday, I was flicking through the entry on "[Constructivist Mathematics][cm]" in the [Stanford Encyclopedia of Philosophy][seop] and found a simple example of some of the problems with non-constructive take on what disjunction means in mathematical statements. The article calls it "well-worn" but I hadn't seen it before.\n\nConsider the statement:\n> There exists irrational numbers a and b such that a<sup>b</sup> is rational.\n\nThe article gives a slick proof that this statement is true by invoking the [law of the excluded middle][lem] (LEM). That is, every number must be either rational or irrational. \n\nNow consider [tex]\\sqrt{2}^\\sqrt{2}[/tex].  By the LEM, this must rational or irrational:\n\n  * Case 1: If it is rational then we have proved the statement since we know [tex]a = b = \\sqrt{2}[/tex] is irrational. \n\n  * Case 2: If [tex]\\sqrt{2}^\\sqrt{2}[/tex] is irrational then choosing [tex]a = \\sqrt{2}^\\sqrt{2}[/tex] and [tex]b = \\sqrt{2}[/tex] as our two irrational numbers gives [tex]{\\sqrt{2}^{\\sqrt{2}^\\sqrt{2}}} = {\\sqrt{2}^2} = 2[/tex] -- a rational number. \n\nEither way, we've proven the existence of two irrational numbers yielding a rational one.\nThe problem with this is that this argument is non-constructive and so we don't know which of case 1 and case 2 is true, we only know that one of them must be[^1]. This is a simple case of <i>reductio ad absurdum</i> in disguise.\n\nAs a born-again computer scientist (my undergraduate degree was pure maths and my PhD in computer science) I've become increasingly suspicious of these sorts of proof and more [constructivist][] -- even [intuitionist][] -- in my tastes. I think the seed of doubt was planted during the awkward discussions of the [Axiom of Choice][] in my functional analysis lectures. The sense of unease is summed up nicely in the following joke:\n\n> The Axiom of Choice is obviously true, the well-ordering principle obviously false, \n> and who can tell about Zorn's lemma?\n\nOf course, all those concepts are equivalent but that's far from intuitive.\n\nI don't think I'm extremist enough to take a wholeheartedly computational view of mathematics -- denying all but the computable real numbers and functions, thereby making [all functions continuous][] -- but it is a tempting view of the subject.\n\nIn machine learning, I think there is a fairly pragmatic take on the philosophy of mathematics. For example, classical theorems from functional analysis are used to derive results involving kernels but when it comes to implementation, estimations and approximations are used with abandon. In my opinion, this is a [healthy way for the theory in this area to proceed][lemire]. As in physics, if the experimental work reveals inconsistencies with a theory, revisit the maths. If that doesn't work, [talk to the philosophers][dim].\n\n[ipom]: http://www.librarything.com/work/3362656/book/17581191\n[cm]: http://plato.stanford.edu/entries/mathematics-constructive/\n[seop]: http://plato.stanford.edu/\n[lem]: http://en.wikipedia.org/wiki/Law_of_the_excluded_middle\n[intuitionist]: http://en.wikipedia.org/wiki/Intuitionism\n[constructivist]: http://en.wikipedia.org/wiki/Constructivism_%28mathematics%29\n[axiom of choice]: http://en.wikipedia.org/wiki/Axiom_of_choice\n[all functions continuous]: http://math.andrej.com/2006/03/27/sometimes-all-functions-are-continuous/\n[lemire]: http://www.daniel-lemire.com/blog/archives/2008/06/05/why-pure-theory-is-wasteful/\n[dim]: http://diveintomark.org/archives/2008/06/11/purity\n\n[^1]: It turns out that, by [Gelfond's Theorem](http://en.wikipedia.org/wiki/Gelfond's_theorem) that [tex]\\sqrt{2}^\\sqrt{2}[/tex] is transcendental, and therefore irrational so the second case alone proves the statement. However, I'm not sure what machinery is required to prove Gelfond's theorem.\n\n
43	PCA of All Libraries	all_libraries	2008-06-17 02:07:10	2008-06-17 02:07:10	Plot of the books across all libraries in the ACRP database	inherit	
44	ICML Discussion Site	icml-discussion-site	2008-07-01 15:40:23	2008-07-01 15:40:23		publish	A little while ago, John Langford [suggested][jl1] that a discussion site be set up for ICML that allows attendees and others to talk about the accepted papers.\n\nHaving played around with various wiki systems and discussion sites in the past, I volunteered to help set something up. As John has [noted on his blog][jl2] the discussion site is now [up and running][icml]. \n\nThe main aim with this first attempt was to provide basic functionality: papers can be browsed by author, title and keyword; each paper has a discussion thread where anyone can leave comments. There are no comments at the time of writing this but I'm hoping this will change once the conference gets underway.\n\nProvided there are no disasters, the site will remain up for as long as it is useful. Ultimately, I'd like to add earlier conference proceedings to the site and ensure future conferences can be added as well. We will see how it goes this year and incorporate and feedback into future versions of the site.\n\nFor those interested in the technical details, I used [DokuWiki](http://wiki.splitbrain.org/wiki:dokuwiki) as the engine for the site along with a number of plugins, most importantly the [discussion plugin](http://wiki.splitbrain.org/plugin:discussion).\n\n[jl1]: http://hunch.net/?p=327\n[jl2]: http://hunch.net/?p=335\n[icml]: http://conflate.net/icml
45	Evaluation Methods for Machine Learning	evaluation-methods-for-machine-learning	2008-07-21 11:18:19	2008-07-21 11:18:19	Some thoughts on the workshop on evaluation methods that I attended as part of ICML 2008 in Helsinki.	publish	Although I wasn't able to attend the talks at [ICML 2008][] I was able to participate in the [Workshop on Evaluation Methods for Machine Learning][emml] run by William Klement, [Chris Drummond][], and [Nathalie Japkowicz][].\n\n[icml 2008]: http://icml2008.cs.helsinki.fi/\n[emml]: http://www.site.uottawa.ca/ICML08WS/\n[nathalie japkowicz]: http://www.site.uottawa.ca/~nat/\n[chris drummond]: http://www.site.uottawa.ca/~cdrummon/\n\nThis workshop at ICML was a continuation of previous workshops held at AAAI that aim to cast a critical eye on the methods used in machine learning to experimentally evaluate the performance of algorithms.\n\nIt kicked off with a series of mini debates with Nathalie and Chris articulating the opposing sides. The questions included the following:\n\n * Should we change how evaluation is done?\n * Is evaluation central to empirical work?\n * Are statistical tests critical to evaluation?\n * Are the UCI data sets sufficient for evaluation?\n\nThere were three papers I particularly liked: [Janez Demsar][]'s talk "[On the Appropriateness of Statistical Tests in Machine Learning][appropriateness]", [Edith Law][]'s "[The Problem of Accuracy as an Evaluation Criterion][accuracy]", and [Chris Drummond][]'s call for a mild-mannered revolution "[Finding a Balance between Anarchy and Orthodoxy][anarchy]".\n\n[janez demsar]: http://www.ailab.si/janez/\n[appropriateness]: http://www.site.uottawa.ca/ICML08WS/papers/J_Demsar.pdf\n[edith law]: http://www.cs.cmu.edu/~elaw/\n[accuracy]: http://www.site.uottawa.ca/ICML08WS/papers/E_Law.pdf\n[anarchy]: http://www.site.uottawa.ca/ICML08WS/papers/C_Drummond.pdf\n\nJanez's talk touched on a number of criticisms that [I had found in Jacob Cohen's paper "The Earth is Round (p < 0.05)"][round earth] making the case that people often incorrectly report and incorrectly interpret p-values for statistical tests. Unfortunately, as Janez points out, since machine learning is a discipline that (rightly) places emphasis on results it is difficult as a reviewer to reject a paper that presents an ill-motivated and confusing idea if its authors have shown that, statistically, it outperforms similar approaches.\n\n[round earth]: http://conflate.net/inductio/2008/04/the-earth-is-round/\n\nEdith's talk argued that accuracy is sometimes a poor measure of performance making all this concern over whether we are constructing statistical tests for it (or AUC) moot. In particular, for tasks like salient region detection in images, language translation and music tagging there is no single correct region, translation or tag. Whether or not a particular region/translation/tag is "correct" or not is impossible to determine independent of the more difficult tasks of image recognition/language understanding/music identification. Solving these for the purposes of evaluation would make a solution to the smaller tasks redundant. Instead of focusing on evaluation of the smaller tasks, Edith suggests ways in which games that humans play on the web -- such as the [ESP Game][] -- can be used to evaluate machine performance on these tasks by playing learning algorithms against humans.\n\n[esp game]: http://www.espgame.org/\n\nFinally, Chris's talk made the bold claim that the way we approach evaluation in machine learning is an "impoverished realization of a controversial methodology", namely statistical hypothesis testing. "Impoverished" because when we do do hypothesis testing it is in the narrowest of senses, mainly to test that my algorithm is better than yours on this handful of data sets. "Controversial" since many believe science to have social, exploratory and accidental aspects --- much more than just the clinical proposing of hypotheses for careful testing.\n\nWhat these papers and the workshop as a whole showed me was how unresolved my position is on these and other questions regarding evaluation. On the one hand I spent a lot of time painstakingly setting up, running and analysing experiments for my [PhD research][] on inductive transfer in order to evaluate the methods I was proposing. I taught myself how to correctly control for confounding factors, use the [Bonferroni correction][] to adjust significance levels and other esoterica of statistical testing. Applying all these procedures carefully to my work felt very scientific and I was able to create many pretty graphs and tables replete with confidence intervals, p-values and the like. On the other hand -- and with sufficient hindsight -- it's not clear how much value this type of analysis added to the thesis overall (apart from demonstrating to my reviewers that I could do it). \n\n[phd research]: http://www.library.unsw.edu.au/~thesis/adt-NUN/public/adt-NUN20070512.173744/index.html\n[bonferroni correction]: http://en.wikipedia.org/wiki/Bonferroni_correction\n\nThe dilemma is this: when one algorithm or approach clearly dominates another details such as p-values, t-tests and the like only obscure the results; and when two algorithms are essentially indistinguishable using "significance" levels to pry them apart seems to be grasping at straws.\n\nThat's not to say that we should get rid of empirical evaluation all together. Rather, we should carefully choose (or create) our data sets and empirical questions so as to gain as much insight as possible and go beyond "my algorithm is better than yours". Statistical tests should not mark the end of an experimental evaluation but rather act as a starting point for further questions and carefully constructed experiments that resolve those questions.\n
46	COLT 2008 Highlights	colt-2008-highlights	2008-07-27 23:19:39	2008-07-27 11:40:46		publish	I'm a little late to the party since several machine learning bloggers have already noted their favourite papers at the recent joint [ICML][]/[UAI][]/[COLT][] conferences in Helsinki. \n\n[John][] listed a few COLT papers he liked, [Hal][] has covered some tutorials as well as several ICML and a few UAI and COLT papers, while [Jurgen][] has given an overview of the [non-parametric Bayes workshop][npbayes] workshop.\n\n[icml]: http://icml2008.cs.helsinki.fi/\n[uai]: http://uai2008.cs.helsinki.fi/\n[colt]: http://colt2008.cs.helsinki.fi/\n\n[john]: http://hunch.net/?p=341\n[hal]: http://nlpers.blogspot.com/2008/07/icmluaicolt-2008-retrospective.html\n[jurgen]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n[npbayes]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n\nI didn't make it to the main ICML sessions but I did catch the workshop on evaluation in machine learning. Since I've already [written about][evaluation] that and didn't attend any of the UAI sessions, I'll focus on the COLT stuff I found interesting.\n\n[evaluation]: http://conflate.net/inductio/2008/07/evaluation-methods-for-machine-learning/\n\nThe joint COLT/UAI invited talks were all excellent and covered a diverse range of topics. [Robin Hanson][] gave a great introduction to prediction markets. It was clear he'd given this type of talk before as he handled the many subsequent questions directly and decisively. I'm really interested in the work being done here so I'll write more about prediction markets in a later post. \n\n[robin hanson]: http://hanson.gmu.edu/home.html\n\n[Gábor Lugosi][] talkcovered a number of important concentration inequalities, focusing on the logarithmic Sobolev inequalities.  It was a perfect example of a maths talk where details were eschewed without compromising accuracy in order to give insight into the main results. \n\n[gábor lugosi]: http://www.econ.upf.es/~lugosi/\n\n[Dan Klein][] presented some impressive results pertaining to the unsupervised learning in three natural language problems: grammar refinement (inventing new annotations to improve parsing), coreference resolution (determining which nouns refer to the same thing) and lexicon translation (matching words across languages). By setting up simple Bayesian models and throwing a ton of unlabelled examples at them he was able to get results competitive with the best supervised learning approaches on some problems. \n\nThe lexicon translation was particularly impressive. Given a set of English documents and a set of Chinese documents his system was able to do a passable job of translating single words between the language. What was impressive is that there was no information saying that, for example, this English document is a translation of that Chinese document. They could have all been pulled randomly from .co.uk and .cn sites. \n\nIf I understand it correctly, Klein's system simply looked for common patterns of words within documents of one language and then tried to match those patterns to similar patterns in the documents of the other. When the languages were "closer" - such as with Spanish and English - the system also made use of patterns of letters within words (e.g., "<i>direc</i>tion" and "<i>direc</i>ción") to find possible cognates. \n\n[Dan Klein]: http://www.cs.berkeley.edu/~klein/\n\nThere was a variety of good papers at COLT this year. Of the talks I saw, two stood out for me.\n\n[The True Sample Complexity of Active Learning][balcan hanneke] by Balcan, Hanneke and Wortman showed the importance of choosing the right theoretic model for analysis. In active learning the learner is able to choose which unlabelled examples have their labels revealed. \nIntuitively, one would think that this should make learning easier than the normal supervised learning where the learner has no say in the matter. \n\nPrevious results showed that this was basically not the case. Subtly, those results asked that the active learner achieve a certain error rate but also _verify_ that that rate was achieved. What Nina and her co-authors showed was that if you remove this extra requirement then active learning does indeed make learning much easier, often with exponential improvements in sample complexity over "passive" learning.\n\n[balcan hanneke]: http://colt2008.cs.helsinki.fi/papers/108-Balcan.pdf\n\n[An Efficient Reduction of Ranking to Classification][ailon] by Ailon and Mohri was also impressive. They build on earlier work that showed how a ranking problem can be reduced to learning a binary preference relation between the items to be ranked. One crucial part of this reduction is turning a learnt preference relation into a ranking. That is, taking pair-wise assessments of relative item quality and laying out all those items along a line so as to best preserve those pair-wise relationships. \n\nWhat Ailon and Mohri show is that simply applying a randomised quicksort to the pair-wise comparisons for n items will give a good reduction to a ranking in O(n log n) time. "Good" here means that the regret of the reduction-based ranking over the best possible is bounded by the regret of the classifier that learns the preference relation over the best possible classification. Furthermore, if you are only interested in the top k of n items you can get a good ranking in O(k log k + n) time. What's particularly nice about this work is the tools they use to analysis randomised quicksort are very general and will probably find other applications.\n\n[ailon]: http://colt2008.cs.helsinki.fi/papers/32-Ailon.pdf\n\nFinally, while I didn't attend the talk at COLT, a couple of people have told me that Abernethy et al.'s paper [Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization][abernethy] was very good. I've since skimmed through it and it is a very nice paper, well-written and replete with interesting connections. I'm not that familiar with bandit learning work but this paper is has a good summary of recent results and is intriguing enough to make me want to investigate further.\n\n[sridharan]: http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf \n[abernethy]: http://colt2008.cs.helsinki.fi/papers/123-Abernethy.pdf\n\n
48	Prediction Markets	47-revision	2008-07-25 07:18:14	2008-07-25 07:18:14		inherit	The invited talks were all really interesing. [Robin Hanson][] gave a great introduction to prediction markets, describing how they can be used to extract information through the use of market scoring rules. Essentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\n
49	COLT 2008 Highlights	46-revision	2008-07-25 07:24:46	2008-07-25 07:24:46		inherit	I'm a little late to the party since several machine learning bloggers have already noted their favourite papers at the recent joint [ICML][]/[UAI][]/[COLT][] conferences in Helsinki. \n\n[John][] listed a few COLT papers he liked, [Hal][] has covered some tutorials as well as several ICML and a few UAI and COLT papers, while [Jurgen][] has given an overview of the [non-parametric Bayes workshop][npbayes] workshop.\n\n[icml]: http://icml2008.cs.helsinki.fi/\n[uai]: http://uai2008.cs.helsinki.fi/\n[colt]: http://colt2008.cs.helsinki.fi/\n\n[john]: http://hunch.net/?p=341\n[hal]: http://nlpers.blogspot.com/2008/07/icmluaicolt-2008-retrospective.html\n[jurgen]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n[npbayes]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n\nI didn't make it to the main ICML sessions but I did catch the workshop on evaluation in machine learning. Since I've already [written about][evaluation] that and didn't attend any of the UAI sessions, I'll focus on the COLT stuff I found interesting.\n\n[evaluation]: http://conflate.net/inductio/2008/07/evaluation-methods-for-machine-learning/\n\nThe joint COLT/UAI invited talks were all excellent and covered a diverse range of topics. [Robin Hanson][] gave a great introduction to prediction markets. It was clear he'd given this type of talk before as he handled the many subsequent questions directly and decisively. I'm really interested in the work being done here so I'll write more about prediction markets in a latter post. \n\n[Gábor Lugosi][] talkcovered a number of important concentration inequalities, focusing on mainly on the logarithmic Sobelov inequalities. \n\nConcentration Inequalities\n-----------------------------\nGábor Lugosi's talk \non concentration inequalities was very well done, covering a number of \nimportant theorems in this area in a very clear manner. It was a perfect\nexample of a maths talk where details were eschewed without compromising\naccuracy in order to give insight into the inequalities.\n\nPapers\n-------\n* [The True Sample Complexity of Active Learning][balcan hanneke] Balcan, Hanneke and Wortman.\n\n* [An Efficient Reduction of Ranking to Classification][ailon] Ailon and Mohri.\n\n* [Improved Guarantees for Learning with Similarity Functions][balcan blum] Balcan, Blum and\nSrebo\n\n* [An Information Theoretic Framework for Multi-view Learning][sridharan] by Sridharan and Kakade\n\nWhile I didn't attend the talk at COLT, a couple of people have told me that Abernethy et al.'s paper [Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization][abernethy] was very good. I've since skimmed through it and it is a very nice paper, well-written and replete with interesting connections.\n\n[ailon]: http://colt2008.cs.helsinki.fi/papers/32-Ailon.pdf\n[balcan hanneke]: http://colt2008.cs.helsinki.fi/papers/108-Balcan.pdf\n[balcan blum]: http://colt2008.cs.helsinki.fi/papers/86-Balcan.pdf\n[sridharan]: http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf \n[abernethy]: http://colt2008.cs.helsinki.fi/papers/123-Abernethy.pdf\n\n
50	COLT 2008 Highlights	46-revision-2	2008-07-26 08:20:39	2008-07-26 08:20:39		inherit	I'm a little late to the party since several machine learning bloggers have already noted their favourite papers at the recent joint [ICML][]/[UAI][]/[COLT][] conferences in Helsinki. \n\n[John][] listed a few COLT papers he liked, [Hal][] has covered some tutorials as well as several ICML and a few UAI and COLT papers, while [Jurgen][] has given an overview of the [non-parametric Bayes workshop][npbayes] workshop.\n\n[icml]: http://icml2008.cs.helsinki.fi/\n[uai]: http://uai2008.cs.helsinki.fi/\n[colt]: http://colt2008.cs.helsinki.fi/\n\n[john]: http://hunch.net/?p=341\n[hal]: http://nlpers.blogspot.com/2008/07/icmluaicolt-2008-retrospective.html\n[jurgen]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n[npbayes]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n\nI didn't make it to the main ICML sessions but I did catch the workshop on evaluation in machine learning. Since I've already [written about][evaluation] that and didn't attend any of the UAI sessions, I'll focus on the COLT stuff I found interesting.\n\n[evaluation]: http://conflate.net/inductio/2008/07/evaluation-methods-for-machine-learning/\n\nThe joint COLT/UAI invited talks were all excellent and covered a diverse range of topics. [Robin Hanson][] gave a great introduction to prediction markets. It was clear he'd given this type of talk before as he handled the many subsequent questions directly and decisively. I'm really interested in the work being done here so I'll write more about prediction markets in a later post. \n\n[Gábor Lugosi][] talkcovered a number of important concentration inequalities, focusing on the logarithmic Sobolev inequalities.  It was a perfect example of a maths talk where details were eschewed without compromising accuracy in order to give insight into the main results. \n\n[gábor lugosi]: http://www.econ.upf.es/~lugosi/\n\n[Dan Klein][] presented some impressive results pertaining to the unsupervised learning in three natural language problems: grammar refinement (inventing new annotations to improve parsing), coreference resolution (determining which nouns refer to the same thing) and lexicon translation (matching words across languages). By setting up simple Bayesian models and throwing a ton of unlabelled examples at them he was able to get results competitive with the best supervised learning approaches on some problems. \n\nThe lexicon translation was particularly impressive. Given a set of English documents and a set of Chinese documents his system was able to do a passable job of translating single words between the language. What was impressive is that there was no information saying that, for example, this English document is a translation of that Chinese document. They could have all been pulled randomly from .co.uk and .cn sites. \n\nIf I understand it correctly, Klein's system simply looked for common patterns of words within documents of one language and then tried to match those patterns to similar patterns in the documents of the other. When the languages were "closer" - such as with Spanish and English - the system also made use of patterns of letters within words (e.g., "_direc_tion" and "direc \n\n[Dan Klein]: http://www.cs.berkeley.edu/~klein/\n\nPeter Grünwald.\n\nPapers\n-------\n* [The True Sample Complexity of Active Learning][balcan hanneke] Balcan, Hanneke and Wortman.\n\n* [An Efficient Reduction of Ranking to Classification][ailon] Ailon and Mohri.\n\n* [Improved Guarantees for Learning with Similarity Functions][balcan blum] Balcan, Blum and\nSrebo\n\n* [An Information Theoretic Framework for Multi-view Learning][sridharan] by Sridharan and Kakade\n\nWhile I didn't attend the talk at COLT, a couple of people have told me that Abernethy et al.'s paper [Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization][abernethy] was very good. I've since skimmed through it and it is a very nice paper, well-written and replete with interesting connections.\n\n[ailon]: http://colt2008.cs.helsinki.fi/papers/32-Ailon.pdf\n[balcan hanneke]: http://colt2008.cs.helsinki.fi/papers/108-Balcan.pdf\n[balcan blum]: http://colt2008.cs.helsinki.fi/papers/86-Balcan.pdf\n[sridharan]: http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf \n[abernethy]: http://colt2008.cs.helsinki.fi/papers/123-Abernethy.pdf\n\n
51	COLT 2008 Highlights	46-revision-3	2008-07-26 08:21:28	2008-07-26 08:21:28		inherit	I'm a little late to the party since several machine learning bloggers have already noted their favourite papers at the recent joint [ICML][]/[UAI][]/[COLT][] conferences in Helsinki. \n\n[John][] listed a few COLT papers he liked, [Hal][] has covered some tutorials as well as several ICML and a few UAI and COLT papers, while [Jurgen][] has given an overview of the [non-parametric Bayes workshop][npbayes] workshop.\n\n[icml]: http://icml2008.cs.helsinki.fi/\n[uai]: http://uai2008.cs.helsinki.fi/\n[colt]: http://colt2008.cs.helsinki.fi/\n\n[john]: http://hunch.net/?p=341\n[hal]: http://nlpers.blogspot.com/2008/07/icmluaicolt-2008-retrospective.html\n[jurgen]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n[npbayes]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n\nI didn't make it to the main ICML sessions but I did catch the workshop on evaluation in machine learning. Since I've already [written about][evaluation] that and didn't attend any of the UAI sessions, I'll focus on the COLT stuff I found interesting.\n\n[evaluation]: http://conflate.net/inductio/2008/07/evaluation-methods-for-machine-learning/\n\nThe joint COLT/UAI invited talks were all excellent and covered a diverse range of topics. [Robin Hanson][] gave a great introduction to prediction markets. It was clear he'd given this type of talk before as he handled the many subsequent questions directly and decisively. I'm really interested in the work being done here so I'll write more about prediction markets in a later post. \n\n[Gábor Lugosi][] talkcovered a number of important concentration inequalities, focusing on the logarithmic Sobolev inequalities.  It was a perfect example of a maths talk where details were eschewed without compromising accuracy in order to give insight into the main results. \n\n[gábor lugosi]: http://www.econ.upf.es/~lugosi/\n\n[Dan Klein][] presented some impressive results pertaining to the unsupervised learning in three natural language problems: grammar refinement (inventing new annotations to improve parsing), coreference resolution (determining which nouns refer to the same thing) and lexicon translation (matching words across languages). By setting up simple Bayesian models and throwing a ton of unlabelled examples at them he was able to get results competitive with the best supervised learning approaches on some problems. \n\nThe lexicon translation was particularly impressive. Given a set of English documents and a set of Chinese documents his system was able to do a passable job of translating single words between the language. What was impressive is that there was no information saying that, for example, this English document is a translation of that Chinese document. They could have all been pulled randomly from .co.uk and .cn sites. \n\nIf I understand it correctly, Klein's system simply looked for common patterns of words within documents of one language and then tried to match those patterns to similar patterns in the documents of the other. When the languages were "closer" - such as with Spanish and English - the system also made use of patterns of letters within words (e.g., "_direc_tion" and "_direc_ción") to find possible cognates. \n\n[Dan Klein]: http://www.cs.berkeley.edu/~klein/\n\nPeter Grünwald.\n\nPapers\n-------\n* [The True Sample Complexity of Active Learning][balcan hanneke] Balcan, Hanneke and Wortman.\n\n* [An Efficient Reduction of Ranking to Classification][ailon] Ailon and Mohri.\n\n* [Improved Guarantees for Learning with Similarity Functions][balcan blum] Balcan, Blum and\nSrebo\n\n* [An Information Theoretic Framework for Multi-view Learning][sridharan] by Sridharan and Kakade\n\nWhile I didn't attend the talk at COLT, a couple of people have told me that Abernethy et al.'s paper [Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization][abernethy] was very good. I've since skimmed through it and it is a very nice paper, well-written and replete with interesting connections.\n\n[ailon]: http://colt2008.cs.helsinki.fi/papers/32-Ailon.pdf\n[balcan hanneke]: http://colt2008.cs.helsinki.fi/papers/108-Balcan.pdf\n[balcan blum]: http://colt2008.cs.helsinki.fi/papers/86-Balcan.pdf\n[sridharan]: http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf \n[abernethy]: http://colt2008.cs.helsinki.fi/papers/123-Abernethy.pdf\n\n
52	COLT 2008 Highlights	46-revision-4	2008-07-26 08:42:21	2008-07-26 08:42:21		inherit	I'm a little late to the party since several machine learning bloggers have already noted their favourite papers at the recent joint [ICML][]/[UAI][]/[COLT][] conferences in Helsinki. \n\n[John][] listed a few COLT papers he liked, [Hal][] has covered some tutorials as well as several ICML and a few UAI and COLT papers, while [Jurgen][] has given an overview of the [non-parametric Bayes workshop][npbayes] workshop.\n\n[icml]: http://icml2008.cs.helsinki.fi/\n[uai]: http://uai2008.cs.helsinki.fi/\n[colt]: http://colt2008.cs.helsinki.fi/\n\n[john]: http://hunch.net/?p=341\n[hal]: http://nlpers.blogspot.com/2008/07/icmluaicolt-2008-retrospective.html\n[jurgen]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n[npbayes]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n\nI didn't make it to the main ICML sessions but I did catch the workshop on evaluation in machine learning. Since I've already [written about][evaluation] that and didn't attend any of the UAI sessions, I'll focus on the COLT stuff I found interesting.\n\n[evaluation]: http://conflate.net/inductio/2008/07/evaluation-methods-for-machine-learning/\n\nThe joint COLT/UAI invited talks were all excellent and covered a diverse range of topics. [Robin Hanson][] gave a great introduction to prediction markets. It was clear he'd given this type of talk before as he handled the many subsequent questions directly and decisively. I'm really interested in the work being done here so I'll write more about prediction markets in a later post. \n\n[robin hanson]: \n\n[Gábor Lugosi][] talkcovered a number of important concentration inequalities, focusing on the logarithmic Sobolev inequalities.  It was a perfect example of a maths talk where details were eschewed without compromising accuracy in order to give insight into the main results. \n\n[gábor lugosi]: http://www.econ.upf.es/~lugosi/\n\n[Dan Klein][] presented some impressive results pertaining to the unsupervised learning in three natural language problems: grammar refinement (inventing new annotations to improve parsing), coreference resolution (determining which nouns refer to the same thing) and lexicon translation (matching words across languages). By setting up simple Bayesian models and throwing a ton of unlabelled examples at them he was able to get results competitive with the best supervised learning approaches on some problems. \n\nThe lexicon translation was particularly impressive. Given a set of English documents and a set of Chinese documents his system was able to do a passable job of translating single words between the language. What was impressive is that there was no information saying that, for example, this English document is a translation of that Chinese document. They could have all been pulled randomly from .co.uk and .cn sites. \n\nIf I understand it correctly, Klein's system simply looked for common patterns of words within documents of one language and then tried to match those patterns to similar patterns in the documents of the other. When the languages were "closer" - such as with Spanish and English - the system also made use of patterns of letters within words (e.g., "<i>direc</i>tion" and "<i>direc</i>ción") to find possible cognates. \n\n[Dan Klein]: http://www.cs.berkeley.edu/~klein/\n\nPeter Grünwald.\n\nPapers\n-------\n* [The True Sample Complexity of Active Learning][balcan hanneke] Balcan, Hanneke and Wortman.\n\n* [An Efficient Reduction of Ranking to Classification][ailon] Ailon and Mohri.\n\n* [Improved Guarantees for Learning with Similarity Functions][balcan blum] Balcan, Blum and\nSrebo\n\n* [An Information Theoretic Framework for Multi-view Learning][sridharan] by Sridharan and Kakade\n\nWhile I didn't attend the talk at COLT, a couple of people have told me that Abernethy et al.'s paper [Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization][abernethy] was very good. I've since skimmed through it and it is a very nice paper, well-written and replete with interesting connections.\n\n[ailon]: http://colt2008.cs.helsinki.fi/papers/32-Ailon.pdf\n[balcan hanneke]: http://colt2008.cs.helsinki.fi/papers/108-Balcan.pdf\n[balcan blum]: http://colt2008.cs.helsinki.fi/papers/86-Balcan.pdf\n[sridharan]: http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf \n[abernethy]: http://colt2008.cs.helsinki.fi/papers/123-Abernethy.pdf\n\n
53	COLT 2008 Highlights	46-revision-5	2008-07-27 11:39:02	2008-07-27 11:39:02		inherit	I'm a little late to the party since several machine learning bloggers have already noted their favourite papers at the recent joint [ICML][]/[UAI][]/[COLT][] conferences in Helsinki. \n\n[John][] listed a few COLT papers he liked, [Hal][] has covered some tutorials as well as several ICML and a few UAI and COLT papers, while [Jurgen][] has given an overview of the [non-parametric Bayes workshop][npbayes] workshop.\n\n[icml]: http://icml2008.cs.helsinki.fi/\n[uai]: http://uai2008.cs.helsinki.fi/\n[colt]: http://colt2008.cs.helsinki.fi/\n\n[john]: http://hunch.net/?p=341\n[hal]: http://nlpers.blogspot.com/2008/07/icmluaicolt-2008-retrospective.html\n[jurgen]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n[npbayes]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n\nI didn't make it to the main ICML sessions but I did catch the workshop on evaluation in machine learning. Since I've already [written about][evaluation] that and didn't attend any of the UAI sessions, I'll focus on the COLT stuff I found interesting.\n\n[evaluation]: http://conflate.net/inductio/2008/07/evaluation-methods-for-machine-learning/\n\nThe joint COLT/UAI invited talks were all excellent and covered a diverse range of topics. [Robin Hanson][] gave a great introduction to prediction markets. It was clear he'd given this type of talk before as he handled the many subsequent questions directly and decisively. I'm really interested in the work being done here so I'll write more about prediction markets in a later post. \n\n[robin hanson]: http://hanson.gmu.edu/home.html\n\n[Gábor Lugosi][] talkcovered a number of important concentration inequalities, focusing on the logarithmic Sobolev inequalities.  It was a perfect example of a maths talk where details were eschewed without compromising accuracy in order to give insight into the main results. \n\n[gábor lugosi]: http://www.econ.upf.es/~lugosi/\n\n[Dan Klein][] presented some impressive results pertaining to the unsupervised learning in three natural language problems: grammar refinement (inventing new annotations to improve parsing), coreference resolution (determining which nouns refer to the same thing) and lexicon translation (matching words across languages). By setting up simple Bayesian models and throwing a ton of unlabelled examples at them he was able to get results competitive with the best supervised learning approaches on some problems. \n\nThe lexicon translation was particularly impressive. Given a set of English documents and a set of Chinese documents his system was able to do a passable job of translating single words between the language. What was impressive is that there was no information saying that, for example, this English document is a translation of that Chinese document. They could have all been pulled randomly from .co.uk and .cn sites. \n\nIf I understand it correctly, Klein's system simply looked for common patterns of words within documents of one language and then tried to match those patterns to similar patterns in the documents of the other. When the languages were "closer" - such as with Spanish and English - the system also made use of patterns of letters within words (e.g., "<i>direc</i>tion" and "<i>direc</i>ción") to find possible cognates. \n\n[Dan Klein]: http://www.cs.berkeley.edu/~klein/\n\nThere was a variety of good papers at COLT this year. Of the talks I saw, two stood out for me.\n\n[The True Sample Complexity of Active Learning][balcan hanneke] by Balcan, Hanneke and Wortman showed the importance of choosing the right theoretic model for analysis. In active learning the learner is able to choose which unlabelled examples have their labels revealed. \nIntuitively, one would think that this should make learning easier than the normal supervised learning where the learner has no say in the matter. \n\nPrevious results showed that this was basically not the case. Subtly, those results asked that the active learner achieve a certain error rate but also _verify_ that that rate was achieved. What Nina and her co-authors showed was that if you remove this extra requirement then active learning does indeed make learning much easier, often with exponential improvements in sample complexity over "passive" learning.\n\n[balcan hanneke]: http://colt2008.cs.helsinki.fi/papers/108-Balcan.pdf\n\n[An Efficient Reduction of Ranking to Classification][ailon] by Ailon and Mohri was also impressive. They build on earlier work that showed how a ranking problem can be reduced to learning a binary preference relation between the items to be ranked. One crucial part of this reduction is turning a learnt preference relation into a ranking. That is, taking pair-wise assessments of relative item quality and laying out all those items along a line so as to best preserve those pair-wise relationships. \n\nWhat Ailon and Mohri show is that simply applying a randomised quicksort to the pair-wise comparisons for n items will give a good reduction to a ranking in O(n log n) time. "Good" here means that the regret of the reduction-based ranking over the best possible is bounded by the regret of the classifier that learns the preference relation over the best possible classification. Furthermore, if you are only interested in the top k of n items you can get a good ranking in O(k log k + n) time. What's particularly nice about this work is the tools they use to analysis randomised quicksort are very general and will probably find other applications.\n\n[ailon]: http://colt2008.cs.helsinki.fi/papers/32-Ailon.pdf\n\nFinally, while I didn't attend the talk at COLT, a couple of people have told me that Abernethy et al.'s paper [Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization][abernethy] was very good. I've since skimmed through it and it is a very nice paper, well-written and replete with interesting connections. I'm not that familiar with bandit learning work but this paper is has a good summary of recent results and is intriguing enough to make me want to investigate further.\n\n[sridharan]: http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf \n[abernethy]: http://colt2008.cs.helsinki.fi/papers/123-Abernethy.pdf\n\n
54	COLT 2008 Highlights	46-revision-6	2008-07-27 11:40:46	2008-07-27 11:40:46		inherit	I'm a little late to the party since several machine learning bloggers have already noted their favourite papers at the recent joint [ICML][]/[UAI][]/[COLT][] conferences in Helsinki. \n\n[John][] listed a few COLT papers he liked, [Hal][] has covered some tutorials as well as several ICML and a few UAI and COLT papers, while [Jurgen][] has given an overview of the [non-parametric Bayes workshop][npbayes] workshop.\n\n[icml]: http://icml2008.cs.helsinki.fi/\n[uai]: http://uai2008.cs.helsinki.fi/\n[colt]: http://colt2008.cs.helsinki.fi/\n\n[john]: http://hunch.net/?p=341\n[hal]: http://nlpers.blogspot.com/2008/07/icmluaicolt-2008-retrospective.html\n[jurgen]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n[npbayes]: http://undirectedgrad.blogspot.com/2008/07/npbayes-workshop-at-icml.html\n\nI didn't make it to the main ICML sessions but I did catch the workshop on evaluation in machine learning. Since I've already [written about][evaluation] that and didn't attend any of the UAI sessions, I'll focus on the COLT stuff I found interesting.\n\n[evaluation]: http://conflate.net/inductio/2008/07/evaluation-methods-for-machine-learning/\n\nThe joint COLT/UAI invited talks were all excellent and covered a diverse range of topics. [Robin Hanson][] gave a great introduction to prediction markets. It was clear he'd given this type of talk before as he handled the many subsequent questions directly and decisively. I'm really interested in the work being done here so I'll write more about prediction markets in a later post. \n\n[robin hanson]: http://hanson.gmu.edu/home.html\n\n[Gábor Lugosi][] talkcovered a number of important concentration inequalities, focusing on the logarithmic Sobolev inequalities.  It was a perfect example of a maths talk where details were eschewed without compromising accuracy in order to give insight into the main results. \n\n[gábor lugosi]: http://www.econ.upf.es/~lugosi/\n\n[Dan Klein][] presented some impressive results pertaining to the unsupervised learning in three natural language problems: grammar refinement (inventing new annotations to improve parsing), coreference resolution (determining which nouns refer to the same thing) and lexicon translation (matching words across languages). By setting up simple Bayesian models and throwing a ton of unlabelled examples at them he was able to get results competitive with the best supervised learning approaches on some problems. \n\nThe lexicon translation was particularly impressive. Given a set of English documents and a set of Chinese documents his system was able to do a passable job of translating single words between the language. What was impressive is that there was no information saying that, for example, this English document is a translation of that Chinese document. They could have all been pulled randomly from .co.uk and .cn sites. \n\nIf I understand it correctly, Klein's system simply looked for common patterns of words within documents of one language and then tried to match those patterns to similar patterns in the documents of the other. When the languages were "closer" - such as with Spanish and English - the system also made use of patterns of letters within words (e.g., "<i>direc</i>tion" and "<i>direc</i>ción") to find possible cognates. \n\n[Dan Klein]: http://www.cs.berkeley.edu/~klein/\n\nThere was a variety of good papers at COLT this year. Of the talks I saw, two stood out for me.\n\n[The True Sample Complexity of Active Learning][balcan hanneke] by Balcan, Hanneke and Wortman showed the importance of choosing the right theoretic model for analysis. In active learning the learner is able to choose which unlabelled examples have their labels revealed. \nIntuitively, one would think that this should make learning easier than the normal supervised learning where the learner has no say in the matter. \n\nPrevious results showed that this was basically not the case. Subtly, those results asked that the active learner achieve a certain error rate but also _verify_ that that rate was achieved. What Nina and her co-authors showed was that if you remove this extra requirement then active learning does indeed make learning much easier, often with exponential improvements in sample complexity over "passive" learning.\n\n[balcan hanneke]: http://colt2008.cs.helsinki.fi/papers/108-Balcan.pdf\n\n[An Efficient Reduction of Ranking to Classification][ailon] by Ailon and Mohri was also impressive. They build on earlier work that showed how a ranking problem can be reduced to learning a binary preference relation between the items to be ranked. One crucial part of this reduction is turning a learnt preference relation into a ranking. That is, taking pair-wise assessments of relative item quality and laying out all those items along a line so as to best preserve those pair-wise relationships. \n\nWhat Ailon and Mohri show is that simply applying a randomised quicksort to the pair-wise comparisons for n items will give a good reduction to a ranking in O(n log n) time. "Good" here means that the regret of the reduction-based ranking over the best possible is bounded by the regret of the classifier that learns the preference relation over the best possible classification. Furthermore, if you are only interested in the top k of n items you can get a good ranking in O(k log k + n) time. What's particularly nice about this work is the tools they use to analysis randomised quicksort are very general and will probably find other applications.\n\n[ailon]: http://colt2008.cs.helsinki.fi/papers/32-Ailon.pdf\n\nFinally, while I didn't attend the talk at COLT, a couple of people have told me that Abernethy et al.'s paper [Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization][abernethy] was very good. I've since skimmed through it and it is a very nice paper, well-written and replete with interesting connections. I'm not that familiar with bandit learning work but this paper is has a good summary of recent results and is intriguing enough to make me want to investigate further.\n\n[sridharan]: http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf \n[abernethy]: http://colt2008.cs.helsinki.fi/papers/123-Abernethy.pdf\n\n
55	Prediction Markets	47-revision-2	2008-07-27 23:23:13	2008-07-27 23:23:13		inherit	[Robin Hanson][] gave a great introduction to prediction markets, describing how they can be used to extract information through the use of market scoring rules. Essentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning 
56	Prediction Markets	47-revision-3	2008-08-04 11:34:53	2008-08-04 11:34:53		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nMarket Scoring Rules\n------------------------\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n[book and market maker]: http://blog.commerce.net/?p=251
57	Prediction Markets	47-revision-4	2008-08-04 12:12:29	2008-08-04 12:12:29		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\nExample: tossing a coin with unknown bias. A square scoring rule pays (1-r)^2 or (r-0)^2 depending on the outcome. To maximise your expected return you will report an r as close to what you think the true probability is as possible.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n[book and market maker]: http://blog.commerce.net/?p=251
58	Prediction Markets	47-revision-5	2008-08-05 04:27:22	2008-08-05 04:27:22		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\nExample: tossing a coin with unknown bias. A square scoring rule pays (1-r)^2 or (r-0)^2 depending on the outcome. To maximise your expected return you will report an r as close to what you think the true probability is as possible.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251
59	Prediction Markets	47-revision-6	2008-08-05 06:56:12	2008-08-05 06:56:12		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\nExample: tossing a coin with unknown bias. A square scoring rule pays (1-r)^2 or (r-0)^2 depending on the outcome. To maximise your expected return you will report an r as close to what you think the true probability is as possible.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), 
60	Prediction Markets	47-revision-7	2008-08-05 06:58:36	2008-08-05 06:58:36		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\nExample: tossing a coin with unknown bias. A square scoring rule pays (1-r)^2 or (r-0)^2 depending on the outcome. To maximise your expected return you will report an r as close to what you think the true probability is as possible.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)
61	Prediction Markets	47-revision-8	2008-08-05 23:55:25	2008-08-05 23:55:25		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\nExample: tossing a coin with unknown bias. A square scoring rule pays (1-r)^2 or (r-0)^2 depending on the outcome. To maximise your expected return you will report an r as close to what you think the true probability is as possible.\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
62	Prediction Markets	47-revision-9	2008-08-06 00:25:49	2008-08-06 00:25:49		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say <math>r</math> -- and then I toss the coin. If it comes up heads then I pay you <math>1-(1-r)^2</math> \n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible.\n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
63	Prediction Markets	47-revision-10	2008-08-06 00:27:06	2008-08-06 00:27:06		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex> otherwise I pay you <math>1 - r^2</math>.\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible.\n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
64	Prediction Markets	47-revision-11	2008-08-06 00:39:22	2008-08-06 00:39:22		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as\n[tex]\ndisplaymath \ns(r) = (1-(1-r)^2, 1-r^2).\n[/tex]\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as\n[tex]\ns(r)(w) = left<(1-(1-r)^2, 1-r^2), (1-w, w)right> = (1-(1-r)(1-w)\n[/tex]\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
65	Prediction Markets	47-revision-12	2008-08-06 00:40:43	2008-08-06 00:40:43		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector\n[tex]\ns(r) = (1-(1-r)^2, 1-r^2).\n[/tex]\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as\n[tex]\ns(r)(w) = left< (1-(1-r)^2, 1-r^2), (1-w, w) right> = (1-(1-r)^2)(1-w) + (1-r^2)w.\n[/tex]\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
66	Prediction Markets	47-revision-13	2008-08-06 00:45:28	2008-08-06 00:45:28		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = [1-(1-r)^2, 1-r^2].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex]\ndisplaystyle\ns(r)(w) = < [1-(1-r)^2, 1-r^2], [w, 1-w] > = (1-(1-r)^2)(1-w) + (1-r^2)w.\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
69	Prediction Markets	47-revision-16	2008-08-06 00:48:21	2008-08-06 00:48:21		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = [1-(1-r)^2, 1-r^2].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] displaystyle s(r)(w) = 1-(1-r)^2 , 1-r^2 ] [w, 1-w] = (1-(1-r)^2)w + (1-r^2)(1-w). [/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
67	Prediction Markets	47-revision-14	2008-08-06 00:45:57	2008-08-06 00:45:57		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = [1-(1-r)^2, 1-r^2].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex]\ndisplaystyle\ns(r)(w) = < [1-(1-r)^2, 1-r^2], [w, 1-w] > = (1-(1-r)^2)w + (1-r^2)(1-w).\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
68	Prediction Markets	47-revision-15	2008-08-06 00:46:35	2008-08-06 00:46:35		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = [1-(1-r)^2, 1-r^2].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex]\ndisplaystyle\ns(r)(w) = [1-(1-r)^2, 1-r^2] cdot [w, 1-w] = (1-(1-r)^2)w + (1-r^2)(1-w).\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
70	Prediction Markets	47-revision-17	2008-08-06 00:48:34	2008-08-06 00:48:34		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = [1-(1-r)^2, 1-r^2].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] displaystyle s(r)(w) = [ 1-(1-r)^2 , 1-r^2 ] [ w , 1-w ] = (1-(1-r)^2)w + (1-r^2)(1-w). [/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
73	Prediction Markets	47-revision-20	2008-08-06 00:50:52	2008-08-06 00:50:52		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] s(r)(w) = left[ 1-(1-r)^2 , 1-r^2 right] left[ w , 1-w right] \n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
71	Prediction Markets	47-revision-18	2008-08-06 00:49:21	2008-08-06 00:49:21		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] displaystyle s(r)(w) = left[ 1-(1-r)^2 , 1-r^2 right] left[ w , 1-w right] = (1-(1-r)^2)w + (1-r^2)(1-w). [/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
72	Prediction Markets	47-revision-19	2008-08-06 00:49:52	2008-08-06 00:49:52		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] s(r)(w) = left[ 1-(1-r)^2 , 1-r^2 right] left[ w , 1-w right] = (1-(1-r)^2)w + (1-r^2)(1-w). [/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
74	Prediction Markets	47-revision-21	2008-08-06 00:51:16	2008-08-06 00:51:16		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] s(r)(w) = left[ 1-(1-r)^2 , 1-r^2 right] left[ w , 1-w right] [/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
75	Prediction Markets	47-revision-22	2008-08-06 00:52:51	2008-08-06 00:52:51		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{eqnarray}\ns(r)(w) & = langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle [/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
76	Prediction Markets	47-revision-23	2008-08-06 00:53:21	2008-08-06 00:53:21		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{eqnarray}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w). \nend{eqnarray}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
77	Prediction Markets	47-revision-24	2008-08-06 00:53:54	2008-08-06 00:53:54		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{eqnarray}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & x. \nend{eqnarray}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
78	Prediction Markets	47-revision-25	2008-08-06 00:54:47	2008-08-06 00:54:47		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{eqnarray*}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & x + y. \nend{eqnarray*}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
79	Prediction Markets	47-revision-26	2008-08-06 00:55:45	2008-08-06 00:55:45		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & x + y. \nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected \n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
80	Prediction Markets	47-revision-27	2008-08-06 01:19:09	2008-08-06 01:19:09		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? \n\n(See Jeffrey[^1] for a discussion of betting arguments for probability).\n\nScoring rules are a class of reward schemes that encourage truthful reporting. \n\n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads you will see that\n<center>\n[tex]\nmathbb{E}_p[ s(r)\n[/tex]\n</center>\n\n\n\nLambert et al.[^2] beautifully characterise which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. I'll write more about this in another post.\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076) N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
81	Prediction Markets	47-revision-28	2008-08-06 01:40:02	2008-08-06 01:40:02		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^1] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^2] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely but not the true probability [tex]p[/tex]\n\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
82	Prediction Markets	47-revision-29	2008-08-06 01:40:16	2008-08-06 01:40:16		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nPrediction Markets\n---------------------\nSuppose you really wanted to know whether or not\n\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future.\n\nEssentially, people trade in contracts such as "Pays $1 if it rains next Monday". If you're 100% sure it will rain that day then that contract is worth $1 to you. If you think there is a 30% chance of rain then the contract's expected value is $0.30 to you. If you think my guess at the chance of rain is wrong then you'll be willing to a pay different amount and can buy it off me for that price. As this process continues the price of the contract will reflect the true chance of rain as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^1] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^2] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\n[robin hanson]: http://hanson.gmu.edu/\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
83	Prediction Markets and Scoring Rules	47-revision-30	2008-08-06 02:37:52	2008-08-06 02:37:52		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nMarkets and Prediction\n--------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. \n\nInstead of stocks that pay dividends, participants in predication markets trade in contracts about future events. For example we can consider the contract "Pays $1 to bearer if it rains next Monday". If I'm 50% sure it will rain that day then the expected value of that contract to me $0.50. If you think there is a 30% chance of rain then the contract's expected value for you is $0.30. \n\nSuppose you own the contract and I offer you $0.50 for it. You would happily trade it since you would then bank the $0.50 which is 20 cents more than you expected to receive from keeping the contract until maturity.\n\nThe key idea of prediction markets is that if you allow many people to trade contracts on future events the market price of the contract will reflect the true chance of the event as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^1] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^2] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
84	Prediction Markets and Scoring Rules	47-revision-31	2008-08-06 07:08:52	2008-08-06 07:08:52		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nMarkets and Prediction\n--------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. \n\nInstead of stocks that pay dividends, participants in predication markets trade in contracts about future events. For example we can consider the contract "Pays $1 to bearer if it rains next Monday". If I'm 50% sure it will rain that day then the expected value of that contract to me $0.50. If you think there is a 30% chance of rain then the contract's expected value for you is $0.30. \n\nSuppose you own the contract and I offer you $0.50 for it. You would happily trade it since you would then bank the $0.50 which is 20 cents more than you expected to receive from keeping the contract until maturity.\n\nThe key idea of prediction markets is that if you allow many people to trade contracts on future events the market price of the contract will reflect the true chance of the event as more and more information is brought to bear on the prediction problem.\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^1] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^2] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a similar analysis of Hanson's logarithmic market scoring rule that helped me understand the example I pre\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
85	Prediction Markets and Scoring Rules	47-revision-32	2008-08-06 07:10:26	2008-08-06 07:10:26		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through [market scoring rules][]. I've been investigating certain aspects of "vanilla" [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n[market scoring rules]: http://www.midasoracle.org/2007/09/16/hansons-market-scoring-rule-explained-in-five-sentences-why-betfair-gets-so-little-us-press-coverage-and-other-half-baked-commentary-by-michael-giberson/\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nMarkets and Prediction\n--------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. \n\nInstead of stocks that pay dividends, participants in predication markets trade in contracts about future events. For example we can consider the contract "Pays $1 to bearer if it rains next Monday". If I'm 50% sure it will rain that day then the expected value of that contract to me $0.50. If you think there is a 30% chance of rain then the contract's expected value for you is $0.30. \n\nSuppose you own the contract and I offer you $0.50 for it. You would happily trade it since you would then bank the $0.50 which is 20 cents more than you expected to receive from keeping the contract until maturity.\n\nThe key idea of prediction markets is that if you allow many people to trade contracts on future events the market price of the contract will reflect the true chance of the event as more and more information is brought to bear on the prediction problem.\n\nThe trick as a "market maker" is to set pr\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^1] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^2] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^2]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
86	Prediction Markets and Scoring Rules	47-revision-33	2008-08-07 00:09:16	2008-08-07 00:09:16		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nMarkets and Prediction\n--------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. \n\nInstead of stocks that pay dividends, participants in predication markets trade in contracts about future events. For example we can consider the contract "Pays $1 to bearer if it rains next Monday". If I'm 50% sure it will rain that day then the expected value of that contract to me $0.50. If you think there is a 30% chance of rain then the contract's expected value for you is $0.30. \n\nSuppose you own the contract and I offer you $0.50 for it. You would happily trade it since you would then bank the $0.50 which is 20 cents more than you expected to receive from keeping the contract until maturity.\n\nThe key idea of prediction markets is that if you allow many people to trade contracts on future events the market price of the contract will reflect the true chance of the event as more and more information is brought to bear on the prediction problem.\n\n[TODO: The trick as a "market maker" is to be able to update the prices you set after each trade in order to converge to the true probability of an event.]\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^2] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^3]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
87	Prediction Markets and Scoring Rules	47-revision-34	2008-08-07 06:52:12	2008-08-07 06:52:12		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nMarkets and Prediction\n--------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. \n\nInstead of stocks that pay dividends, participants in predication markets trade in contracts about future events. For example we can consider contracts for whether or not it rains next Monday. For a binary event like this the contracts come in the pair: A) "Pays $1 to bearer if it rains next Monday" and B) "Pays $1 to bearer if it does not rain next Monday". If I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50. If you buy one of B) it suggests that you think the chance of rain is more than 0.5. In this case I should update my prices to reflect what this trade has told me about what you think the chances are. If you buy another, I should raise my price slightly again. Continuing this, eventually I'll reach a price where \n\nSuppose you own the contract and I offer you $0.50 for it. You would happily trade it since you would then bank the $0.50 which is 20 cents more than you expected to receive from keeping the contract until maturity.\n\nThe key idea of prediction markets is that if you allow many people to trade contracts on future events the market price of the contract will reflect the true chance of the event as more and more information is brought to bear on the prediction problem.\n\n[TODO: The trick as a "market maker" is to be able to update the prices you set after each trade in order to converge to the true probability of an event.]\n\nScoring Rules\n---------------\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^2] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^3]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
88	Prediction Markets and Scoring Rules	47-revision-35	2008-08-07 12:23:06	2008-08-07 12:23:06		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement was by John McCarthy[^2] in 1956 and a more in depth was given by Savage[^3] in 1971.\n\nA central concept in forecasting is that of _elicitation_. How do you ensure that people report probabilities that reflect what they really believe? One answer was given by Savage[^2] in 1971 in the form of _proper scoring rules_ --- a class of reward schemes that encourage truthful reporting. \nThe gist of the idea can be seen through a simple example using a particular proper scoring rule called the Brier score. \n\nSuppose I was about to toss a coin that only you knew had a probability <math>p</math> of landing heads. How could I encourage you to reveal that probability to me? \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^3]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
89	Prediction Markets and Scoring Rules	47-revision-36	2008-08-07 12:33:42	2008-08-07 12:33:42		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as _proper scoring rules_ was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971.\n\n\n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\n[Subjective Probability: The Real Thing](http://www.princeton.edu/~bayesway/Book*.pdf), [Review](http://ndpr.nd.edu/review.cfm?id=4401)\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
90	Prediction Markets and Scoring Rules	47-revision-37	2008-08-08 01:00:49	2008-08-08 01:00:49		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as _proper scoring rules_ was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971.\n\n\n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
92	Prediction Markets and Scoring Rules	47-revision-39	2008-08-10 23:28:15	2008-08-10 23:28:15		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as _proper scoring rules_ was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert et al.[^4]\n\nA scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function that computes the expected value of a correct prediction --- [tex]s_1(r)[/tex] --- and an incorrect prediction --- [tex]s_0(r)[/tex] --- from a reported probability [tex]r[/tex] of an event. This simplifies the game of gradually increasing the cost of the contracts as more are bought to a simple offer of a payoff for a reported probability. The key feature of a _proper_ scoring rule is that its expected value is maximised when the true probability of an event is reported. That is, if [tex]pin[0,1][/tex] is the true probability of an event then\n<center>\n[tex]\ndisplaystyle\nmax_{rin[0,1]} mathbb{E}_p[ s(r) ] = mathbb{E}_p[ s(p) ].\n[/tex]\n</center> \n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
91	Prediction Markets and Scoring Rules	47-revision-38	2008-08-10 23:25:39	2008-08-10 23:25:39		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as _proper scoring rules_ was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert et al.[^4]\n\nA scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function that computes the expected value of a correct ([tex]s_1(r)[/tex]) and an incorrect ([tex]s_0(r)[/tex]) prediction from a reported probability [tex]r[/tex] of an event. This simplifies the game of gradually increasing the cost of the contracts as more are bought to a simple offer of a payoff for a reported probability. The key feature of a _proper_ scoring rule is that its expected value is maximised when the true probability of an event is reported. That is, if [tex]pin[0,1][/tex] is the true probability of an event then\n<center>\n[tex]\ndisplaystyle\nmax_rin[0,1] mathbb{E}_p[ s(r) ] = mathbb{E}_p[ s(p) ]\n[/tex]\n</center> \n\nOne way is if I set up the following wager: you first tell me the probability of heads -- say [tex]r[/tex] -- and then I toss the coin. If it comes up heads then I pay you [tex]1-(1-r)^2[/tex] dollars otherwise I pay you [tex]1 - r^2[/tex] dollars. When a wager is dependent on your report [tex]r[/tex] like this it is known as a _scoring rule_ and can be summarised as the vector function\n[tex]\ns(r) = left[ 1-(1-r)^2 , 1-r^2 right].\n[/tex]\n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
93	Prediction Markets and Scoring Rules	47-revision-40	2008-08-10 23:28:41	2008-08-10 23:28:41		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as _proper scoring rules_ was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert et al.[^4]\n\nA scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function that computes the expected value of a correct prediction --- [tex]s_1(r)[/tex] --- and an incorrect prediction --- [tex]s_0(r)[/tex] --- from a reported probability [tex]r[/tex] of an event. This simplifies the game of gradually increasing the cost of the contracts as more are bought to a simple offer of a payoff for a reported probability. The key feature of a _proper_ scoring rule is that its expected value is maximised when the true probability of an event is reported. That is, if [tex]pin[0,1][/tex] is the true probability of an event then\n<center>\n[tex]\ndisplaystyle\nmax_{rin [0,1]} mathbb{E}_p [ s(r) ] = mathbb{E}_p [ s(p) ].\n[/tex]\n</center> \n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
94	Scoring Rules and Prediction Markets	47-revision-41	2008-08-11 10:44:25	2008-08-11 10:44:25		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as _proper scoring rules_ was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert <i>et al.</i>[^4]\n\nFor a single binary event, a scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function of a _report_ [tex]r = [r_0, r_1][/tex] of the probabilities for that event. If you report [tex]r[/tex] and the event occurs you are paid [tex]s_1(r)[/tex]. If the event does not occur you are paid [tex]s_0(r)[/tex]. A convenient shorthand is to let [tex]w_1[/tex] be a random variable that is 1 if the event occurs and 0 otherwise. Then the payment from the scoring rule for a given report [tex]r[/tex] is the inner product\n[tex]langle s(r), w rangle[/tex] where [tex]w = [1-w_1, w_1][/tex].\n\nIf you know the scoring rule I use in advance then the game of gradually increasing the cost of the contracts as you buy more can be simplified. Now you just report the probabilities you believe will maximise what I will pay you using the scoring rule. \n\nIn order to ensure you report what you really believe to be the true probabilities I need to construct the scoring rule in such a way that its expected payoff is maximised when you report truthfully. That is, if [tex]p[/tex] is the true probability of the event occurring (<i>i.e.</i>, [tex]w_1 = 1[/tex]) then\n<center>\n[tex]\ndisplaystyle\nmax_{r} mathbb{E}_p [ s(r) ] = mathbb{E}_p [ s(p) ].\n[/tex]\n</center> \n\nIf the random variable [tex]w[/tex] is 1 when the coin lands heads and 0 for tails, we can write the payment you will receive as a projection onto [1,0] for heads or [0,1] for tails.\n<center>\n[tex] \ndisplaystyle\nbegin{array}{rcl}\ns(r)(w) & = & langle left[ 1-(1-r)^2 , 1-r^2 right], left[ w , 1-w right] rangle \\\n          & = & (1-(1-r)^2)w + (1-r^2)(1-w).\nend{array}\n[/tex]\n</center>\n\nTo maximise your expected return you will report an r as close to what you think the true probability is as possible. Why is this? Well, if you write out the expected return under the assumption that [tex]p[/tex] is the true probability of heads (i.e., w = 1), with a bit of algebra you will see that\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = p^2 - p + 1 - (p - r)^2\n[/tex]\n</center>\nwhich is clearly maximised only when p = r. That is, you maximise your expected payment when your report of the probability of heads is equal to the true probability of heads.\n\nAs noted earlier, the Brier score is one of a whole class of proper scoring rules defined by the property that they are maximised by reporting the true probability for an event.\nIt turns out that this class of functions has quite a lot of structure. Recently, Lambert et al.[^3] have characterised which scoring rules are proper and go further to describe what general properties of distributions can be elicited using proper scoring rules. It's a very nice piece of work which I'll write more about in later post.\n\nAs a quick aside, the linear scoring rule [tex]s(r) = [r, 1-r][/tex] would appear to be a simpler and more natural alternative to the Brier score for elicitation but it is, in fact, not a proper scoring rule. This is easy to see since its expectation is\n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p[ s(r)(w) ] = 1 - p + r(2p - 1).\n[/tex]\n</center>\nIf [tex]p > 0.5[/tex] then [tex]2p-1 > 0[/tex] and so this quantity is maximised by choosing [tex]r = 1[/tex]. Alternatively, if [tex]p < 0.5[/tex] it is maximised by [tex]r = 0[/tex]. This means that this rule would elicit a correct _classification_ of whether heads is more likely than tails or not but will not elicit the true probability [tex]p[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
95	Scoring Rules and Prediction Markets	47-revision-42	2008-08-11 11:07:24	2008-08-11 11:07:24		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as "proper scoring rules" was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert <i>et al.</i>[^4]\n\nFor a single binary event, a scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function of a "report" [tex]r = [r_0, r_1][/tex] of the probabilities for that event. If you report [tex]r[/tex] and the event occurs you are paid [tex]s_1(r)[/tex]. If the event does not occur you are paid [tex]s_0(r)[/tex]. \n\nA convenient shorthand is to let [tex]w_1[/tex] be a random variable that is 1 if the event occurs and 0 otherwise. Then the payment from the scoring rule for a given report [tex]r[/tex] is the inner product [tex]langle s(r), w rangle[/tex] where [tex]w = [1-w_1, w_1][/tex]. This is because if [tex]w_1 = 1[/tex] then [tex]w = [0,1][/tex] and so [tex]langle s(r), w rangle = s_1(r)[/tex] and similarly the inner product is [tex]s_0(r)[/tex] if [tex]w_1 = 0[/tex].\n\nIf you know the scoring rule I use in advance then the game of gradually increasing the cost of the contracts as you buy more can be simplified. Now you just report the probabilities you believe will maximise what I will pay you using the scoring rule. \n\nIn order to ensure you report what you really believe to be the true probabilities I need to construct the scoring rule in such a way that your expected payoff is maximised when you report truthfully. That is, if [tex]p = [1-p_1, p_1][/tex] is the true probability distribution for the event then\n<center>\n[tex]\ndisplaystyle\nmax_{r} mathbb{E}_p  langle s(r), w rangle  = mathbb{E}_p  langle s(p), w rangle .\n[/tex]\n</center>\nScoring rules that meet this criteria are described as "proper" or "Fisher consistent".\n\nThe reason the inner product notation is a useful shorthand is that, thanks to its linearity, we can now pull the expectation inside it to show that \n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p  langle s(r), w rangle \n= langle s(r), mathbb{E}_p  w rangle\n= langle s(r), p rangle\n[/tex]\n<center>\nsince [tex]mathbb{E}_p w = p[/tex]. If everything is suitably differentiable the Fisher consistency (or "properness") condition can be restated as requiring that the gradient of the scoring rule disappear when\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
96	Scoring Rules and Prediction Markets	47-revision-43	2008-08-11 11:08:21	2008-08-11 11:08:21		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as "proper scoring rules" was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert <i>et al.</i>[^4]\n\nFor a single binary event, a scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function of a "report" [tex]r = [r_0, r_1][/tex] of the probabilities for that event. If you report [tex]r[/tex] and the event occurs you are paid [tex]s_1(r)[/tex]. If the event does not occur you are paid [tex]s_0(r)[/tex]. \n\nA convenient shorthand is to let [tex]w_1[/tex] be a random variable that is 1 if the event occurs and 0 otherwise. Then the payment from the scoring rule for a given report [tex]r[/tex] is the inner product [tex]langle s(r), w rangle[/tex] where [tex]w = [1-w_1, w_1][/tex]. This is because if [tex]w_1 = 1[/tex] then [tex]w = [0,1][/tex] and so [tex]langle s(r), w rangle = s_1(r)[/tex] and similarly the inner product is [tex]s_0(r)[/tex] if [tex]w_1 = 0[/tex].\n\nIf you know the scoring rule I use in advance then the game of gradually increasing the cost of the contracts as you buy more can be simplified. Now you just report the probabilities you believe will maximise what I will pay you using the scoring rule. \n\nIn order to ensure you report what you really believe to be the true probabilities I need to construct the scoring rule in such a way that your expected payoff is maximised when you report truthfully. That is, if [tex]p = [1-p_1, p_1][/tex] is the true probability distribution for the event then\n<center>\n[tex]\ndisplaystyle\nmax_{r} mathbb{E}_p  langle s(r), w rangle  = mathbb{E}_p  langle s(p), w rangle .\n[/tex]\n</center>\nScoring rules that meet this criteria are described as "proper" or "Fisher consistent".\n\nThe reason the inner product notation is a useful shorthand is that, thanks to its linearity, we can now pull the expectation inside it to show that \n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p  langle s(r), w rangle \n= langle s(r), mathbb{E}_p  w rangle\n= langle s(r), p rangle\n[/tex]\n<center>\nsince [tex]mathbb{E}_p w = p[/tex]. If everything is suitably differentiable the Fisher consistency (or "properness") condition can be restated as requiring that the gradient of the scoring rule disappear when [tex]r = p[/tex]. That is, [tex](nabla_r s)(p) = 0[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
97	Scoring Rules and Prediction Markets	47-revision-44	2008-08-11 11:09:38	2008-08-11 11:09:38		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as "proper scoring rules" was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert <i>et al.</i>[^4]\n\nFor a single binary event, a scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function of a "report" [tex]r = [r_0, r_1][/tex] of the probabilities for that event. If you report [tex]r[/tex] and the event occurs you are paid [tex]s_1(r)[/tex]. If the event does not occur you are paid [tex]s_0(r)[/tex]. \n\nA convenient shorthand is to let [tex]w_1[/tex] be a random variable that is 1 if the event occurs and 0 otherwise. Then the payment from the scoring rule for a given report [tex]r[/tex] is the inner product [tex]langle s(r), w rangle[/tex] where [tex]w = [1-w_1, w_1][/tex]. This is because if [tex]w_1 = 1[/tex] then [tex]w = [0,1][/tex] and so [tex]langle s(r), w rangle = s_1(r)[/tex] and similarly the inner product is [tex]s_0(r)[/tex] if [tex]w_1 = 0[/tex].\n\nIf you know the scoring rule I use in advance then the game of gradually increasing the cost of the contracts as you buy more can be simplified. Now you just report the probabilities you believe will maximise what I will pay you using the scoring rule. \n\nIn order to ensure you report what you really believe to be the true probabilities I need to construct the scoring rule in such a way that your expected payoff is maximised when you report truthfully. That is, if [tex]p = [1-p_1, p_1][/tex] is the true probability distribution for the event then\n<center>\n[tex]\ndisplaystyle\nmax_{r} mathbb{E}_p  langle s(r), w rangle  = mathbb{E}_p  langle s(p), w rangle .\n[/tex]\n</center>\nScoring rules that meet this criteria are described as "proper" or "Fisher consistent".\n\nThe reason the inner product notation is a useful shorthand is that, thanks to its linearity, we can now pull the expectation inside it to show that \n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p  langle s(r), w rangle = langle s(r), mathbb{E}_p  w rangle = langle s(r), p rangle\n[/tex]\n<center>\nsince [tex]mathbb{E}_p w = p[/tex]. If everything is suitably differentiable the Fisher consistency (or "properness") condition can be restated as requiring that the gradient of the scoring rule disappear when [tex]r = p[/tex]. That is, [tex](nabla_r s)(p) = 0[/tex].\n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
98	Scoring Rules and Prediction Markets	47-revision-45	2008-08-11 11:53:40	2008-08-11 11:53:40		inherit	[Robin Hanson][] gave a great introduction to prediction markets at [COLT this year][colt]. He covered a range of issues included how prediction markets can be used to aggregate information through "market scoring rules"[^1]. I've been investigating certain aspects of plain [scoring rules][] for a while now so I was curious to understand how they are extended and, more generally, curious about the workings of information markets. \n\n[robin hanson]: http://hanson.gmu.edu/\n[colt]: http://conflate.net/inductio/2008/07/colt-2008-highlights/\n[scoring rules]: http://en.wikipedia.org/wiki/Scoring_rule\n\nThis post is a first attempt at understanding prediction markets and a travelogue of the links and papers I've uncovered along the way. My strongest impression at present is that there is a lot of interesting work going on in this area at the moment. Consequently, what I present here will be -- for my sake -- a very simplified view.\n\nTrading Cash for Probability\n-------------------------------\nPrediction markets are a natural extension of what goes on in financial markets everyday: people buying and selling stocks depending on whether they think a company will make a profit and return dividends sometime in the future. Instead of stocks that pay dividends, participants in predication markets trade in contracts that pay out should a well-defined future event take place. \n\nFor example we can consider contracts for whether or not it rains next Monday at a specific location. For a binary event like this the contracts come in the pair:\n\n* A) "Pays $1 to bearer if it rains next Monday", and \n* B) "Pays $1 to bearer if it does not rain next Monday". \n\nIf I'm 50% sure it will rain that day then the expected values of contract A and B to me are both $0.50. If you think there is a 30% chance of rain then contract A's expected value for you is $0.30 and contract B's value is $0.70. \n\nIf I'm selling these contracts I would set an initial price for both at $0.50, reflecting my belief in the chance of rain. If you buy contract B from me at that price it suggests that you think the chance of rain is less than 0.5 since, if your odds for rain are correct, you stand to make $0.20. \n\nIn the case of such a trade I should update my prices to, say $0.49 for contract A and $0.51 for contract B to reflect the information I've gleaned from your purchase. If you buy another contract for B, I should raise my price slightly again. Let's say I modify the price by $0.01 each time. \n\nContinuing this, I'll reduce your expected gain on each subsequent contract B you buy. After 20 purchases I'll reach prices of $0.30 and $0.70 respectively for contracts A and B. When this happens you will stop purchasing contracts from me since you no longer expect to gain any benefit from holding either.\n\nOnce the process is complete we wait for Monday to see if it rains. If your beliefs are correct then with probability 0.3 you will lose money --- specifically, $(0.50 + 0.51 + ... + 0.69) = $11.90 --- since your 20 copies of contract B will be worthless. However, with probability 0.7 it will rain and your 20 copies of contract B will be worth $20 and you will gain $20 - $11.90 = $8.10. Your expected gain (and my expected loss) if your beliefs are correct is therefore $2.10\n\nAnother way to look at this is that I will expect to pay $2.10 for eliciting your correct belief in the probability of rain.\n\nProper Scoring Rules\n------------------------\nThis idea of eliciting probabilities via incentives such as in the above example has a long history. The first general statement of what are now known as "proper scoring rules" was by John McCarthy[^2] in 1956 and a more in depth study by Leonard Savage[^3] was published in 1971. The presentation of scoring rules I use here is influenced a very recent paper by Lambert <i>et al.</i>[^4]\n\nFor a single binary event, a scoring rule [tex]s(r) = [s_0(r), s_1(r)] [/tex] is a function of a "report" [tex]r = [r_0, r_1][/tex] of the probabilities for that event. If you report [tex]r[/tex] and the event occurs you are paid [tex]s_1(r)[/tex]. If the event does not occur you are paid [tex]s_0(r)[/tex]. \n\nA convenient shorthand is to let [tex]w_1[/tex] be a random variable that is 1 if the event occurs and 0 otherwise. Then the payment from the scoring rule for a given report [tex]r[/tex] is the inner product [tex]langle s(r), w rangle[/tex] where [tex]w = [1-w_1, w_1][/tex]. This is because if [tex]w_1 = 1[/tex] then [tex]w = [0,1][/tex] and so [tex]langle s(r), w rangle = s_1(r)[/tex] and similarly the inner product is [tex]s_0(r)[/tex] if [tex]w_1 = 0[/tex].\n\nIf you know the scoring rule I use in advance then the game of gradually increasing the cost of the contracts as you buy more can be simplified. Now you just report the probabilities you believe will maximise what I will pay you using the scoring rule. \n\nIn order to ensure you report what you really believe to be the true probabilities I need to construct the scoring rule in such a way that your expected payoff is maximised when you report truthfully. That is, if [tex]p = [1-p_1, p_1][/tex] is the true probability distribution for the event then\n<center>\n[tex]\ndisplaystyle\nmax_{r} mathbb{E}_p  langle s(r), w rangle  = mathbb{E}_p  langle s(p), w rangle .\n[/tex]\n</center>\nScoring rules that meet this criteria are described as "proper" or "Fisher consistent".\n\nThe reason the inner product notation is a useful shorthand is that, thanks to its linearity, we can now pull the expectation inside it to show that \n<center>\n[tex]\ndisplaystyle\nmathbb{E}_p  langle s(r), w rangle = langle s(r), mathbb{E}_p  w rangle = langle s(r), p rangle\n[/tex]\n</center>\nsince [tex]mathbb{E}_p w = p[/tex]. If everything is suitably differentiable the Fisher consistency (or "properness") condition requires that the derivatives of the scoring rule satisfy, for all [tex]p[/tex],\n<center>\n[tex]\ndisplaystyle\nlangle frac{partial}{partial r_i} s(p), p rangle = 0.\n[/tex]\n</center>\nThat means the derivatives of the scoring rule must be orthogonal to [tex]p[/tex]. \n\nMarket Scoring Rules\n------------------------\n\nExample: suppose someone else thinks you guess is wrong. What is she willing to pay to get an expected return? This can go on as long as there is a perceived discrepancy between the current guess and someone else's.\n\nDavid Pennock has a [similar analysis][pennock] of Hanson's logarithmic market scoring rule that helped me understand market scoring rules enough to present the (hopefully simpler) example I here.\n\n[pennock]: http://blog.oddhead.com/2006/10/30/implementing-hansons-market-maker/\n\n[This leads to telescoping rule for MSRs]\n\nThe enticement of a possible reward acts as an incentive to find out more about the coin and its bias. People might study coins similar to the one thrown, learn about defects in their manufacture that might impart a bias, look at the history of the person throwing it, their star sign, etc.\n\nI asked Robin a pretty naïve question while speaking to him after his talk: How do these markets get started since someone has to pay the contracts out when they mature? The answer is "the person who wants the information". Such a person sets the initial prices of the contracts to reflect their beliefs about some events and then any improvement in accuracy for the probabilities for those events is converted to money when the contracts are paid out.\n\nPrediction Markets in the Wild\n----------------------------------\n\nThese markets can get quite sophisticated and keeping track of combinations of contracts can get tricky. [David Pennock][] is doing some nice work in this area and has even implemented some of his ideas as a Facebook betting application called [Yoopick][]. He also had a number of really good papers in the [ACM conference on electronic commerce][ec08] that was running at the same time as COLT but in Chicago.\n\nAnother site using prediction markets is [hubdub][]. Here people can bet "play money" on various types of news coverage.\n\n[hubdub]: http://www.hubdub.com/\n[david pennock]: http://dpennock.com/\n[yoopick]: http://blog.oddhead.com/2008/07/03/yoopick-a-sports-prediction-contest-on-facebook-with-a-research-twist/\n[ec08]: http://www.sigecom.org/ec08/\n\nThoughts on information become a commodity. Machine learning will make certain simple types of decision making a commodity too (analogy: human habits and instinctual behaviour leaves the mind free for higher-order planning and decision-making).\n\nResearch shows that in the areas they have been used prediction markets are [powerful][].\n\n[John][] recently pointed out the [electoralmarkets][] site that takes data from [Intrade][] to track, state-by-state, the predicted results of the upcoming US federal election.\n\n[powerful]: http://artificialmarkets.com/\n[electoralmarkets]: http://www.electoralmarkets.com/\n[john]: http://hunch.net/?p=396\n[intrade]: http://www.intrade.com/\n\nDavid Pennock puts forward a [convincing argument][pam] that the so-called "terrorism market" was not as bad an idea as I first though. The main points of David's argument is: the terrorist activities made up a tiny part of contracts for events in the Middle East; terrorists could not get rich playing this market since bets were limited to $100 making it more effective for them to trade on the financial markets' reaction to terrorism on airline and oil companies; we bet against bad things happening to us when we take out insurance.\n\n[pam]: http://dpennock.com/pam.html\n\n\n[book and market maker]: http://blog.commerce.net/?p=251\n\nReferences\n------------\n[^1]: [Combinatorial Information Market Design](http://www.citeulike.org/user/mdreid/article/3093106), R. Hanson, Information Systems Frontiers pp. 107-119 (2003).\n[^2]: [Measures of the Value of Information](http://www.citeulike.org/user/mdreid/article/3095794), J. Mccarthy, Proceedings of the National Academy of Sciences of the United States of America 42, 654 (1956).\n[^3]: [Elicitation of Personal Probabilities and Expectations](http://www.citeulike.org/user/mdreid/article/2309030), L. J. Savage, Journal of the American Statistical Association 66, 783 (1971).\n[^4]: [Elicitability](http://www.citeulike.org/user/mdreid/article/3026076), N. Lambert, D. Pennock, Y. Shoham, Proceedings of the ACM Conference on Electronic Commerce (2008).
99	Prediction and the Axiom of Choice	prediction-and-the-axiom-of-choice	2008-09-22 21:55:08	2008-08-29 01:32:51	Some thoughts on Hardin and Taylor's paper "A Peculiar Connection Between the Axiom of Choice and Predicting the Future".	publish	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIts main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem][] --- otherwise known as the [Axiom of Choice][]. \n\n[well-ordering theorem]: http://en.wikipedia.org/wiki/Well-ordering_theorem\n[axiom of choice]: http://en.wikipedia.org/wiki/Axiom_of_choice\n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member, say u, of [v]<sub>t</sub> with respect to the ordering and outputs u(t).\n\nSo, apart for the bit where you have to invoke the well-ordering theorem to order the set of all functions over the reals, it's a very simple strategy.\n\nThe author's suggest that the chosen well-ordering can be thought of as a measure of simplicity. In this case the µ-strategy just chooses the "simplest" function consistent with the observations to date. More generally, the well-ordering can be thought of as a bias for the strategy. Different choices of bias will lead to different predictions based on the same past observations. What's odd about the result is that *no matter what bias is chosen the µ-strategy will only ever make countably many mistakes*.\n\nUltimately, the [intuitionist][] in me looks upon this result much as I see the [Banach-Tarski paradox][]. That is, as evidence against the use of the axiom of choice (and its equivalents) in mathematics that's even vaguely practical. Still, the result is an interesting one that analyses the problem of induction in a very abstract setting.\n\n[banach-tarski paradox]: http://en.wikipedia.org/wiki/Banach-Tarski_paradox\n[intuitionist]: http://conflate.net/inductio/2008/06/constructive-and-classical-mathematics/
100	Prediction and the Axiom of Choice	99-revision	2008-08-25 09:18:02	2008-08-25 09:18:02		inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via \n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html
101	Prediction and the Axiom of Choice	99-revision-2	2008-08-27 10:03:28	2008-08-27 10:03:28		inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nThe well-ordering acts as a bias (in the sense of Mitchell) but it is non-constructible. The theorem says that any choice of bias will lead to good prediction since the µ-strategy works for any choice of ordering.\n\nOne way to look at this is evidence against the use of the axiom of choice (Zorn's lemma / well-ordering principle), adding to the Banach-Tarski paradox. 
102	Prediction and the Axiom of Choice	99-revision-3	2008-08-28 08:03:28	2008-08-28 08:03:28		inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIt's main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem](http://en.wikipedia.org/wiki/Well-ordering_theorem) --- otherwise known as the [Axiom of Choice](http://en.wikipedia.org/wiki/Axiom_of_choice). \n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member of [v]<sub>t</sub> with respect to\n\nThe well-ordering acts as a bias (in the sense of Mitchell) but it is non-constructible. The theorem says that any choice of bias will lead to good prediction since the µ-strategy works for any choice of ordering.\n\nOne way to look at this is evidence against the use of the axiom of choice (Zorn's lemma / well-ordering principle), adding to the Banach-Tarski paradox. 
103	Prediction and the Axiom of Choice	99-revision-4	2008-08-28 08:09:00	2008-08-28 08:09:00		inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIt's main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem](http://en.wikipedia.org/wiki/Well-ordering_theorem) --- otherwise known as the [Axiom of Choice](http://en.wikipedia.org/wiki/Axiom_of_choice). \n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member, say u, of [v]<sub>t</sub> with respect to the ordering and outputs u(t).\n\nSo, apart for the bit where you have to invoke the well-ordering theorem to order the set of all functions over the reals, it's a very simple strategy.\n\nThe well-ordering acts as a bias (in the sense of Mitchell) but it is non-constructible. The theorem says that any choice of bias will lead to good prediction since the µ-strategy works for any choice of ordering.\n\nUltimately, the [intuitionist][] in me looks upon this result much as I see the [Banach-Tarski paradox][] --- evidence against the use of the axiom of choice (and its equivalents) in matheme\nOne way to look at this is evidence against the use of the axiom of choice (Zorn's lemma / well-ordering principle), adding to the. \n\n[intuitionist]: http://conflate.net/inductio/2008/06/constructive-and-classical-mathematics/
104	Prediction and the Axiom of Choice	99-revision-5	2008-08-29 01:27:30	2008-08-29 01:27:30		inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIt's main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem][] --- otherwise known as the [Axiom of Choice][]. \n\n[well-ordering theorem]: http://en.wikipedia.org/wiki/Well-ordering_theorem\n[axiom of choice]: http://en.wikipedia.org/wiki/Axiom_of_choice\n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member, say u, of [v]<sub>t</sub> with respect to the ordering and outputs u(t).\n\nSo, apart for the bit where you have to invoke the well-ordering theorem to order the set of all functions over the reals, it's a very simple strategy.\n\nThe author's suggest that the chosen well-ordering can be thought of as a measure of simplicity. In this case the µ-strategy just chooses the "simplest" function consistent with the observations to date. More generally, the well-ordering can be thought of as a bias for the strategy. Different choices of bias will lead to different predictions based on the same past observations. What's odd about the result is that *no matter what bias is chosen the µ-strategy will only ever make countably many mistakes*.\n\nUltimately, the [intuitionist][] in me looks upon this result much as I see the [Banach-Tarski paradox][]. That is, as evidence against the use of the axiom of choice (and its equivalents) in mathematics that's even vaguely practical. Still, the paper is a \n\n[intuitionist]: http://conflate.net/inductio/2008/06/constructive-and-classical-mathematics/
105	Prediction and the Axiom of Choice	99-revision-6	2008-08-29 01:28:18	2008-08-29 01:28:18		inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIt's main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem][] --- otherwise known as the [Axiom of Choice][]. \n\n[well-ordering theorem]: http://en.wikipedia.org/wiki/Well-ordering_theorem\n[axiom of choice]: http://en.wikipedia.org/wiki/Axiom_of_choice\n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member, say u, of [v]<sub>t</sub> with respect to the ordering and outputs u(t).\n\nSo, apart for the bit where you have to invoke the well-ordering theorem to order the set of all functions over the reals, it's a very simple strategy.\n\nThe author's suggest that the chosen well-ordering can be thought of as a measure of simplicity. In this case the µ-strategy just chooses the "simplest" function consistent with the observations to date. More generally, the well-ordering can be thought of as a bias for the strategy. Different choices of bias will lead to different predictions based on the same past observations. What's odd about the result is that *no matter what bias is chosen the µ-strategy will only ever make countably many mistakes*.\n\nUltimately, the [intuitionist][] in me looks upon this result much as I see the [Banach-Tarski paradox][]. That is, as evidence against the use of the axiom of choice (and its equivalents) in mathematics that's even vaguely practical. Still, the result is an interesting one that analyses the problem of induction in a very abstract setting.\n\n[intuitionist]: http://conflate.net/inductio/2008/06/constructive-and-classical-mathematics/
106	Prediction and the Axiom of Choice	99-revision-7	2008-08-29 01:30:19	2008-08-29 01:30:19	Some thoughts on Hardin and Taylor's paper "A Peculiar Connection Between the Axiom of Choice and Predicting the Future".	inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIt's main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem][] --- otherwise known as the [Axiom of Choice][]. \n\n[well-ordering theorem]: http://en.wikipedia.org/wiki/Well-ordering_theorem\n[axiom of choice]: http://en.wikipedia.org/wiki/Axiom_of_choice\n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member, say u, of [v]<sub>t</sub> with respect to the ordering and outputs u(t).\n\nSo, apart for the bit where you have to invoke the well-ordering theorem to order the set of all functions over the reals, it's a very simple strategy.\n\nThe author's suggest that the chosen well-ordering can be thought of as a measure of simplicity. In this case the µ-strategy just chooses the "simplest" function consistent with the observations to date. More generally, the well-ordering can be thought of as a bias for the strategy. Different choices of bias will lead to different predictions based on the same past observations. What's odd about the result is that *no matter what bias is chosen the µ-strategy will only ever make countably many mistakes*.\n\nUltimately, the [intuitionist][] in me looks upon this result much as I see the [Banach-Tarski paradox][]. That is, as evidence against the use of the axiom of choice (and its equivalents) in mathematics that's even vaguely practical. Still, the result is an interesting one that analyses the problem of induction in a very abstract setting.\n\n[intuitionist]: http://conflate.net/inductio/2008/06/constructive-and-classical-mathematics/
107	Prediction and the Axiom of Choice	99-revision-8	2008-08-29 01:32:22	2008-08-29 01:32:22	Some thoughts on Hardin and Taylor's paper "A Peculiar Connection Between the Axiom of Choice and Predicting the Future".	inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIt's main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem][] --- otherwise known as the [Axiom of Choice][]. \n\n[well-ordering theorem]: http://en.wikipedia.org/wiki/Well-ordering_theorem\n[axiom of choice]: http://en.wikipedia.org/wiki/Axiom_of_choice\n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member, say u, of [v]<sub>t</sub> with respect to the ordering and outputs u(t).\n\nSo, apart for the bit where you have to invoke the well-ordering theorem to order the set of all functions over the reals, it's a very simple strategy.\n\nThe author's suggest that the chosen well-ordering can be thought of as a measure of simplicity. In this case the µ-strategy just chooses the "simplest" function consistent with the observations to date. More generally, the well-ordering can be thought of as a bias for the strategy. Different choices of bias will lead to different predictions based on the same past observations. What's odd about the result is that *no matter what bias is chosen the µ-strategy will only ever make countably many mistakes*.\n\nUltimately, the [intuitionist][] in me looks upon this result much as I see the [Banach-Tarski paradox][]. That is, as evidence against the use of the axiom of choice (and its equivalents) in mathematics that's even vaguely practical. Still, the result is an interesting one that analyses the problem of induction in a very abstract setting.\n\n[banach-tarski paradox]: http://en.wikipedia.org/wiki/Banach-Tarski_paradox\n[intuitionist]: http://conflate.net/inductio/2008/06/constructive-and-classical-mathematics/
109	Structured Machine Learning: The Next Ten Years		2008-09-03 04:35:36	0000-00-00 00:00:00		draft	\n\n[paper]: http://dx.doi.org/10.1007/s10994-008-5079-1
110	Structured Machine Learning: The Next Ten Years	109-revision	2008-09-03 04:35:20	2008-09-03 04:35:20		inherit	
111	A Year of Research Blogging	a-year-of-research-blogging	2008-09-22 06:59:46	2008-09-22 07:00:09	Looking back on a year of research blogging about machine learning.	publish	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've had almost 6,000 visits and over 10,000 page views.\n\n<div class="center">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" />\n</div>\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php). It's my most read post with over 2,500 views.\n\nThe second and third most popular posts were on [prediction and the axiom of choice](http://conflate.net/inductio/2008/08/prediction-and-the-axiom-of-choice/) with about 1,300 views and [Visualising ROC and cost curve duality](http://conflate.net/inductio/2008/04/visualising-roc-and-cost-curve-duality/) with just over 400 views.\n\nOf course, it is natural to want to increase those figures but overall I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
112	year-stats	picture-1	2008-09-11 23:06:22	2008-09-11 23:06:22	Visitors over the last year	inherit	
113	A Year of Research Blogging	111-revision	2008-09-11 23:16:14	2008-09-11 23:16:14	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've jad almost 6,000 visits and over 10,000 page views.\n\n[caption id="attachment_112" align="alignnone" width="300" caption="Visitors over the last year"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" width="300" height="113" class="size-medium wp-image-112" /></a>[/caption]\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php).\n\nOf course, it is natural to increase both those figures but overall, I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include expositions on some of the work I've been turning into papers.
115	A Year of Research Blogging	111-revision-3	2008-09-11 23:17:50	2008-09-11 23:17:50	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've jad almost 6,000 visits and over 10,000 page views.\n\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" width="400" class="size-medium wp-image-112" />\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php).\n\nOf course, it is natural to increase both those figures but overall, I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
114	A Year of Research Blogging	111-revision-2	2008-09-11 23:16:50	2008-09-11 23:16:50	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've jad almost 6,000 visits and over 10,000 page views.\n\n[caption id="attachment_112" align="alignnone" width="300" caption="Visitors over the last year"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" width="300" height="113" class="size-medium wp-image-112" /></a>[/caption]\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php).\n\nOf course, it is natural to increase both those figures but overall, I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
116	A Year of Research Blogging	111-revision-4	2008-09-11 23:18:24	2008-09-11 23:18:24	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've jad almost 6,000 visits and over 10,000 page views.\n\n<div class="center">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" />\n</div>\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php).\n\nOf course, it is natural to increase both those figures but overall, I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
117	Cheap Supervised Training Instances		2008-09-16 01:02:14	0000-00-00 00:00:00	A brief discussion of a paper describing the use of the Amazon Mechanical Turk to buy cheap, human annotations for the creation of supervised training sets.	draft	\n\n[paper]: http://blog.doloreslabs.com/wp-content/uploads/2008/09/amt_emnlp08_accepted.pdf\n[dolores]: http://blog.doloreslabs.com/2008/09/amt-fast-cheap-good-machine-learning/\n[lingpipe]: http://lingpipe-blog.com/2008/09/15/dolores-labs-text-entailment-data-from-amazon-mechanical-turk/
118	Cheap Supervised Training Instances	117-revision	2008-09-16 00:50:14	2008-09-16 00:50:14		inherit	\n\n[paper]: http://blog.doloreslabs.com/wp-content/uploads/2008/09/amt_emnlp08_accepted.pdf\n[dolores]: http://blog.doloreslabs.com/2008/09/amt-fast-cheap-good-machine-learning/\n[lingpipe]: http://lingpipe-blog.com/2008/09/15/dolores-labs-text-entailment-data-from-amazon-mechanical-turk/
119	A Year of Research Blogging	111-revision-5	2008-09-11 23:19:43	2008-09-11 23:19:43	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've jad almost 6,000 visits and over 10,000 page views.\n\n<div class="center">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" />\n</div>\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php). Since then I've had a fairly steady rate of around 30 visits a day.\n\nOf course, it is natural to want to increase those figures but overall I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
120	A Year of Research Blogging	111-revision-6	2008-09-21 02:47:50	2008-09-21 02:47:50	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've had almost 6,000 visits and over 10,000 page views.\n\n<div class="center">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" />\n</div>\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php). It's my most read post with over 2,500 views.\n\nThe second and third most popular posts were on [prediction and the axiom of choice](http://conflate.net/inductio/2008/08/prediction-and-the-axiom-of-choice/) with about 1,300 views and [Visualising ROC and cost curve duality](http://conflate.net/inductio/2008/04/visualising-roc-and-cost-curve-duality/) with just over 400 views.\n\nOf course, it is natural to want to increase those figures but overall I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
121	A Year of Research Blogging	111-revision-7	2008-09-21 11:31:49	2008-09-21 11:31:49	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've had almost 6,000 visits and over 10,000 page views.\n\n<div class="center">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" />\n</div>\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php). It's my most read post with over 2,500 views.\n\nThe second and third most popular posts were on [prediction and the axiom of choice](http://conflate.net/inductio/2008/08/prediction-and-the-axiom-of-choice/) with about 1,300 views and [Visualising ROC and cost curve duality](http://conflate.net/inductio/2008/04/visualising-roc-and-cost-curve-duality/) with just over 400 views.\n\nOf course, it is natural to want to increase those figures but overall I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
122	A Year of Research Blogging	111-revision-8	2008-09-22 06:59:10	2008-09-22 06:59:10	Looking back on a year of research blogging about machine learning.	inherit	Just a short post to reflect on the year that has passed since I [started this blog](http://conflate.net/inductio/2007/09/introducing-inductio-ex-machina/). \n\nA quick trawl through the archives reveals I have published 21 posts (not including this one) for an average of just less than two posts per month. A quick look at my traffic statistics show that I've had almost 6,000 visits and over 10,000 page views.\n\n<div class="center">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/09/picture-1-300x113.png" alt="Visitors over the last year" title="year-stats" />\n</div>\n\nThat huge spike of almost 500 visitors on a single day was for my post on [books that have affected my research](http://conflate.net/inductio/2008/05/research-changing-books/). Most of the incoming traffic for that post was from Kevin Kelly's [post on the same topic](http://kk.org/cooltools/archives/002879.php). It's my most read post with over 2,500 views.\n\nThe second and third most popular posts were on [prediction and the axiom of choice](http://conflate.net/inductio/2008/08/prediction-and-the-axiom-of-choice/) with about 1,300 views and [Visualising ROC and cost curve duality](http://conflate.net/inductio/2008/04/visualising-roc-and-cost-curve-duality/) with just over 400 views.\n\nOf course, it is natural to want to increase those figures but overall I've been fairly happy at the frequency of posts I've written and the number of readers I've attracted to what is a fairly narrow subject area.\n\nSo, one year down and one year to go on my current post-doc. In the year ahead, I'll continue writing about research by others in machine learning but I'll also try to include more expositions of some of my own work.
123	Prediction and the Axiom of Choice	99-revision-9	2008-08-29 01:32:51	2008-08-29 01:32:51	Some thoughts on Hardin and Taylor's paper "A Peculiar Connection Between the Axiom of Choice and Predicting the Future".	inherit	A curious paper entitled ["A Peculiar Connection Between the Axiom of Choice and Predicting the Future"][paper] by [Christopher Hardin][hardin] and [Alan Taylor][taylor] caught my attention recently via the blog [XOR's Hammer][xor].\n\n[paper]: http://maven.smith.edu/~chardin/pub/peculiar.pdf\n[hardin]: http://maven.smith.edu/~chardin/\n[taylor]: http://www.math.union.edu/people/faculty/taylora.html\n[xor]: http://xorshammer.wordpress.com/2008/08/23/set-theory-and-weather-prediction/\n\nIt's main claim is that there exists an almost infallible prediction strategy. That is, one that will almost always predict the correct present value of some unknown function given all its past values. More specifically, they describe the µ-strategy which, when given the values of a function v for all points in time up to but not including t, correctly predicts the value of v(t) for all but countably many points t. They also show that this same strategy can almost always extrapolate correctly into the future (i.e., correctly predict v(s) for t ? s < t + ?).\n\n"Well", you think, "that's induction solved then. I'm going to grab that paper, implement that strategy and retire on the immense wealth I will accumulate from the stock market." Unfortunately for your bank balance, the authors note that\n> We should emphasize that these results do not give a practical means of predicting \n> the future, just as the time dilation one would experience standing near the event \n> horizon of a black hole does not give a practical time machine.\n\nIn other words, the result is purely theoretical. Worse than that, the definition of the µ-strategy requires the [well-ordering theorem][] --- otherwise known as the [Axiom of Choice][]. \n\n[well-ordering theorem]: http://en.wikipedia.org/wiki/Well-ordering_theorem\n[axiom of choice]: http://en.wikipedia.org/wiki/Axiom_of_choice\n\nAside from being completely non-constructive the µ-strategy is relatively straight-forward. First, using the well-ordering theorem, choose some ordering of the set of all possible functions. Now, for each point in time t denote by [v]<sub>t</sub> the equivalence class of functions that are equal for all -? < s < t. When presented with the values for some unknown v up to time t the µ-strategy simply chooses the "smallest" member, say u, of [v]<sub>t</sub> with respect to the ordering and outputs u(t).\n\nSo, apart for the bit where you have to invoke the well-ordering theorem to order the set of all functions over the reals, it's a very simple strategy.\n\nThe author's suggest that the chosen well-ordering can be thought of as a measure of simplicity. In this case the µ-strategy just chooses the "simplest" function consistent with the observations to date. More generally, the well-ordering can be thought of as a bias for the strategy. Different choices of bias will lead to different predictions based on the same past observations. What's odd about the result is that *no matter what bias is chosen the µ-strategy will only ever make countably many mistakes*.\n\nUltimately, the [intuitionist][] in me looks upon this result much as I see the [Banach-Tarski paradox][]. That is, as evidence against the use of the axiom of choice (and its equivalents) in mathematics that's even vaguely practical. Still, the result is an interesting one that analyses the problem of induction in a very abstract setting.\n\n[banach-tarski paradox]: http://en.wikipedia.org/wiki/Banach-Tarski_paradox\n[intuitionist]: http://conflate.net/inductio/2008/06/constructive-and-classical-mathematics/
124	Super Crunchers	super-crunchers	2008-09-27 06:49:14	2008-09-27 06:49:14	A review of the book "Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart" by Ian Ayers.	publish	[Ian Ayers][] is a surprisingly engaging writer, taking what many would consider a very dry topic -- statistics -- and turning it into a thought-provoking, but flawed, book entitled [Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart][sc].\n\n[Ian Ayers]: http://islandia.law.yale.edu/ayers/indexhome.htm\n[sc]: http://www.randomhouse.com/bantamdell/supercrunchers/\n\nFrom the opening pages, Ayers pits the "super crunchers" -- people applying statistics to large data sets -- against experts in an area, be it viticulture, baseball, or marketing. With barely suppressed glee he describes how number crunching out-predicts the experts time and time again. The point being that as collecting, storing and analysing large amounts of data becomes cheaper and cheaper, more and more decision-making will take the results of "super crunching" into account, with experts either having to step aside or learn some statistical chops. To back arguments for the rise of "super crunching" Ayers draws on a large number of examples from a variety of areas and even experiments with the technique himself, describing how he used it to help choose the title of his book. \n\nAlthough I am more or less convinced by Ayers' arguments I found myself questioning his credibility in several places during the book. I think the main reason for this was due to the tone of the book occasionally crossing the fine line separating "enthusiastic, popular account" and "overly simplistic, gushing rave". The constant use of "super crunching" throughout the book got on my nerves after a while. It began to overemphasise the newness of what could as easily be called "statistical analysis". After a while I mentally replaced "super crunching" with the less sensational "statistical analysis" wherever I encountered it.\n\nConversely, Ayers constantly refers to "regression" when talking about the techniques analysts use to make predictions. At first, I thought this was a convenient short-hand for a range of techniques that he didn't want to spend time distinguishing between. It was only when neural networks are described as "a newfangled competitor to the tried-and-true regression formula" and "an important contributor to the Super Crunching revolution" that I realised that Ayers may not know as much about the nuts and bolts of computational statistics as I first thought. This impression was confirmed when Ayers later confuses "summary statistics" for "sufficient statistics" and talks tautologically of "binary bytes".\n\nStylistically, there is too much foreshadowing and repetition of topics throughout the book for my liking. This feels a little condescending at times, as does him directly asking the reader to stop and think about a concept or problem at various points. \n\nOverall, I wanted to like this book more than I did. It was a light, enjoyable read and I wholeheartedly agree with Ayers' belief in the continuing importance of statistics in decision-making and his call to improve the average person's intuition of statistics. Unfortunately, I found much of "Super Crunchers" substituting enthusiasm for coherence, as well as impressions and anecdote for any kind of meaningful argument. \n\n
125	Super Crunchers	124-revision	2008-09-22 23:31:07	2008-09-22 23:31:07	A review of the book "Super Crunchers: Why Thinking-By-N" by 	inherit	Ayers is a very engaging writer\n\nneural networks, lack of discussion about model biases.\n\nrepetition, foreshadowing\n\n"summary statistics"\n\n"binary bytes"\n\nAgree with his discussion in the last chapter about the need for people to become intuitive about statistics and understand things like a standard deviation.
126	Big Data and Super Crunchers	124-revision-2	2008-09-25 05:56:47	2008-09-25 05:56:47	A review of the book "Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart" by Ian Ayers and its similarities to a recent controversial article in Wired on "Big Data".	inherit	Ayers is a very engaging writer\n\nneural networks, lack of discussion about model biases.\n\nrepetition, foreshadowing\n\n"summary statistics"\n\n"binary bytes"\n\nAgree with his discussion in the last chapter about the need for people to become intuitive about statistics and understand things like a standard deviation.\n\nShalizi on Chris Anderson: http://cscs.umich.edu/~crshalizi/weblog/581.html points to Pierara's criticisms (which can equally be applied to Super Crunchers). In a nutshell, 
127	Big Data and Super Crunchers	124-revision-3	2008-09-25 05:57:41	2008-09-25 05:57:41	A review of the book "Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart" by Ian Ayers and its similarities to a recent controversial article in Wired on "Big Data".	inherit	Ayers is a very engaging writer\n\nneural networks, lack of discussion about model biases.\n\nrepetition, foreshadowing\n\n"summary statistics"\n\n"binary bytes"\n\nAgree with his discussion in the last chapter about the need for people to become intuitive about statistics and understand things like a standard deviation.\n\n[Shalizi on Chris Anderson](http://cscs.umich.edu/~crshalizi/weblog/581.html) points to Pierara's criticisms (which can equally be applied to Super Crunchers). In a nutshell, if you don't have constraints (a.k.a. inductive biases) you will just memorise the training examples.
128	Big Data and Super Crunchers	124-revision-4	2008-09-25 05:59:33	2008-09-25 05:59:33	A review of the book "Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart" by Ian Ayers and its similarities to a recent controversial article in Wired on "Big Data".	inherit	Ayers is a very engaging writer\n\nneural networks, lack of discussion about model biases.\n\nrepetition, foreshadowing\n\n"summary statistics"\n\n"binary bytes"\n\nAgree with his discussion in the last chapter about the need for people to become intuitive about statistics and understand things like a standard deviation.\n\n[Shalizi on Chris Anderson](http://cscs.umich.edu/~crshalizi/weblog/581.html) points to Pierara's criticisms (which can equally be applied to Super Crunchers). In a nutshell, if you don't have constraints (a.k.a. inductive biases) you will just memorise the training examples. Shalizi also points to [Danny Hillis's response](http://www.edge.org/3rd_culture/bios/hillis.html) to Anderson's article.
129	Big Data and the Super Crunchers	124-revision-5	2008-09-25 06:01:24	2008-09-25 06:01:24	A review of the book "Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart" by Ian Ayers and its similarities to a recent controversial article in Wired on "Big Data".	inherit	Ayers is a very engaging writer\n\nneural networks, lack of discussion about model biases.\n\nrepetition, foreshadowing\n\n"summary statistics"\n\n"binary bytes"\n\nAgree with his discussion in the last chapter about the need for people to become intuitive about statistics and understand things like a standard deviation.\n\n[Chris Anderson](http://www.edge.org/3rd_culture/anderson08/anderson08_index.html)\n\n[Shalizi on Chris Anderson](http://cscs.umich.edu/~crshalizi/weblog/581.html) points to Pierara's criticisms (which can equally be applied to Super Crunchers). In a nutshell, if you don't have constraints (a.k.a. inductive biases) you will just memorise the training examples. Shalizi also points to [Danny Hillis's response](http://www.edge.org/3rd_culture/bios/hillis.html) to Anderson's article.
131	Super Crunchers	124-revision-7	2008-09-27 06:48:33	2008-09-27 06:48:33	A review of the book "Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart" by Ian Ayers.	inherit	[Ian Ayers][] is a surprisingly engaging writer, taking what many would consider a very dry topic -- statistics -- and turning it into a thought-provoking, but flawed, book entitled [Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart][sc].\n\n[Ian Ayers]: http://islandia.law.yale.edu/ayers/indexhome.htm\n[sc]: http://www.randomhouse.com/bantamdell/supercrunchers/\n\nFrom the opening pages, Ayers pits the "super crunchers" -- people applying statistics to large data sets -- against experts in an area, be it viticulture, baseball, or marketing. With barely suppressed glee he describes how number crunching out-predicts the experts time and time again. The point being that as collecting, storing and analysing large amounts of data becomes cheaper and cheaper, more and more decision-making will take the results of "super crunching" into account, with experts either having to step aside or learn some statistical chops. To back arguments for the rise of "super crunching" Ayers draws on a large number of examples from a variety of areas and even experiments with the technique himself, describing how he used it to help choose the title of his book. \n\nAlthough I am more or less convinced by Ayers' arguments I found myself questioning his credibility in several places during the book. I think the main reason for this was due to the tone of the book occasionally crossing the fine line separating "enthusiastic, popular account" and "overly simplistic, gushing rave". The constant use of "super crunching" throughout the book got on my nerves after a while. It began to overemphasise the newness of what could as easily be called "statistical analysis". After a while I mentally replaced "super crunching" with the less sensational "statistical analysis" wherever I encountered it.\n\nConversely, Ayers constantly refers to "regression" when talking about the techniques analysts use to make predictions. At first, I thought this was a convenient short-hand for a range of techniques that he didn't want to spend time distinguishing between. It was only when neural networks are described as "a newfangled competitor to the tried-and-true regression formula" and "an important contributor to the Super Crunching revolution" that I realised that Ayers may not know as much about the nuts and bolts of computational statistics as I first thought. This impression was confirmed when Ayers later confuses "summary statistics" for "sufficient statistics" and talks tautologically of "binary bytes".\n\nStylistically, there is too much foreshadowing and repetition of topics throughout the book for my liking. This feels a little condescending at times, as does him directly asking the reader to stop and think about a concept or problem at various points. \n\nOverall, I wanted to like this book more than I did. It was a light, enjoyable read and I wholeheartedly agree with Ayers' belief in the continuing importance of statistics in decision-making and his call to improve the average person's intuition of statistics. Unfortunately, I found much of "Super Crunchers" substituting enthusiasm for coherence, as well as impressions and anecdote for any kind of meaningful argument. \n\n
130	Super Crunchers	124-revision-6	2008-09-27 06:47:53	2008-09-27 06:47:53	A review of the book "Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart" by Ian Ayers and its similarities to a recent controversial article in Wired on "Big Data".	inherit	[Ian Ayers][] is a surprisingly engaging writer, taking what many would consider a very dry topic -- statistics -- and turning it into a thought-provoking, but flawed, book entitled [Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart][sc].\n\n[Ian Ayers]: http://islandia.law.yale.edu/ayers/indexhome.htm\n[sc]: http://www.randomhouse.com/bantamdell/supercrunchers/\n\nFrom the opening pages, Ayers pits the "super crunchers" -- people applying statistics to large data sets -- against experts in an area, be it viticulture, baseball, or marketing. With barely suppressed glee he describes how number crunching out-predicts the experts time and time again. The point being that as collecting, storing and analysing large amounts of data becomes cheaper and cheaper, more and more decision-making will take the results of "super crunching" into account, with experts either having to step aside or learn some statistical chops. To back arguments for the rise of "super crunching" Ayers draws on a large number of examples from a variety of areas and even experiments with the technique himself, describing how he used it to help choose the title of his book. \n\nAlthough I am more or less convinced by Ayers' arguments I found myself questioning his credibility in several places during the book. I think the main reason for this was due to the tone of the book occasionally crossing the fine line separating "enthusiastic, popular account" and "overly simplistic, gushing rave". The constant use of "super crunching" throughout the book got on my nerves after a while. It began to overemphasise the newness of what could as easily be called "statistical analysis". After a while I mentally replaced "super crunching" with the less sensational "statistical analysis" wherever I encountered it.\n\nConversely, Ayers constantly refers to "regression" when talking about the techniques analysts use to make predictions. At first, I thought this was a convenient short-hand for a range of techniques that he didn't want to spend time distinguishing between. It was only when neural networks are described as "a newfangled competitor to the tried-and-true regression formula" and "an important contributor to the Super Crunching revolution" that I realised that Ayers may not know as much about the nuts and bolts of computational statistics as I first thought. This impression was confirmed when Ayers later confuses "summary statistics" for "sufficient statistics" and talks tautologically of "binary bytes".\n\nStylistically, there is too much foreshadowing and repetition of topics throughout the book for my liking. This feels a little condescending at times, as does him directly asking the reader to stop and think about a concept or problem at various points. \n\nOverall, I wanted to like this book more than I did. It was a light, enjoyable read and I wholeheartedly agree with Ayers' belief in the continuing importance of statistics in decision-making and his call to improve the average person's intuition of statistics. Unfortunately, I found much of "Super Crunchers" substituting enthusiasm for coherence, as well as impressions and anecdote for any kind of meaningful argument. \n\nI started and finished reading Ian Ayer's book [Super Crunchers: Why Thinking-By-Numbers is the New Way To Be Smart][sc] over a couple of days last week. In a nutshell, it was an engaging read, replete with interesting anecdotes about how the low cost of collecting, storing and analysing data has led to statistical techniques out-predicting experts in many areas of business. Unfortunately, \n\nI've posted a [review][] of it at [LibraryThing][]\n\n[sc]: http://www.librarything.com/work/book/36140381\n[review]: \n[LibraryThing]: http://librarything.com\n\nAyers is a very engaging writer\n\nInteresting anecdotes on how randomisation is being used by business, especially web business, to aid decision-making. Also describes pitfalls.\n\nneural networks, lack of discussion about model biases.\n\nrepetition, foreshadowing, slightly condescending in places.\n\n"summary statistics"\n\n"binary bytes"\n\nAgree with his discussion in the last chapter about the need for people to become intuitive about statistics and understand things like a standard deviation.\n\n[Chris Anderson](http://www.edge.org/3rd_culture/anderson08/anderson08_index.html)\n\n[Shalizi on Chris Anderson](http://cscs.umich.edu/~crshalizi/weblog/581.html) points to Pierara's criticisms (which can equally be applied to Super Crunchers). In a nutshell, if you don't have constraints (a.k.a. inductive biases) you will just memorise the training examples. Shalizi also points to [Danny Hillis's response](http://www.edge.org/3rd_culture/bios/hillis.html) to Anderson's article.
132	Big Data		2008-09-27 06:49:44	0000-00-00 00:00:00		draft	[Chris Anderson](http://www.edge.org/3rd_culture/anderson08/anderson08_index.html)\n\n[Shalizi on Chris Anderson](http://cscs.umich.edu/~crshalizi/weblog/581.html) points to Pierara's criticisms (which can equally be applied to Super Crunchers). In a nutshell, if you don't have constraints (a.k.a. inductive biases) you will just memorise the training examples. Shalizi also points to [Danny Hillis's response](http://www.edge.org/3rd_culture/bios/hillis.html) to Anderson's article.
133	Big Data	132-revision	2008-09-27 06:49:20	2008-09-27 06:49:20		inherit	
134	Snuck, flied and wedded 	snuck-flied-and-wedded	2008-10-19 21:04:41	2008-10-19 10:33:10	A quick summary of a paper in Nature last year that analyses the rate at which words shift from irregular to regular.	publish	Ben Allen over at [PLEKTIX][] highlighted (highlit?) a paper in Nature last year that compiled and analysis some hard data regarding the evolution of the English language. Entitled [Quantifying the evolutionary dynamics of language][paper], the paper by Lieberman and colleagues looked at the shift from irregular to regular English verbs over the last 1200 years. \n\n[PLEKTIX]: http://plektix.blogspot.com/2008/10/evolution-of-irregular-verbs.html\n[paper]: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2460562\n\nThe question the authors of the paper ask is, "At what rate do words shift from irregular (_go_/_went_) to regular (_talk_/_talked_)?" They find a very simple rule to describe this rate: the rate of "regularization" of a word is inversely proportional to the square root of its usage frequency. That is, if an irregular verb is used 100 times more than another it takes 10 times longer before it become regular.\n\nExtrapolating from this rule, the author's note that they can predict which currently irregular verbs will soonest become regular. The suggest _wed_/_wed_ is one such precarious irregular verb, soon to become _wed_/_wedded_.\n\nPinker discusses this type of transition in his book [Words and Rules][] and suggests that only commonly used words can stay irregular. He argues that keeping a big list of exceptions like irregular verbs around requires their constant repetition. This study nicely complements this by collecting the empirical evidence and quantifying the change. \n\n[words and rules]: http://pinker.wjh.harvard.edu/books/wr/index.html
135	Snuck, flied and wedded 	134-revision	2008-10-15 05:45:31	2008-10-15 05:45:31		inherit	Ben Allen over at [PLEKTIX][] highlighted (highlit?) a paper in Nature last year that compiled and analysis some hard data regarding the evolution of the English language. Entitled [Quantifying the evolutionary dynamics of language][paper], the paper by Lieberman and colleagues looked at the shift from irregular and iregular verbs over the last \n\n[PLEKTIX]: http://plektix.blogspot.com/2008/10/evolution-of-irregular-verbs.html\n[paper]: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2460562
136	Snuck, flied and wedded 	134-revision-2	2008-10-19 10:23:20	2008-10-19 10:23:20		inherit	Ben Allen over at [PLEKTIX][] highlighted (highlit?) a paper in Nature last year that compiled and analysis some hard data regarding the evolution of the English language. Entitled [Quantifying the evolutionary dynamics of language][paper], the paper by Lieberman and colleagues looked at the shift from irregular to regular English verbs over the last 1200 years. \n\n[PLEKTIX]: http://plektix.blogspot.com/2008/10/evolution-of-irregular-verbs.html\n[paper]: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2460562\n\nThe question the authors of the paper ask is, "At what rate do words shift from irregular (_go_/_went_) to regular (_talk_/_talked_)?" They find a very simple rule to describe this rate: the rate of "regularization" of a word is proportional to the square root of its usage frequency. That is, if an irregular verb is used 100 times more than another it takes 10 times longer before it become regular.\n\nExtrapolating from this rule, the author's note that they can predict which currently irregular verbs will soonest become regular. The suggest _wed_/_wed_ is one such precarious irregular verb, soon to become _wed_/_wedded_
137	Snuck, flied and wedded 	134-revision-3	2008-10-19 10:32:01	2008-10-19 10:32:01	A quick summary of a paper in Nature last year that analyses the rate at which words shift from irregular to regular.	inherit	Ben Allen over at [PLEKTIX][] highlighted (highlit?) a paper in Nature last year that compiled and analysis some hard data regarding the evolution of the English language. Entitled [Quantifying the evolutionary dynamics of language][paper], the paper by Lieberman and colleagues looked at the shift from irregular to regular English verbs over the last 1200 years. \n\n[PLEKTIX]: http://plektix.blogspot.com/2008/10/evolution-of-irregular-verbs.html\n[paper]: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2460562\n\nThe question the authors of the paper ask is, "At what rate do words shift from irregular (_go_/_went_) to regular (_talk_/_talked_)?" They find a very simple rule to describe this rate: the rate of "regularization" of a word is proportional to the square root of its usage frequency. That is, if an irregular verb is used 100 times more than another it takes 10 times longer before it become regular.\n\nExtrapolating from this rule, the author's note that they can predict which currently irregular verbs will soonest become regular. The suggest _wed_/_wed_ is one such precarious irregular verb, soon to become _wed_/_wedded_.\n\nPinker discusses this type of transition in his book [Words and Rules][] and suggests that only commonly used words can stay irregular. He argues that keeping a big list of exceptions like irregular verbs around requires their constant repetition. This study nicely complements this by collecting the empirical evidence and quantifying the change. \n\n[words and rules]: http://pinker.wjh.harvard.edu/books/wr/index.html
138	Snuck, flied and wedded 	134-revision-4	2008-10-19 10:32:14	2008-10-19 10:32:14	A quick summary of a paper in Nature last year that analyses the rate at which words shift from irregular to regular.	inherit	Ben Allen over at [PLEKTIX][] highlighted (highlit?) a paper in Nature last year that compiled and analysis some hard data regarding the evolution of the English language. Entitled [Quantifying the evolutionary dynamics of language][paper], the paper by Lieberman and colleagues looked at the shift from irregular to regular English verbs over the last 1200 years. \n\n[PLEKTIX]: http://plektix.blogspot.com/2008/10/evolution-of-irregular-verbs.html\n[paper]: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2460562\n\nThe question the authors of the paper ask is, "At what rate do words shift from irregular (_go_/_went_) to regular (_talk_/_talked_)?" They find a very simple rule to describe this rate: the rate of "regularization" of a word is proportional to the square root of its usage frequency. That is, if an irregular verb is used 100 times more than another it takes 10 times longer before it become regular.\n\nExtrapolating from this rule, the author's note that they can predict which currently irregular verbs will soonest become regular. The suggest _wed_/_wed_ is one such precarious irregular verb, soon to become _wed_/_wedded_.\n\nPinker discusses this type of transition in his book [Words and Rules][] and suggests that only commonly used words can stay irregular. He argues that keeping a big list of exceptions like irregular verbs around requires their constant repetition. This study nicely complements this by collecting the empirical evidence and quantifying the change. \n\n[words and rules]: http://pinker.wjh.harvard.edu/books/wr/index.html
139	Snuck, flied and wedded 	134-revision-5	2008-10-19 10:33:10	2008-10-19 10:33:10	A quick summary of a paper in Nature last year that analyses the rate at which words shift from irregular to regular.	inherit	Ben Allen over at [PLEKTIX][] highlighted (highlit?) a paper in Nature last year that compiled and analysis some hard data regarding the evolution of the English language. Entitled [Quantifying the evolutionary dynamics of language][paper], the paper by Lieberman and colleagues looked at the shift from irregular to regular English verbs over the last 1200 years. \n\n[PLEKTIX]: http://plektix.blogspot.com/2008/10/evolution-of-irregular-verbs.html\n[paper]: http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2460562\n\nThe question the authors of the paper ask is, "At what rate do words shift from irregular (_go_/_went_) to regular (_talk_/_talked_)?" They find a very simple rule to describe this rate: the rate of "regularization" of a word is proportional to the square root of its usage frequency. That is, if an irregular verb is used 100 times more than another it takes 10 times longer before it become regular.\n\nExtrapolating from this rule, the author's note that they can predict which currently irregular verbs will soonest become regular. The suggest _wed_/_wed_ is one such precarious irregular verb, soon to become _wed_/_wedded_.\n\nPinker discusses this type of transition in his book [Words and Rules][] and suggests that only commonly used words can stay irregular. He argues that keeping a big list of exceptions like irregular verbs around requires their constant repetition. This study nicely complements this by collecting the empirical evidence and quantifying the change. \n\n[words and rules]: http://pinker.wjh.harvard.edu/books/wr/index.html
140	Artificial AI	artificial-ai	2008-11-07 01:52:41	2008-11-07 01:52:41		publish	Anyone who has working in the area for long enough knows how difficult creating any type of artificial intelligence can be. Like many before me, I've decided to cheat a little and create an artificial AI. I take partial credit for the initial idea but it is my wife, Julieanne, who has been responsible for most of the development over the last nine months. \n\nWe had our official release on the 26th of October and even though she's been here for less than two weeks she is already exceeding all our expectations. \n\nWe refer to her as "Ada Molly Reid".\n\n<center>\n<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/ada.jpg"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/ada.jpg" alt="An Artificial AI, completed on the 26th of October, 2008." title="Ada Molly Reid" width="170" height="130" class="size-full wp-image-141" /></a></center>\n\n
141	Ada Molly Reid	ada	2008-11-07 01:48:53	2008-11-07 01:48:53	An Artificial AI, completed on the 26th of October, 2008.	inherit	
142	Artificial AI	140-revision	2008-11-07 01:52:09	2008-11-07 01:52:09		inherit	Anyone who has working in the area for long enough knows how difficult creating any type of artificial intelligence can be. Like many before me, I've decided to cheat a little and create an artificial AI. I take partial credit for the initial idea but it is my wife, Julieanne, who has been responsible for most of the development over the last nine months. \n\nWe had our official release on the 26th of October and even though she's been here for less than two weeks she is already exceeding all our expectations. \n\nWe refer to her as "Ada Molly Reid".\n\n<center>\n<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/ada.jpg"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/ada.jpg" alt="An Artificial AI, completed on the 26th of October, 2008." title="Ada Molly Reid" width="170" height="130" class="size-full wp-image-141" /></a></center>\n\n
143	Behold! Jensen's Inequality	behold-jensens-inequality	2008-11-17 06:26:15	2008-11-17 06:26:15	Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.	publish	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over some finite dimensional convex set <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\n\\displaystyle J_f(x) := \\mathbb{E}\\left[ f\\left(x\\right) \\right] - f\\left(\\mathbb{E}\\left[ x \\right]\\right)\n[/tex]\n</center>\nwhere [tex]\\mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) \\geq 0[/tex] or equivalently,\n<center>\n[tex]\n\\displaystyle \\mathbb{E}\\left[ f\\left(x\\right) \\right] \\geq f\\left(\\mathbb{E}\\left[ x \\right]\\right).\n[/tex]\n</center>\n\nThis is a fairly simple but important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki][] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was quite happy then to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it. \n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub>"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen\\&#039;s Inequality" title="Jensen\\&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\nUnfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen's inequality for a discrete distribution where the random variable [tex]x[/tex] takes on one of the <i>n</i> values [tex]x_i[/tex] with with probability [tex]p_i[/tex].\n\nNote that the points [tex](x_i, f(x_i))[/tex] form the vertices of a polygon which, by the convexity of , must also be convex and lie within the epigraph of  (the blue shaded area above ). Furthermore, since the [tex]p_i[/tex] are probabilities they satisfy [tex]\\sum_i p_i = 1[/tex]. This means the expected value of the random variable [tex](x, f(x))[/tex] given by\n<center>\n[tex]\\displaystyle \n   \\mathbb{E}[(x, f(x))] = \\sum_{i=1}^n p_i \\left(x_i, f(x_i)\\right) \n[/tex]\n</center>\nis a convex combination and so must also lie within the dashed polygon. In fact, since [tex]\\mathbb{E}[(x, f(x))] = \\left(\\mathbb{E}[x], \\mathbb{E}[f(x)]\\right)[/tex] it must lie above [tex]f\\left(\\mathbb{E}[x]\\right)[/tex] thus giving the result.\n\nAlthough the diagram in Figure 1 assumes a 1-dimensional space <i>X</i> the above argument generalises to higher dimensions in an analogous manner. Also, the general result for non-discrete distributions can be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more [tex]x_i[/tex] to the diagram the dashed polygon the shaded area will approximate the graph of  better. So, by the earlier argument for the discrete case, the expected value of [tex]x[/tex] will remain within the polygon and thus within the shaded area and thus above . Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.\n\nA somewhat surprising fact about Jensen's inequality is that its converse is also true. By this I mean that if  is a function such that its Jensen gap [tex]J_f(x)[/tex] is non-negative for all distributions of the random variable <i>x</i> then  is necessarily convex. The contrapositive of this statement is:  non-convex implies the existence of a random variable <i>x</i> so that [tex]J_f(x) < 0[/tex].\n\nConsidering Figure 1 again gives some intuition as to why this must be the case. If  was non-convex then its epigraph must, by definition, also be non-convex. This means I could choose some [tex]x_i[/tex] so that one of the dashed lines lies outside the shaded area. This means I can then choose [tex]p_i[/tex] so that the mean [tex]\\mathbb{E}[(x, f(x))][/tex] lies outside the shaded area and thus below the graph of .\n\nOf course, no self-respecting mathematician would call the above arguments a proof of Jensen's inequality. There are too many edge cases and subtleties (especially in the continuous case) that I've ignored. That said, I believe the statement and thrust of the inequality can be quickly arrived at from the simple diagram above. When using tools like Jensen's inequality, I find this type of quick insight more valuable than a long, careful technical statement and proof. The latter is valuable to but if I need this level of detail I would look it up rather than try to dredge it up from my sometimes unreliable memory.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
144	Behold! Jensen's Inequality	143-revision	2008-11-17 02:02:18	2008-11-17 02:02:18		inherit	I have been making quite a bit of use of Jensen's inequality recently. It is a fairly simple but very important inequality in the study of convex functions.\n<blockquote>\n[tex]\ndisplaystyle fleft(mathbb{E}left[ X right]right) leq mathbb{E}left[\n[/tex]\n</blockquote>\n\nIt turns out that DeGroot's notion of statistical information[^DeGroot1962] and measures of the distance between probability distributions called [f-divergences][] can be expressed as the "gap" between the two side of the inequality.\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567
145	Behold! Jensen's Inequality	143-revision-2	2008-11-17 02:11:36	2008-11-17 02:11:36		inherit	I have been making quite a bit of use of Jensen's inequality recently. It is a fairly simple but very important inequality in the study of convex functions. I have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true \n\n<blockquote style="text-align: center">\n[tex]\ndisplaystyle fleft(mathbb{E}left[ X right]right) leq mathbb{E}left[ fleft(Xright) right]\n[/tex]\n</blockquote>\n\n<blockquote>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n lambda_i x_i right) leq sum_{i=1}^n lambda_ifleft(Xright) right]\n[/tex]\n</blockquote>\n\nIt turns out that DeGroot's notion of statistical information[^DeGroot1962] and measures of the distance between probability distributions called [f-divergences][] can be expressed as the "gap" between the two side of the inequality.\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567
146	Behold! Jensen's Inequality	143-revision-3	2008-11-17 02:29:00	2008-11-17 02:29:00		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if [tex]f : X to mathbb{R}[tex] is a real-valued convex function over [tex]X[tex] and [tex]x in X[tex] is a random variable then \n<center>\n[tex]\ndisplaystyle fleft(mathbb{E}left[ x right]right) leq mathbb{E}left[ fleft(xright) right]\n[/tex]\n</center>\nwhere [tex]mathbb{E}[tex] represents the \n\nIt is a fairly simple but very important inequality in the study of convex functions. I have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite \nhappy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n\nIt turns out that DeGroot's notion of statistical information[^DeGroot1962] and measures of the distance between probability distributions called [f-divergences][] can be expressed as the "gap" between the two side of the inequality.\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/
147	Behold! Jensen's Inequality	143-revision-4	2008-11-17 02:30:19	2008-11-17 02:30:19		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if \n[tex]f : X to mathbb{R}[tex] is a real-valued convex function over [tex]X[tex] and [tex]x in X[tex] is a random variable then \n<center>\n[tex]displaystyle fleft(mathbb{E}left[ x right]right) leq mathbb{E}left[ fleft(xright) right][/tex]\n</center>\nwhere [tex]mathbb{E}[tex] denotes expectation.\n\nIt is a fairly simple but very important inequality in the study of convex functions. I have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite \nhappy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n\nIt turns out that DeGroot's notion of statistical information[^DeGroot1962] and measures of the distance between probability distributions called [f-divergences][] can be expressed as the "gap" between the two side of the inequality.\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/
148	Behold! Jensen's Inequality	143-revision-5	2008-11-17 02:30:21	2008-11-17 02:30:21		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if \n[tex]f : X to mathbb{R}[tex] \nis a real-valued convex function over [tex]X[tex] and [tex]x in X[tex] is a random variable then \n<center>\n[tex]displaystyle fleft(mathbb{E}left[ x right]right) leq mathbb{E}left[ fleft(xright) right][/tex]\n</center>\nwhere [tex]mathbb{E}[tex] denotes expectation.\n\nIt is a fairly simple but very important inequality in the study of convex functions. I have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite \nhappy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n\nIt turns out that DeGroot's notion of statistical information[^DeGroot1962] and measures of the distance between probability distributions called [f-divergences][] can be expressed as the "gap" between the two side of the inequality.\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/
149	Behold! Jensen's Inequality	143-revision-6	2008-11-17 02:31:15	2008-11-17 02:31:15		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if [tex]f[/tex] is a real-valued convex function over [tex]X[/tex] and [tex]x in X[/tex] is a random variable then \n<center>\n[tex]\ndisplaystyle fleft(mathbb{E}left[ x right]right) leq mathbb{E}left[ fleft(xright) right]\n[/tex]\n</center>\nwhere [tex]mathbb{E}[tex] denotes expectation.\n\nIt is a fairly simple but very important inequality in the study of convex functions. I have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite \nhappy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n\nIt turns out that DeGroot's notion of statistical information[^DeGroot1962] and measures of the distance between probability distributions called [f-divergences][] can be expressed as the "gap" between the two side of the inequality.\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/
151	Behold! Jensen's Inequality	143-revision-8	2008-11-17 02:56:23	2008-11-17 02:56:23		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. It can also be used to derive a [general AM-GM inequality][amgm] and many I've been interested in it because DeGroot's notion of _statistical information_[^DeGroot1962] and measures of the distance between probability distributions called _[f-divergences][]_ can be expressed as a Jensen gap. Furthermore, these two quantities are \n\n\nI have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite \nhappy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality
150	Behold! Jensen's Inequality	143-revision-7	2008-11-17 02:48:28	2008-11-17 02:48:28		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is always\n\nIt is a fairly simple but very important inequality in the study of convex functions. I have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite \nhappy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n\nIt turns out that DeGroot's notion of statistical information[^DeGroot1962] and measures of the distance between probability distributions called [f-divergences][] can be expressed as the "gap" between the two side of the inequality.\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/
152	Behold! Jensen's Inequality	143-revision-9	2008-11-17 02:57:21	2008-11-17 02:57:21		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of _statistical information_[^DeGroot1962] and measures of the distance between probability distributions called _[f-divergences][]_ can be expressed as a Jensen gap. \n\nI have read and understood several proofs of it and they are all almost a direct consequence of the definition of convexity. However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite happy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n[^DeGroot1962]: M. H. DeGroot, [Uncertainty, Information, and Sequential Experiments][uise]\n_Ann. Math. Statist._ Volume 33, Number 2 (1962), 404-419.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality
153	Behold! Jensen's Inequality	143-revision-10	2008-11-17 03:06:00	2008-11-17 03:06:00		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n\n However, I've had a bit of trouble intuitively grasping why its true. I was therefore quite happy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality
154	Jensen's Inequality	jensen	2008-11-17 03:43:57	2008-11-17 03:43:57	Jensen's Inequality	inherit	A graphical proof of Jensen's Inequality
155	Behold! Jensen's Inequality	143-revision-11	2008-11-17 03:44:37	2008-11-17 03:44:37		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was therefore quite happy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n[caption id="attachment_154" align="aligncenter" width="485" caption="Jensen's Inequality"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality
156	Behold! Jensen's Inequality	143-revision-12	2008-11-17 04:56:45	2008-11-17 04:56:45		inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was therefore quite happy to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it.\n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are "]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\n\nFirst of all here's a statement of Jensen's inequality for discrete distributions.\n<center>\n[tex]\ndisplaystyle fleft(sum_{i=1}^n p_i x_i right) leq sum_{i=1}^n p_i fleft(x_iright) \n[/tex]\n</center>\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
157	Visualising 19th Century Reading in Australia	40-revision	2008-06-17 05:09:52	2008-06-17 05:09:52	A description of a visualisation of some 19th century Australian borrowing records from the Australian Common Readers Project.	inherit	I've recently spent a bit of time collaborating with my wife on a research project. Research collaboration by couples is not new but given that Julieanne is a [lecturer in the English program][j] and I'm part of the [computer sciences laboratory][csl], this piece of joint research is a little unusual. \n\nThe rest of this post describes the intersection of our interests --- data from the Australian Common Reader Project --- and the visualisation tool I wrote to explore it. The tool itself is based on a simple application of linear Principal Component Analysis (PCA). I'll attempt to explain it here in such a way that readers who have not studied this technique might still be able to make use of the tool.\n\n[j]: http://cass.anu.edu.au/humanities/school_sites/staff.php\n[csl]: http://csl.cecs.anu.edu.au/\n\nThe Australian Common Reader Project\n--------------------------------------------\nOne of Julieanne's research interests is the Australian audience of the late 19th and early 20th centuries. As part of her PhD, she made use of an amazing database that is part of the [Australian Common Reader Project][acrp] --- a project that has collected and entered library borrowing records from Australian libraries along with annotations about when books were borrowed, their genres, borrower occupations, author information, <i>etc</i>. This sort of information makes it possible for Australian literature and cultural studies academics to ask empirical questions about Australian readers' relationship with books and periodicals. \n\n[acrp]: http://www.api-network.com/hosted/acrp/\n\nEver on the lookout for [interesting data sets][meta-index], I suggested that we apply some basic data analysis tools to the database to see what kind of relationships between books and borrowers we might find. When asked if we could have access to the database, [Tim Dolin][] graciously agreed and enlisted [Jason Ensor][] to help with our technical questions. \n\n[meta-index]: http://conflate.net/inductio/2008/02/a-meta-index-of-data-sets/\n[tim dolin]: http://www.humanities.curtin.edu.au/staff.cfm/t.dolin\n[jason ensor]: http://www.humanities.curtin.edu.au/staff.cfm/j.ensor \n\nBooks and Borrowers\n------------------------\nAfter an initial inspection, my first thought was to try to visualise the similarity of the books in the database as measured by the number of borrowers they have in common. \nThe full database contains 99,692 loans of 7,078 different books from 11 libraries by one of the 2,642 people. To make this more manageable, I focused on books that had at least 20 different borrowers and only considered people who had borrowed one of these books.\nThis distilled the database down to a simple table with each row representing one of 1,616 books and each column representing one of 2,473 people. \n\n<table class="aligncenter">\n<caption>Table 1: A portion of the book and borrower table. A 1 indicates that the borrower (column)\nborrowed the book (row) at least once. A 0 indicates that the borrower never borrowed the book.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="4" class="title">Borrower ID</th></tr>\n<tr><th>1</th><th>2</th><th>...</th><th>2,473</th></tr>\n<tr><th>1</th><td>1</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>2</th><td>1</td><td>1</td><td>...</td><td>0</td></tr>\n<tr><th>3</th><td>0</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>1</td><td>1</td><td>...</td><td>1</td></tr>\n</table>\n\nConceptually, each cell in the table contains a 1 if the person associated with the cell's column borrowed the book associated with the cell's row. If there was no such loan between a given book and borrower the corresponding cell contains a 0. For example, Table 1 shows that book 2 was borrowed (at least once) by borrower 1 but never by borrower 2,473.\n\nBook Similarity\n-----------------\nThe table view of the books and their borrowers does not readily lend itself to insight. The approach we took to get a better picture of this information was to plot each book as a point on a graph so that similar books are placed closer together than dissimilar books. To do this a notion of what "similar books" is required.\n\nMathematically, row [tex]i[/tex] of Table 1 can be represented as a vector [tex]mathbf{b}_i[/tex] of 1s and 0s. The value of the cell in the [tex]j[/tex]<sup>th</sup> column of that row will be denoted [tex]b_{i,j}[/tex]. For example, the 2<sup>nd</sup> row in the table can be written as the vector [tex]mathbf{b}_2 = (1,1,ldots,0)[/tex] and the value in its first column is [tex]b_{2,1} = 1[/tex].\n\nA crude measure of the similarity between book 1 and book 2 can be computed from this table by counting how many borrowers they have in common. That is, the number of columns that have a `1` in the row for book 1 and the row for book 2. \n \nIn terms of the vector representation, this similarity measure is simply the "[inner product][]" between [tex]mathbf{b}_1[/tex] and [tex]mathbf{b}_2[/tex] and is written [tex]left<mathbf{b}_1,mathbf{b}_2right> = b_{1,1}b_{2,1} + cdots + b_{1,N}b_{2,N}[/tex] where N = 2,473 is the total number of borrowers.\n\n[inner product]: http://en.wikipedia.org/wiki/Inner_product_space\n\nIt turns out that simply counting the number of borrowers two books is not a great measure of similarity. The problem is that two very popular books, each with 100 borrowers, that only share 10% of their borrowers would be considered as similar as two books, each with 10 readers, that share all of their borrowers. An easy way to correct this is to "normalise" the borrower counts by making sure the similarity of a book with itself is always equal to 1. A common way of doing this is by dividing the inner product of two books by the "size" of each of the vectors for those books. \n\nMathematically, we will denote the size of a book vector [tex]mathbf{b}_i[/tex] as [tex]|mathbf{b}_i| = sqrt{left<mathbf{b}_i,mathbf{b}_iright>}[/tex]. The similarity between two books then becomes:\n\n<center>\n[tex]displaystyle\n    text{sim}(mathbf{b}_i,mathbf{b}_j) \n     = frac{left<mathbf{b}_i,mathbf{b}_jright>}{|mathbf{b}_i||mathbf{b}_j|}\n[/tex]\n</center>\n\nPrincipal Component Analysis\n---------------------------------\nNow that we have a similarity measure between books the idea is to create a plot of points -- one per book -- so that similar books are placed close together and dissimilar books are kept far apart. \n\nA standard technique for doing this is [Principal Component Analysis][pca]. Intuitively, this technique aims to find a way of reducing the number of coordinates in each book vector  in such a way that when the similarity between two books is computed using these smaller vectors it is as close as possible to the original similarity. That is, PCA creates a new table that represents books in terms of only two columns.\n\n[pca]: http://en.wikipedia.org/wiki/Principal_components_analysis\n\n<table class="aligncenter">\n<caption>Table 2: A portion of the book table after PCA. The values in the two new columns (PCA IDs) can be used to plot the books.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="2" class="title">PCA ID</th></tr>\n<tr>                 <th>1</th><th>2</th></tr>\n<tr><th>1</th><td>-8.2</td><td>2.3</td></tr>\n<tr><th>2</th><td>0.4</td><td>-4.3</td></tr>\n<tr><th>3</th><td>-1.3</td><td>-3.7</td></tr>\n<tr><th>...</th><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>2.2</td><td>-5.6</td></tr>\n</table>\n\nTable 2 gives an example of the book table after PCA that reduces the book vectors (rows) from 2,473 to two entries. The PCA columns cannot be as easily interpreted as the borrowers columns in Table 1 but the values in the columns are such that the similarity of the books in Table 2 are roughly as similar as if the values in Table 1 were used. That is, if [tex]mathbf{c}_1 = (-8.2,2.3)[/tex] and [tex]mathbf{c}_2=(0.4,-4.3)[/tex] are the vectors\nfor the first two rows of Table 2 then [tex]text{sim}(mathbf{c}_1,mathbf{c}_2)[/tex]\nwould be close to [tex]text{sim}(mathbf{b}_1,mathbf{b}_2)[/tex], the similarity of the\nfirst two rows in Table 1.[^1]\n\n[^1]: Technically, the guarantee of the "closeness" of the similarity measures only holds on average, that is, over all possible pairs of books. There is no guarantee any particular pair's\nsimilarity is estimated well.\n\nVisualising the Data\n----------------------\nFigure 1 shows a plot of the PCA reduced book data. Each circle represents one of the 1,616 books, plotted according to the coordinates in a table like Table 2. The size of each circle indicates how many borrowers each book had and its colour indicates which library the book belongs to.[^2]\n\n[^2]: A book can belong to more than one library. In this case one library is chosen at random to determine a circle's colour.\n\n<div class="image">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/06/all_libraries.png" alt="Plot of the books across all libraries in the ACRP database" width="550" class="aligncenter wp-image-43" />\n<p>Figure 1: A PCA plot of all the books in the ACRP database coloured according to which library they belong to. The size of each circle indicates the number of borrowers of the corresponding book.\n</div>\n\nOne immediate observation is that books are clustered according to which library they belong to. This is not too surprising since the books in a library limit what borrowers from that library can read. This means it is likely that two voracious readers that frequent the same library will read the same books. This, in turn, will mean the similarity of two books from a library will be higher than books from different libraries as there are very few borrowers that use more than one library.\n\nDrilling Down and Interacting\n---------------------------------\nTo get a better picture of the data, we decided to focus on books from a single library to avoid this clustering. The library we focused on was the [Lambton][] Miners' and Mechanics' Institute in New South Wales. This library had the largest number of loans (20,253) and so was most likely to have interesting similarity data.\n\n[lambton]: http://en.wikipedia.org/wiki/Lambton,_New_South_Wales\n\nThere are a total of 789 books in the Lambton institute and 469 borrowers of those books. A separate PCA reduction was performed on this restricted part of the database to create a plot of only the Lambton books.\n\nTo make it easier to explore this data, I wrote a simple tool that allows a viewer to interact with the PCA plot. A screenshot from this tool is shown in Figure 2. Once again, larger circles represent books with a larger number of borrowers. \n\nClicking on the figure will open a new window and, after a short delay, the tool will run. The same page can also be accessed from [this link][applet]. \n\n[applet]: /inductio/wp-content/public/acrp/\n\n<div class="image">\n<a href='http://conflate.net/inductio/wp-content/public/acrp/' target="_"><img src="http://conflate.net/inductio/wp-content/uploads/2008/06/acrp.png" alt="Click to open visualisation applet" width="550" class="aligncenter wp-image-41" /></a>\n<p>Figure 2: A screenshot of the ACRP visualisation tool showing books from the Lambton Institute. Click the image to run the tool in a new window.</p>\n</div>\n\nInstructions describing how to use the tool can be found below it. \nIn a nutshell: hovering over a circle will reveal the title of the book corresponding to that circle; clicking on a circle will draw lines to its most similar neighbours; altering the "Borrowers" bar will only show books with at least that many borrowers; and altering the "Similarity" bar will only draw lines to books with at least that proportion of books in common.\n\nFuture Work and Distant Reading\n-------------------------------------\nJulieanne and I are still at the early stages of our research using the ACRP database. The use of PCA for visualisation was a first step in our pursuit of what [Franco Moretti][] calls "distant reading" -- looking at books as objects and how they are read rather than the "close reading" of the text of individual books. \n\n[Franco Moretti]: http://en.wikipedia.org/wiki/Franco_Moretti \n\nNow that we have this tool, we are able to quickly explore relationships between these books based on the reading habits of Australians at the turn of the century. Of course, there are many caveats that apply to any patterns we might see in these plots. For instance, the similarity between books is only based on habits of a small number of readers and will be influenced by the peculiarities of the libraries and the books they choose to buy. For this reason, these plots are not intended to provide conclusive answers to questions we might. \n\nInstead we hope that exploring the ACRP database in this way will lead us to interesting questions about particular pairs or groups of books that can be followed up by a more thorough analysis of their readers, their text as well as other historical and cultural factors about them.\n\nData and Code\n----------------\nFor the technically minded, I have made the code I used to do the visualisation is available on [GitHub][]. It is a combination of [SQL][] for data preprocessing, [R][] for the PCA reduction and [Processing][] for creating the visualisation tool. You will also find a number of images and some notes at the same location.\n\n[github]: http://github.com/mreid/acrp/tree/master \n[SQL]: http://en.wikipedia.org/wiki/SQL\n[R]: http://www.r-project.org/\n[Processing]: http://processing.org/\n\nAccess to the data that the code acts upon is not mine to give, so the code is primarily to show how I did the visualisation rather than a way to let others analyse the data. If the founders of the [ACRP][] project decide to release the data to the public at a later date I will link to it from here.\n\n
158	Behold! Jensen's Inequality	143-revision-13	2008-11-17 05:49:27	2008-11-17 05:49:27	Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.	inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki][] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was quite happy then to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it. \n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub>"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\nUnfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen's inequality for a discrete distribution where the random variable [tex]x[/tex] takes on one of the <i>n</i> values [tex]x_i[/tex] with with probability [tex]p_i[/tex].\n\nNote that the points [tex](x_i, f(x_i))[/tex] form the vertices of a polygon which, by the convexity of , must also be convex and lie within its epigraph (the blue shaded area above ). Furthermore, since the [tex]p_i[/tex] are probabilities they satisfy [tex]sum_i p_i = 1[/tex]. This means the expected value of the random variable [tex](x, f(x))[/tex] given by\n<center>\n[tex]displaystyle \n   mathbb{E}[(x, f(x))] = sum_{i=1}^n p_i left(x_i, f(x_i)right)\n[/tex]\n</center>\nis a convex combination and so must also lie within the dashed polygon. Thus, \n\nThe general result for non-discrete distributions can also be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more [tex]x_i[/tex] to the diagram the dashed polygon the shaded area will approximate the graph of  better. So, by the earlier argument for the discrete case, the expected value of [tex]x[/tex] will remain within the polygon and thus within the shaded area and thus above . Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.\n\nA somewhat surprising fact about Jensen's inequality is that its converse is also true. By this I mean that if  is a function such that its Jensen gap [tex]J_f(x)[/tex] is non-negative for all distributions of the random variable <i>x</i> then  is necessarily convex. The contrapositive of this statement is:  non-convex implies the existence of a random variable <i>x</i> so that [tex]J_f(x) < 0[/tex].\n\nConsidering Figure 1 again gives some intuition as to why this must be the case. If  was non-convex then its epigraph (the blue shaded area) must, by definition, also be non-convex. This means I could choose some [tex]x_i[/tex] so that one of the dashed lines lies outside the shaded area. This means I can then choose [tex]p_i[/tex] so that the mean [tex]mathbb{E}[(x, f(x))][/tex] lies outside the shaded area and thus below the graph of .\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
159	Behold! Jensen's Inequality	143-revision-14	2008-11-17 06:21:18	2008-11-17 06:21:18	Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.	inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki][] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was quite happy then to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it. \n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub>"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\nUnfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen's inequality for a discrete distribution where the random variable [tex]x[/tex] takes on one of the <i>n</i> values [tex]x_i[/tex] with with probability [tex]p_i[/tex].\n\nNote that the points [tex](x_i, f(x_i))[/tex] form the vertices of a polygon which, by the convexity of , must also be convex and lie within the epigraph of  (the blue shaded area above ). Furthermore, since the [tex]p_i[/tex] are probabilities they satisfy [tex]sum_i p_i = 1[/tex]. This means the expected value of the random variable [tex](x, f(x))[/tex] given by\n<center>\n[tex]displaystyle \n   mathbb{E}[(x, f(x))] = sum_{i=1}^n p_i left(x_i, f(x_i)right) \n   = left(mathbb{E}[x], mathbb{E}[f(x)]right)\n[/tex]\n</center>\nis a convex combination and so must also lie within the dashed polygon. In fact, since [tex]mathbb{E}[(x, f(x))] = left(mathbb{E}[x], mathbb{E}[f(x)]right)[/tex] it must lie above [tex]fleft(mathbb{E}[x]right)[/tex] thus giving the result.\n\nAlthough the diagram in Figure 1 assumes a 1-dimension space <i>X</i> the above argument generalises to higher dimensions in an analogous manner. Also, the general result for non-discrete distributions can be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more [tex]x_i[/tex] to the diagram the dashed polygon the shaded area will approximate the graph of  better. So, by the earlier argument for the discrete case, the expected value of [tex]x[/tex] will remain within the polygon and thus within the shaded area and thus above . Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.\n\nA somewhat surprising fact about Jensen's inequality is that its converse is also true. By this I mean that if  is a function such that its Jensen gap [tex]J_f(x)[/tex] is non-negative for all distributions of the random variable <i>x</i> then  is necessarily convex. The contrapositive of this statement is:  non-convex implies the existence of a random variable <i>x</i> so that [tex]J_f(x) < 0[/tex].\n\nConsidering Figure 1 again gives some intuition as to why this must be the case. If  was non-convex then its epigraph (the blue shaded area) must, by definition, also be non-convex. This means I could choose some [tex]x_i[/tex] so that one of the dashed lines lies outside the shaded area. This means I can then choose [tex]p_i[/tex] so that the mean [tex]mathbb{E}[(x, f(x))][/tex] lies outside the shaded area and thus below the graph of .\n\nOf course, no self-respecting mathematician would call the above arguments a proof of Jensen's inequality. There are too many edge cases and subtleties (especially in the continuous case) that I've ignored. That said, I believe the statement and thrust of the inequality can be quickly arrived at from the simple diagram above. When using tools like Jensen's inequality, I find this type of quick insight more valuable than a long, careful technical statement and proof. The latter is valuable to but if I need this level of detail\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
160	Behold! Jensen's Inequality	143-revision-15	2008-11-17 06:21:56	2008-11-17 06:21:56	Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.	inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but very important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki][] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was quite happy then to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it. \n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub>"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\nUnfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen's inequality for a discrete distribution where the random variable [tex]x[/tex] takes on one of the <i>n</i> values [tex]x_i[/tex] with with probability [tex]p_i[/tex].\n\nNote that the points [tex](x_i, f(x_i))[/tex] form the vertices of a polygon which, by the convexity of , must also be convex and lie within the epigraph of  (the blue shaded area above ). Furthermore, since the [tex]p_i[/tex] are probabilities they satisfy [tex]sum_i p_i = 1[/tex]. This means the expected value of the random variable [tex](x, f(x))[/tex] given by\n<center>\n[tex]displaystyle \n   mathbb{E}[(x, f(x))] = sum_{i=1}^n p_i left(x_i, f(x_i)right) \n   = left(mathbb{E}[x], mathbb{E}[f(x)]right)\n[/tex]\n</center>\nis a convex combination and so must also lie within the dashed polygon. In fact, since [tex]mathbb{E}[(x, f(x))] = left(mathbb{E}[x], mathbb{E}[f(x)]right)[/tex] it must lie above [tex]fleft(mathbb{E}[x]right)[/tex] thus giving the result.\n\nAlthough the diagram in Figure 1 assumes a 1-dimension space <i>X</i> the above argument generalises to higher dimensions in an analogous manner. Also, the general result for non-discrete distributions can be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more [tex]x_i[/tex] to the diagram the dashed polygon the shaded area will approximate the graph of  better. So, by the earlier argument for the discrete case, the expected value of [tex]x[/tex] will remain within the polygon and thus within the shaded area and thus above . Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.\n\nA somewhat surprising fact about Jensen's inequality is that its converse is also true. By this I mean that if  is a function such that its Jensen gap [tex]J_f(x)[/tex] is non-negative for all distributions of the random variable <i>x</i> then  is necessarily convex. The contrapositive of this statement is:  non-convex implies the existence of a random variable <i>x</i> so that [tex]J_f(x) < 0[/tex].\n\nConsidering Figure 1 again gives some intuition as to why this must be the case. If  was non-convex then its epigraph (the blue shaded area) must, by definition, also be non-convex. This means I could choose some [tex]x_i[/tex] so that one of the dashed lines lies outside the shaded area. This means I can then choose [tex]p_i[/tex] so that the mean [tex]mathbb{E}[(x, f(x))][/tex] lies outside the shaded area and thus below the graph of .\n\nOf course, no self-respecting mathematician would call the above arguments a proof of Jensen's inequality. There are too many edge cases and subtleties (especially in the continuous case) that I've ignored. That said, I believe the statement and thrust of the inequality can be quickly arrived at from the simple diagram above. When using tools like Jensen's inequality, I find this type of quick insight more valuable than a long, careful technical statement and proof. The latter is valuable to but if I need this level of detail I would look it up rather than try to dredge it up from my sometimes unreliable memory.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
161	Behold! Jensen's Inequality	143-revision-16	2008-11-17 06:24:32	2008-11-17 06:24:32	Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.	inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over some finite dimensional convex set <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki][] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was quite happy then to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it. \n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub>"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\nUnfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen's inequality for a discrete distribution where the random variable [tex]x[/tex] takes on one of the <i>n</i> values [tex]x_i[/tex] with with probability [tex]p_i[/tex].\n\nNote that the points [tex](x_i, f(x_i))[/tex] form the vertices of a polygon which, by the convexity of , must also be convex and lie within the epigraph of  (the blue shaded area above ). Furthermore, since the [tex]p_i[/tex] are probabilities they satisfy [tex]sum_i p_i = 1[/tex]. This means the expected value of the random variable [tex](x, f(x))[/tex] given by\n<center>\n[tex]displaystyle \n   mathbb{E}[(x, f(x))] = sum_{i=1}^n p_i left(x_i, f(x_i)right) \n[/tex]\n</center>\nis a convex combination and so must also lie within the dashed polygon. In fact, since [tex]mathbb{E}[(x, f(x))] = left(mathbb{E}[x], mathbb{E}[f(x)]right)[/tex] it must lie above [tex]fleft(mathbb{E}[x]right)[/tex] thus giving the result.\n\nAlthough the diagram in Figure 1 assumes a 1-dimension space <i>X</i> the above argument generalises to higher dimensions in an analogous manner. Also, the general result for non-discrete distributions can be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more [tex]x_i[/tex] to the diagram the dashed polygon the shaded area will approximate the graph of  better. So, by the earlier argument for the discrete case, the expected value of [tex]x[/tex] will remain within the polygon and thus within the shaded area and thus above . Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.\n\nA somewhat surprising fact about Jensen's inequality is that its converse is also true. By this I mean that if  is a function such that its Jensen gap [tex]J_f(x)[/tex] is non-negative for all distributions of the random variable <i>x</i> then  is necessarily convex. The contrapositive of this statement is:  non-convex implies the existence of a random variable <i>x</i> so that [tex]J_f(x) < 0[/tex].\n\nConsidering Figure 1 again gives some intuition as to why this must be the case. If  was non-convex then its epigraph (the blue shaded area) must, by definition, also be non-convex. This means I could choose some [tex]x_i[/tex] so that one of the dashed lines lies outside the shaded area. This means I can then choose [tex]p_i[/tex] so that the mean [tex]mathbb{E}[(x, f(x))][/tex] lies outside the shaded area and thus below the graph of .\n\nOf course, no self-respecting mathematician would call the above arguments a proof of Jensen's inequality. There are too many edge cases and subtleties (especially in the continuous case) that I've ignored. That said, I believe the statement and thrust of the inequality can be quickly arrived at from the simple diagram above. When using tools like Jensen's inequality, I find this type of quick insight more valuable than a long, careful technical statement and proof. The latter is valuable to but if I need this level of detail I would look it up rather than try to dredge it up from my sometimes unreliable memory.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
162	Behold! Jensen's Inequality	143-revision-17	2008-11-17 06:24:51	2008-11-17 06:24:51	Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.	inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over some finite dimensional convex set <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki][] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was quite happy then to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it. \n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub>"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\nUnfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen's inequality for a discrete distribution where the random variable [tex]x[/tex] takes on one of the <i>n</i> values [tex]x_i[/tex] with with probability [tex]p_i[/tex].\n\nNote that the points [tex](x_i, f(x_i))[/tex] form the vertices of a polygon which, by the convexity of , must also be convex and lie within the epigraph of  (the blue shaded area above ). Furthermore, since the [tex]p_i[/tex] are probabilities they satisfy [tex]sum_i p_i = 1[/tex]. This means the expected value of the random variable [tex](x, f(x))[/tex] given by\n<center>\n[tex]displaystyle \n   mathbb{E}[(x, f(x))] = sum_{i=1}^n p_i left(x_i, f(x_i)right) \n[/tex]\n</center>\nis a convex combination and so must also lie within the dashed polygon. In fact, since [tex]mathbb{E}[(x, f(x))] = left(mathbb{E}[x], mathbb{E}[f(x)]right)[/tex] it must lie above [tex]fleft(mathbb{E}[x]right)[/tex] thus giving the result.\n\nAlthough the diagram in Figure 1 assumes a 1-dimension space <i>X</i> the above argument generalises to higher dimensions in an analogous manner. Also, the general result for non-discrete distributions can be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more [tex]x_i[/tex] to the diagram the dashed polygon the shaded area will approximate the graph of  better. So, by the earlier argument for the discrete case, the expected value of [tex]x[/tex] will remain within the polygon and thus within the shaded area and thus above . Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.\n\nA somewhat surprising fact about Jensen's inequality is that its converse is also true. By this I mean that if  is a function such that its Jensen gap [tex]J_f(x)[/tex] is non-negative for all distributions of the random variable <i>x</i> then  is necessarily convex. The contrapositive of this statement is:  non-convex implies the existence of a random variable <i>x</i> so that [tex]J_f(x) < 0[/tex].\n\nConsidering Figure 1 again gives some intuition as to why this must be the case. If  was non-convex then its epigraph must, by definition, also be non-convex. This means I could choose some [tex]x_i[/tex] so that one of the dashed lines lies outside the shaded area. This means I can then choose [tex]p_i[/tex] so that the mean [tex]mathbb{E}[(x, f(x))][/tex] lies outside the shaded area and thus below the graph of .\n\nOf course, no self-respecting mathematician would call the above arguments a proof of Jensen's inequality. There are too many edge cases and subtleties (especially in the continuous case) that I've ignored. That said, I believe the statement and thrust of the inequality can be quickly arrived at from the simple diagram above. When using tools like Jensen's inequality, I find this type of quick insight more valuable than a long, careful technical statement and proof. The latter is valuable to but if I need this level of detail I would look it up rather than try to dredge it up from my sometimes unreliable memory.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
163	Behold! Jensen's Inequality	143-revision-18	2008-11-17 06:25:38	2008-11-17 06:25:38	Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.	inherit	I have been making quite a bit of use of Jensen's inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if  is a real-valued convex function over some finite dimensional convex set <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the _Jensen gap_\n<center>\n[tex]\ndisplaystyle J_f(x) := mathbb{E}left[ fleft(xright) right] - fleft(mathbb{E}left[ x right]right)\n[/tex]\n</center>\nwhere [tex]mathbb{E}[/tex] denotes expectation. Jensen's inequality states that this gap is never negative, that is, [tex]J_f(x) geq 0[/tex] or equivalently,\n<center>\n[tex]\ndisplaystyle mathbb{E}left[ fleft(xright) right] geq fleft(mathbb{E}left[ x right]right).\n[/tex]\n</center>\n\nThis is a fairly simple but important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a [general AM-GM inequality][amgm] and many results in information theory. I've been interested in it because DeGroot's notion of [statistical information][uise] and measures of the distance between probability distributions called [f-divergences][] can both be expressed as a Jensen gap and consequently related to each other.\n\nJensen's inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I've read, including those in books by [Rockafellar][] and by [Dudley][] feel like they are from the [Bourbaki][] school in that they present the proof without recourse to any diagrams.\n\n[rockafellar]: http://books.google.com/books?id=wj4Fh4h_V7QC\n[dudley]: http://books.google.com/books?id=Wv_zxEExK3QC\n\nI was quite happy then to have found a graphical "proof" of Jensen's inequality. By this I mean a proof in the style of the [proof of Pythagoras' theorem][pythagoras] that is simply a diagram with the word "Behold!" above it. \n\n[caption id="attachment_154" align="aligncenter" width="485" caption="Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub>"]<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png"><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen&#039;s Inequality" title="Jensen&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a>[/caption]\n\nUnfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen's inequality for a discrete distribution where the random variable [tex]x[/tex] takes on one of the <i>n</i> values [tex]x_i[/tex] with with probability [tex]p_i[/tex].\n\nNote that the points [tex](x_i, f(x_i))[/tex] form the vertices of a polygon which, by the convexity of , must also be convex and lie within the epigraph of  (the blue shaded area above ). Furthermore, since the [tex]p_i[/tex] are probabilities they satisfy [tex]sum_i p_i = 1[/tex]. This means the expected value of the random variable [tex](x, f(x))[/tex] given by\n<center>\n[tex]displaystyle \n   mathbb{E}[(x, f(x))] = sum_{i=1}^n p_i left(x_i, f(x_i)right) \n[/tex]\n</center>\nis a convex combination and so must also lie within the dashed polygon. In fact, since [tex]mathbb{E}[(x, f(x))] = left(mathbb{E}[x], mathbb{E}[f(x)]right)[/tex] it must lie above [tex]fleft(mathbb{E}[x]right)[/tex] thus giving the result.\n\nAlthough the diagram in Figure 1 assumes a 1-dimensional space <i>X</i> the above argument generalises to higher dimensions in an analogous manner. Also, the general result for non-discrete distributions can be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more [tex]x_i[/tex] to the diagram the dashed polygon the shaded area will approximate the graph of  better. So, by the earlier argument for the discrete case, the expected value of [tex]x[/tex] will remain within the polygon and thus within the shaded area and thus above . Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.\n\nA somewhat surprising fact about Jensen's inequality is that its converse is also true. By this I mean that if  is a function such that its Jensen gap [tex]J_f(x)[/tex] is non-negative for all distributions of the random variable <i>x</i> then  is necessarily convex. The contrapositive of this statement is:  non-convex implies the existence of a random variable <i>x</i> so that [tex]J_f(x) < 0[/tex].\n\nConsidering Figure 1 again gives some intuition as to why this must be the case. If  was non-convex then its epigraph must, by definition, also be non-convex. This means I could choose some [tex]x_i[/tex] so that one of the dashed lines lies outside the shaded area. This means I can then choose [tex]p_i[/tex] so that the mean [tex]mathbb{E}[(x, f(x))][/tex] lies outside the shaded area and thus below the graph of .\n\nOf course, no self-respecting mathematician would call the above arguments a proof of Jensen's inequality. There are too many edge cases and subtleties (especially in the continuous case) that I've ignored. That said, I believe the statement and thrust of the inequality can be quickly arrived at from the simple diagram above. When using tools like Jensen's inequality, I find this type of quick insight more valuable than a long, careful technical statement and proof. The latter is valuable to but if I need this level of detail I would look it up rather than try to dredge it up from my sometimes unreliable memory.\n\n[f-divergences]: http://en.wikipedia.org/wiki/F-divergence\n[uise]: http://projecteuclid.org/euclid.aoms/1177704567\n[pythagoras]: http://www.math.ntnu.no/~hanche/pythagoras/\n[amgm]: http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality\n[bourbaki]: http://en.wikipedia.org/wiki/Nicolas_Bourbaki
164	Machine Learning Summer School 2009	machine-learning-summer-school-2009	2008-11-18 11:18:27	2008-11-18 11:18:27	A plug for the 2009 Machine Learning Summer School in Canberra, Australia. I will be giving a presentation there.	publish	The annual [Machine Learning Summer School][mlss] is being held in Canberra at the [Australian National University][anu] in January next year. It will be part of the joint [Summer Schools in Logic and Learning][ssll]. \n\nFrom the 2009 [MLSS website][mlss2009]:\n> This school is suitable for all levels, both for people without previous knowledge in \n> Machine Learning, and those wishing to broaden their expertise in this area. It will \n> allow the participants to get in touch with international experts in this field. \n> Exchange of students, joint publications and joint projects will result because \n> of this collaboration. \n\nThe Summer schools will run from the 26-30th of January 2009 and [registration][] is open. Note that there is a 20% surcharge for registrations after the 19th of December 2008 so get registering.\n\nI've been fortunate enough to have been given a spot on the [program][]. I'll be talking about some the work I've been doing with [Bob Williamson][bob] this year on analysing relationships between various notions of risk, divergence and information in binary valued prediction problems.\n\nLeave a comment if you're planning to attend and I'll make sure I say hi. \n\nHope to see you there!\n\n[mlss]: http://mlss.cc/\n[mlss2009]: http://ssll.cecs.anu.edu.au/about/mlss\n[anu]: http://anu.edu.au/\n[ssll]: http://ssll.cecs.anu.edu.au/\n[program]: http://ssll.cecs.anu.edu.au/program\n[bob]: http://axiom.anu.edu.au/~williams/\n[registration]: http://ssll.cecs.anu.edu.au/registration
165	Machine Learning Summer School 2009	164-revision	2008-11-18 11:16:49	2008-11-18 11:16:49	A plug for the 2009 Machine Learning Summer School in Canberra, Australia. I will be giving a presentation there.	inherit	The annual [Machine Learning Summer School][mlss] is being held in Canberra at the [Australian National University][anu] in January next year. It will be part of the joint [Summer Schools in Logic and Learning][ssll]. \n\nFrom the 2009 [MLSS website][mlss2009]:\n> This school is suitable for all levels, both for people without previous knowledge in \n> Machine Learning, and those wishing to broaden their expertise in this area. It will \n> allow the participants to get in touch with international experts in this field. \n> Exchange of students, joint publications and joint projects will result because \n> of this collaboration. \n\nThe Summer schools will run from the 26-30th of January 2009 and [registration][] is open. Note that there is a 20% surcharge for registrations after the 19th of December 2008 so get registering.\n\nI've been fortunate enough to have been given a spot on the [program][]. I'll be talking about some the work I've been doing with [Bob Williamson][bob] this year on analysing relationships between various notions of risk, divergence and information in binary valued prediction problems.\n\nLeave a comment if you're planning to attend and I'll make sure I say hi. Hope to see you there!\n\n[mlss]: http://mlss.cc/\n[mlss2009]: http://ssll.cecs.anu.edu.au/about/mlss\n[anu]: http://anu.edu.au/\n[ssll]: http://ssll.cecs.anu.edu.au/\n[program]: http://ssll.cecs.anu.edu.au/program\n[bob]: http://axiom.anu.edu.au/~williams/
166	Machine Learning Summer School 2009	164-revision-2	2008-11-18 11:17:37	2008-11-18 11:17:37	A plug for the 2009 Machine Learning Summer School in Canberra, Australia. I will be giving a presentation there.	inherit	The annual [Machine Learning Summer School][mlss] is being held in Canberra at the [Australian National University][anu] in January next year. It will be part of the joint [Summer Schools in Logic and Learning][ssll]. \n\nFrom the 2009 [MLSS website][mlss2009]:\n> This school is suitable for all levels, both for people without previous knowledge in \n> Machine Learning, and those wishing to broaden their expertise in this area. It will \n> allow the participants to get in touch with international experts in this field. \n> Exchange of students, joint publications and joint projects will result because \n> of this collaboration. \n\nThe Summer schools will run from the 26-30th of January 2009 and [registration][] is open. Note that there is a 20% surcharge for registrations after the 19th of December 2008 so get registering.\n\nI've been fortunate enough to have been given a spot on the [program][]. I'll be talking about some the work I've been doing with [Bob Williamson][bob] this year on analysing relationships between various notions of risk, divergence and information in binary valued prediction problems.\n\nLeave a comment if you're planning to attend and I'll make sure I say hi. \n\nHope to see you there!\n\n[mlss]: http://mlss.cc/\n[mlss2009]: http://ssll.cecs.anu.edu.au/about/mlss\n[anu]: http://anu.edu.au/\n[ssll]: http://ssll.cecs.anu.edu.au/\n[program]: http://ssll.cecs.anu.edu.au/program\n[bob]: http://axiom.anu.edu.au/~williams/\n[registration]: http://ssll.cecs.anu.edu.au/registration
167	Visualising 19th Century Reading in Australia	40-autosave	2008-12-09 12:04:10	2008-12-09 12:04:10	A description of a visualisation of some 19th century Australian borrowing records from the Australian Common Readers Project.	inherit	_Update - 9 Dec 2008_: Julieanne and I presented a much improved version of this visualisation at the [Resourceful Reading][] conference held at the University of Sydney on the 5th of December. I will post the updated application with notes shortly.\n\n[Resourceful Reading]: http://conferences.arts.usyd.edu.au/index.php?cf=20\n\nI've recently spent a bit of time collaborating with my wife on a research project. Research collaboration by couples is not new but given that Julieanne is a [lecturer in the English program][j] and I'm part of the [computer sciences laboratory][csl], this piece of joint research is a little unusual. \n\nThe rest of this post describes the intersection of our interests --- data from the Australian Common Reader Project --- and the visualisation tool I wrote to explore it. The tool itself is based on a simple application of linear Principal Component Analysis (PCA). I'll attempt to explain it here in such a way that readers who have not studied this technique might still be able to make use of the tool.\n\n[j]: http://cass.anu.edu.au/humanities/school_sites/staff.php\n[csl]: http://csl.cecs.anu.edu.au/\n\nThe Australian Common Reader Project\n--------------------------------------------\nOne of Julieanne's research interests is the Australian audience of the late 19th and early 20th centuries. As part of her PhD, she made use of an amazing database that is part of the [Australian Common Reader Project][acrp] --- a project that has collected and entered library borrowing records from Australian libraries along with annotations about when books were borrowed, their genres, borrower occupations, author information, <i>etc</i>. This sort of information makes it possible for Australian literature and cultural studies academics to ask empirical questions about Australian readers' relationship with books and periodicals. \n\n[acrp]: http://www.api-network.com/hosted/acrp/\n\nEver on the lookout for [interesting data sets][meta-index], I suggested that we apply some basic data analysis tools to the database to see what kind of relationships between books and borrowers we might find. When asked if we could have access to the database, [Tim Dolin][] graciously agreed and enlisted [Jason Ensor][] to help with our technical questions. \n\n[meta-index]: http://conflate.net/inductio/2008/02/a-meta-index-of-data-sets/\n[tim dolin]: http://www.humanities.curtin.edu.au/staff.cfm/t.dolin\n[jason ensor]: http://www.humanities.curtin.edu.au/staff.cfm/j.ensor \n\nBooks and Borrowers\n------------------------\nAfter an initial inspection, my first thought was to try to visualise the similarity of the books in the database as measured by the number of borrowers they have in common. \nThe full database contains 99,692 loans of 7,078 different books from 11 libraries by one of the 2,642 people. To make this more manageable, I focused on books that had at least 20 different borrowers and only considered people who had borrowed one of these books.\nThis distilled the database down to a simple table with each row representing one of 1,616 books and each column representing one of 2,473 people. \n\n<table>\n<caption>Table 1: A portion of the book and borrower table. A 1 indicates that the borrower (column)\nborrowed the book (row) at least once. A 0 indicates that the borrower never borrowed the book.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="4" class="title">Borrower ID</th></tr>\n<tr><th>1</th><th>2</th><th>...</th><th>2,473</th></tr>\n<tr><th>1</th><td>1</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>2</th><td>1</td><td>1</td><td>...</td><td>0</td></tr>\n<tr><th>3</th><td>0</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>1</td><td>1</td><td>...</td><td>1</td></tr>\n</table>\n\nConceptually, each cell in the table contains a 1 if the person associated with the cell's column borrowed the book associated with the cell's row. If there was no such loan between a given book and borrower the corresponding cell contains a 0. For example, Table 1 shows that book 2 was borrowed (at least once) by borrower 1 but never by borrower 2,473.\n\nBook Similarity\n-----------------\nThe table view of the books and their borrowers does not readily lend itself to insight. The approach we took to get a better picture of this information was to plot each book as a point on a graph so that similar books are placed closer together than dissimilar books. To do this a notion of what "similar books" is required.\n\nMathematically, row [tex]i[/tex] of Table 1 can be represented as a vector [tex]\\mathbf{b}_i[/tex] of 1s and 0s. The value of the cell in the [tex]j[/tex]<sup>th</sup> column of that row will be denoted [tex]b_{i,j}[/tex]. For example, the 2<sup>nd</sup> row in the table can be written as the vector [tex]\\mathbf{b}_2 = (1,1,\\ldots,0)[/tex] and the value in its first column is [tex]b_{2,1} = 1[/tex].\n\nA crude measure of the similarity between book 1 and book 2 can be computed from this table by counting how many borrowers they have in common. That is, the number of columns that have a `1` in the row for book 1 and the row for book 2. \n \nIn terms of the vector representation, this similarity measure is simply the "[inner product][]" between [tex]\\mathbf{b}_1[/tex] and [tex]\\mathbf{b}_2[/tex] and is written [tex]\\left<\\mathbf{b}_1,\\mathbf{b}_2\\right> = b_{1,1}b_{2,1} + \\cdots + b_{1,N}b_{2,N}[/tex] where N = 2,473 is the total number of borrowers.\n\n[inner product]: http://en.wikipedia.org/wiki/Inner_product_space\n\nIt turns out that simply counting the number of borrowers two books is not a great measure of similarity. The problem is that two very popular books, each with 100 borrowers, that only share 10% of their borrowers would be considered as similar as two books, each with 10 readers, that share all of their borrowers. An easy way to correct this is to "normalise" the borrower counts by making sure the similarity of a book with itself is always equal to 1. A common way of doing this is by dividing the inner product of two books by the "size" of each of the vectors for those books. \n\nMathematically, we will denote the size of a book vector [tex]\\mathbf{b}_i[/tex] as [tex]\\|\\mathbf{b}_i\\| = \\sqrt{\\left<\\mathbf{b}_i,\\mathbf{b}_i\\right>}[/tex]. The similarity between two books then becomes:\n\n<center>\n[tex]\\displaystyle\n    \\text{sim}(\\mathbf{b}_i,\\mathbf{b}_j) \n     = \\frac{\\left<\\mathbf{b}_i,\\mathbf{b}_j\\right>}{\\|\\mathbf{b}_i\\|\\|\\mathbf{b}_j\\|}\n[/tex]\n</center>\n\nPrincipal Component Analysis\n---------------------------------\nNow that we have a similarity measure between books the idea is to create a plot of points -- one per book -- so that similar books are placed close together and dissimilar books are kept far apart. \n\nA standard technique for doing this is [Principal Component Analysis][pca]. Intuitively, this technique aims to find a way of reducing the number of coordinates in each book vector  in such a way that when the similarity between two books is computed using these smaller vectors it is as close as possible to the original similarity. That is, PCA creates a new table that represents books in terms of only two columns.\n\n[pca]: http://en.wikipedia.org/wiki/Principal_components_analysis\n\n<table>\n<caption>Table 2: A portion of the book table after PCA. The values in the two new columns (PCA IDs) can be used to plot the books.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="2" class="title">PCA ID</th></tr>\n<tr>                 <th>1</th><th>2</th></tr>\n<tr><th>1</th><td>-8.2</td><td>2.3</td></tr>\n<tr><th>2</th><td>0.4</td><td>-4.3</td></tr>\n<tr><th>3</th><td>-1.3</td><td>-3.7</td></tr>\n<tr><th>...</th><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>2.2</td><td>-5.6</td></tr>\n</table>\n\nTable 2 gives an example of the book table after PCA that reduces the book vectors (rows) from 2,473 to two entries. The PCA columns cannot be as easily interpreted as the borrowers columns in Table 1 but the values in the columns are such that the similarity of the books in Table 2 are roughly as similar as if the values in Table 1 were used. That is, if [tex]\\mathbf{c}_1 = (-8.2,2.3)[/tex] and [tex]\\mathbf{c}_2=(0.4,-4.3)[/tex] are the vectors\nfor the first two rows of Table 2 then [tex]\\text{sim}(\\mathbf{c}_1,\\mathbf{c}_2)[/tex]\nwould be close to [tex]\\text{sim}(\\mathbf{b}_1,\\mathbf{b}_2)[/tex], the similarity of the\nfirst two rows in Table 1.[^1]\n\n[^1]: Technically, the guarantee of the "closeness" of the similarity measures only holds on average, that is, over all possible pairs of books. There is no guarantee any particular pair's\nsimilarity is estimated well.\n\nVisualising the Data\n----------------------\nFigure 1 shows a plot of the PCA reduced book data. Each circle represents one of the 1,616 books, plotted according to the coordinates in a table like Table 2. The size of each circle indicates how many borrowers each book had and its colour indicates which library the book belongs to.[^2]\n\n[^2]: A book can belong to more than one library. In this case one library is chosen at random to determine a circle's colour.\n\n<div class="image">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/06/all_libraries.png" alt="Plot of the books across all libraries in the ACRP database" width="550" class="aligncenter wp-image-43" />\n<p>Figure 1: A PCA plot of all the books in the ACRP database coloured according to which library they belong to. The size of each circle indicates the number of borrowers of the corresponding book.\n</div>\n\nOne immediate observation is that books are clustered according to which library they belong to. This is not too surprising since the books in a library limit what borrowers from that library can read. This means it is likely that two voracious readers that frequent the same library will read the same books. This, in turn, will mean the similarity of two books from a library will be higher than books from different libraries as there are very few borrowers that use more than one library.\n\nDrilling Down and Interacting\n---------------------------------\nTo get a better picture of the data, we decided to focus on books from a single library to avoid this clustering. The library we focused on was the [Lambton][] Miners' and Mechanics' Institute in New South Wales. This library had the largest number of loans (20,253) and so was most likely to have interesting similarity data.\n\n[lambton]: http://en.wikipedia.org/wiki/Lambton,_New_South_Wales\n\nThere are a total of 789 books in the Lambton institute and 469 borrowers of those books. A separate PCA reduction was performed on this restricted part of the database to create a plot of only the Lambton books.\n\nTo make it easier to explore this data, I wrote a simple tool that allows a viewer to interact with the PCA plot. A screenshot from this tool is shown in Figure 2. Once again, larger circles represent books with a larger number of borrowers. \n\nClicking on the figure will open a new window and, after a short delay, the tool will run. The same page can also be accessed from [this link][applet]. \n\n[applet]: /inductio/wp-content/public/acrp/\n\n<div class="image">\n<a href='http://conflate.net/inductio/wp-content/public/acrp/' target="_"><img src="http://conflate.net/inductio/wp-content/uploads/2008/06/acrp.png" alt="Click to open visualisation applet" width="550" class="aligncenter wp-image-41" /></a>\n<p>Figure 2: A screenshot of the ACRP visualisation tool showing books from the Lambton Institute. Click the image to run the tool in a new window.</p>\n</div>\n\nInstructions describing how to use the tool can be found below it. \nIn a nutshell: hovering over a circle will reveal the title of the book corresponding to that circle; clicking on a circle will draw lines to its most similar neighbours; altering the "Borrowers" bar will only show books with at least that many borrowers; and altering the "Similarity" bar will only draw lines to books with at least that proportion of books in common.\n\nFuture Work and Distant Reading\n-------------------------------------\nJulieanne and I are still at the early stages of our research using the ACRP database. The use of PCA for visualisation was a first step in our pursuit of what [Franco Moretti][] calls "distant reading" -- looking at books as objects and how they are read rather than the "close reading" of the text of individual books. \n\n[Franco Moretti]: http://en.wikipedia.org/wiki/Franco_Moretti \n\nNow that we have this tool, we are able to quickly explore relationships between these books based on the reading habits of Australians at the turn of the century. Of course, there are many caveats that apply to any patterns we might see in these plots. For instance, the similarity between books is only based on habits of a small number of readers and will be influenced by the peculiarities of the libraries and the books they choose to buy. For this reason, these plots are not intended to provide conclusive answers to questions we might. \n\nInstead we hope that exploring the ACRP database in this way will lead us to interesting questions about particular pairs or groups of books that can be followed up by a more thorough analysis of their readers, their text as well as other historical and cultural factors about them.\n\nData and Code\n----------------\nFor the technically minded, I have made the code I used to do the visualisation is available on [GitHub][]. It is a combination of [SQL][] for data preprocessing, [R][] for the PCA reduction and [Processing][] for creating the visualisation tool. You will also find a number of images and some notes at the same location.\n\n[github]: http://github.com/mreid/acrp/tree/master \n[SQL]: http://en.wikipedia.org/wiki/SQL\n[R]: http://www.r-project.org/\n[Processing]: http://processing.org/\n\nAccess to the data that the code acts upon is not mine to give, so the code is primarily to show how I did the visualisation rather than a way to let others analyse the data. If the founders of the [ACRP][] project decide to release the data to the public at a later date I will link to it from here.\n\n
168	Visualising 19th Century Reading in Australia	40-revision-2	2008-11-17 05:01:20	2008-11-17 05:01:20	A description of a visualisation of some 19th century Australian borrowing records from the Australian Common Readers Project.	inherit	I've recently spent a bit of time collaborating with my wife on a research project. Research collaboration by couples is not new but given that Julieanne is a [lecturer in the English program][j] and I'm part of the [computer sciences laboratory][csl], this piece of joint research is a little unusual. \n\nThe rest of this post describes the intersection of our interests --- data from the Australian Common Reader Project --- and the visualisation tool I wrote to explore it. The tool itself is based on a simple application of linear Principal Component Analysis (PCA). I'll attempt to explain it here in such a way that readers who have not studied this technique might still be able to make use of the tool.\n\n[j]: http://cass.anu.edu.au/humanities/school_sites/staff.php\n[csl]: http://csl.cecs.anu.edu.au/\n\nThe Australian Common Reader Project\n--------------------------------------------\nOne of Julieanne's research interests is the Australian audience of the late 19th and early 20th centuries. As part of her PhD, she made use of an amazing database that is part of the [Australian Common Reader Project][acrp] --- a project that has collected and entered library borrowing records from Australian libraries along with annotations about when books were borrowed, their genres, borrower occupations, author information, <i>etc</i>. This sort of information makes it possible for Australian literature and cultural studies academics to ask empirical questions about Australian readers' relationship with books and periodicals. \n\n[acrp]: http://www.api-network.com/hosted/acrp/\n\nEver on the lookout for [interesting data sets][meta-index], I suggested that we apply some basic data analysis tools to the database to see what kind of relationships between books and borrowers we might find. When asked if we could have access to the database, [Tim Dolin][] graciously agreed and enlisted [Jason Ensor][] to help with our technical questions. \n\n[meta-index]: http://conflate.net/inductio/2008/02/a-meta-index-of-data-sets/\n[tim dolin]: http://www.humanities.curtin.edu.au/staff.cfm/t.dolin\n[jason ensor]: http://www.humanities.curtin.edu.au/staff.cfm/j.ensor \n\nBooks and Borrowers\n------------------------\nAfter an initial inspection, my first thought was to try to visualise the similarity of the books in the database as measured by the number of borrowers they have in common. \nThe full database contains 99,692 loans of 7,078 different books from 11 libraries by one of the 2,642 people. To make this more manageable, I focused on books that had at least 20 different borrowers and only considered people who had borrowed one of these books.\nThis distilled the database down to a simple table with each row representing one of 1,616 books and each column representing one of 2,473 people. \n\n<table>\n<caption>Table 1: A portion of the book and borrower table. A 1 indicates that the borrower (column)\nborrowed the book (row) at least once. A 0 indicates that the borrower never borrowed the book.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="4" class="title">Borrower ID</th></tr>\n<tr><th>1</th><th>2</th><th>...</th><th>2,473</th></tr>\n<tr><th>1</th><td>1</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>2</th><td>1</td><td>1</td><td>...</td><td>0</td></tr>\n<tr><th>3</th><td>0</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>1</td><td>1</td><td>...</td><td>1</td></tr>\n</table>\n\nConceptually, each cell in the table contains a 1 if the person associated with the cell's column borrowed the book associated with the cell's row. If there was no such loan between a given book and borrower the corresponding cell contains a 0. For example, Table 1 shows that book 2 was borrowed (at least once) by borrower 1 but never by borrower 2,473.\n\nBook Similarity\n-----------------\nThe table view of the books and their borrowers does not readily lend itself to insight. The approach we took to get a better picture of this information was to plot each book as a point on a graph so that similar books are placed closer together than dissimilar books. To do this a notion of what "similar books" is required.\n\nMathematically, row [tex]i[/tex] of Table 1 can be represented as a vector [tex]mathbf{b}_i[/tex] of 1s and 0s. The value of the cell in the [tex]j[/tex]<sup>th</sup> column of that row will be denoted [tex]b_{i,j}[/tex]. For example, the 2<sup>nd</sup> row in the table can be written as the vector [tex]mathbf{b}_2 = (1,1,ldots,0)[/tex] and the value in its first column is [tex]b_{2,1} = 1[/tex].\n\nA crude measure of the similarity between book 1 and book 2 can be computed from this table by counting how many borrowers they have in common. That is, the number of columns that have a `1` in the row for book 1 and the row for book 2. \n \nIn terms of the vector representation, this similarity measure is simply the "[inner product][]" between [tex]mathbf{b}_1[/tex] and [tex]mathbf{b}_2[/tex] and is written [tex]left<mathbf{b}_1,mathbf{b}_2right> = b_{1,1}b_{2,1} + cdots + b_{1,N}b_{2,N}[/tex] where N = 2,473 is the total number of borrowers.\n\n[inner product]: http://en.wikipedia.org/wiki/Inner_product_space\n\nIt turns out that simply counting the number of borrowers two books is not a great measure of similarity. The problem is that two very popular books, each with 100 borrowers, that only share 10% of their borrowers would be considered as similar as two books, each with 10 readers, that share all of their borrowers. An easy way to correct this is to "normalise" the borrower counts by making sure the similarity of a book with itself is always equal to 1. A common way of doing this is by dividing the inner product of two books by the "size" of each of the vectors for those books. \n\nMathematically, we will denote the size of a book vector [tex]mathbf{b}_i[/tex] as [tex]|mathbf{b}_i| = sqrt{left<mathbf{b}_i,mathbf{b}_iright>}[/tex]. The similarity between two books then becomes:\n\n<center>\n[tex]displaystyle\n    text{sim}(mathbf{b}_i,mathbf{b}_j) \n     = frac{left<mathbf{b}_i,mathbf{b}_jright>}{|mathbf{b}_i||mathbf{b}_j|}\n[/tex]\n</center>\n\nPrincipal Component Analysis\n---------------------------------\nNow that we have a similarity measure between books the idea is to create a plot of points -- one per book -- so that similar books are placed close together and dissimilar books are kept far apart. \n\nA standard technique for doing this is [Principal Component Analysis][pca]. Intuitively, this technique aims to find a way of reducing the number of coordinates in each book vector  in such a way that when the similarity between two books is computed using these smaller vectors it is as close as possible to the original similarity. That is, PCA creates a new table that represents books in terms of only two columns.\n\n[pca]: http://en.wikipedia.org/wiki/Principal_components_analysis\n\n<table>\n<caption>Table 2: A portion of the book table after PCA. The values in the two new columns (PCA IDs) can be used to plot the books.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="2" class="title">PCA ID</th></tr>\n<tr>                 <th>1</th><th>2</th></tr>\n<tr><th>1</th><td>-8.2</td><td>2.3</td></tr>\n<tr><th>2</th><td>0.4</td><td>-4.3</td></tr>\n<tr><th>3</th><td>-1.3</td><td>-3.7</td></tr>\n<tr><th>...</th><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>2.2</td><td>-5.6</td></tr>\n</table>\n\nTable 2 gives an example of the book table after PCA that reduces the book vectors (rows) from 2,473 to two entries. The PCA columns cannot be as easily interpreted as the borrowers columns in Table 1 but the values in the columns are such that the similarity of the books in Table 2 are roughly as similar as if the values in Table 1 were used. That is, if [tex]mathbf{c}_1 = (-8.2,2.3)[/tex] and [tex]mathbf{c}_2=(0.4,-4.3)[/tex] are the vectors\nfor the first two rows of Table 2 then [tex]text{sim}(mathbf{c}_1,mathbf{c}_2)[/tex]\nwould be close to [tex]text{sim}(mathbf{b}_1,mathbf{b}_2)[/tex], the similarity of the\nfirst two rows in Table 1.[^1]\n\n[^1]: Technically, the guarantee of the "closeness" of the similarity measures only holds on average, that is, over all possible pairs of books. There is no guarantee any particular pair's\nsimilarity is estimated well.\n\nVisualising the Data\n----------------------\nFigure 1 shows a plot of the PCA reduced book data. Each circle represents one of the 1,616 books, plotted according to the coordinates in a table like Table 2. The size of each circle indicates how many borrowers each book had and its colour indicates which library the book belongs to.[^2]\n\n[^2]: A book can belong to more than one library. In this case one library is chosen at random to determine a circle's colour.\n\n<div class="image">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/06/all_libraries.png" alt="Plot of the books across all libraries in the ACRP database" width="550" class="aligncenter wp-image-43" />\n<p>Figure 1: A PCA plot of all the books in the ACRP database coloured according to which library they belong to. The size of each circle indicates the number of borrowers of the corresponding book.\n</div>\n\nOne immediate observation is that books are clustered according to which library they belong to. This is not too surprising since the books in a library limit what borrowers from that library can read. This means it is likely that two voracious readers that frequent the same library will read the same books. This, in turn, will mean the similarity of two books from a library will be higher than books from different libraries as there are very few borrowers that use more than one library.\n\nDrilling Down and Interacting\n---------------------------------\nTo get a better picture of the data, we decided to focus on books from a single library to avoid this clustering. The library we focused on was the [Lambton][] Miners' and Mechanics' Institute in New South Wales. This library had the largest number of loans (20,253) and so was most likely to have interesting similarity data.\n\n[lambton]: http://en.wikipedia.org/wiki/Lambton,_New_South_Wales\n\nThere are a total of 789 books in the Lambton institute and 469 borrowers of those books. A separate PCA reduction was performed on this restricted part of the database to create a plot of only the Lambton books.\n\nTo make it easier to explore this data, I wrote a simple tool that allows a viewer to interact with the PCA plot. A screenshot from this tool is shown in Figure 2. Once again, larger circles represent books with a larger number of borrowers. \n\nClicking on the figure will open a new window and, after a short delay, the tool will run. The same page can also be accessed from [this link][applet]. \n\n[applet]: /inductio/wp-content/public/acrp/\n\n<div class="image">\n<a href='http://conflate.net/inductio/wp-content/public/acrp/' target="_"><img src="http://conflate.net/inductio/wp-content/uploads/2008/06/acrp.png" alt="Click to open visualisation applet" width="550" class="aligncenter wp-image-41" /></a>\n<p>Figure 2: A screenshot of the ACRP visualisation tool showing books from the Lambton Institute. Click the image to run the tool in a new window.</p>\n</div>\n\nInstructions describing how to use the tool can be found below it. \nIn a nutshell: hovering over a circle will reveal the title of the book corresponding to that circle; clicking on a circle will draw lines to its most similar neighbours; altering the "Borrowers" bar will only show books with at least that many borrowers; and altering the "Similarity" bar will only draw lines to books with at least that proportion of books in common.\n\nFuture Work and Distant Reading\n-------------------------------------\nJulieanne and I are still at the early stages of our research using the ACRP database. The use of PCA for visualisation was a first step in our pursuit of what [Franco Moretti][] calls "distant reading" -- looking at books as objects and how they are read rather than the "close reading" of the text of individual books. \n\n[Franco Moretti]: http://en.wikipedia.org/wiki/Franco_Moretti \n\nNow that we have this tool, we are able to quickly explore relationships between these books based on the reading habits of Australians at the turn of the century. Of course, there are many caveats that apply to any patterns we might see in these plots. For instance, the similarity between books is only based on habits of a small number of readers and will be influenced by the peculiarities of the libraries and the books they choose to buy. For this reason, these plots are not intended to provide conclusive answers to questions we might. \n\nInstead we hope that exploring the ACRP database in this way will lead us to interesting questions about particular pairs or groups of books that can be followed up by a more thorough analysis of their readers, their text as well as other historical and cultural factors about them.\n\nData and Code\n----------------\nFor the technically minded, I have made the code I used to do the visualisation is available on [GitHub][]. It is a combination of [SQL][] for data preprocessing, [R][] for the PCA reduction and [Processing][] for creating the visualisation tool. You will also find a number of images and some notes at the same location.\n\n[github]: http://github.com/mreid/acrp/tree/master \n[SQL]: http://en.wikipedia.org/wiki/SQL\n[R]: http://www.r-project.org/\n[Processing]: http://processing.org/\n\nAccess to the data that the code acts upon is not mine to give, so the code is primarily to show how I did the visualisation rather than a way to let others analyse the data. If the founders of the [ACRP][] project decide to release the data to the public at a later date I will link to it from here.\n\n
169	Visualising 19th Century Reading in Australia	40-revision-3	2008-12-09 12:04:46	2008-12-09 12:04:46	A description of a visualisation of some 19th century Australian borrowing records from the Australian Common Readers Project.	inherit	_Update - 9 Dec 2008_: Julieanne and I presented a much improved version of this visualisation at the [Resourceful Reading][] conference held at the University of Sydney on the 5th of December. Those looking for the application I presented there: stay tuned, I will post the updated version here shortly.\n\n[Resourceful Reading]: http://conferences.arts.usyd.edu.au/index.php?cf=20\n\nI've recently spent a bit of time collaborating with my wife on a research project. Research collaboration by couples is not new but given that Julieanne is a [lecturer in the English program][j] and I'm part of the [computer sciences laboratory][csl], this piece of joint research is a little unusual. \n\nThe rest of this post describes the intersection of our interests --- data from the Australian Common Reader Project --- and the visualisation tool I wrote to explore it. The tool itself is based on a simple application of linear Principal Component Analysis (PCA). I'll attempt to explain it here in such a way that readers who have not studied this technique might still be able to make use of the tool.\n\n[j]: http://cass.anu.edu.au/humanities/school_sites/staff.php\n[csl]: http://csl.cecs.anu.edu.au/\n\nThe Australian Common Reader Project\n--------------------------------------------\nOne of Julieanne's research interests is the Australian audience of the late 19th and early 20th centuries. As part of her PhD, she made use of an amazing database that is part of the [Australian Common Reader Project][acrp] --- a project that has collected and entered library borrowing records from Australian libraries along with annotations about when books were borrowed, their genres, borrower occupations, author information, <i>etc</i>. This sort of information makes it possible for Australian literature and cultural studies academics to ask empirical questions about Australian readers' relationship with books and periodicals. \n\n[acrp]: http://www.api-network.com/hosted/acrp/\n\nEver on the lookout for [interesting data sets][meta-index], I suggested that we apply some basic data analysis tools to the database to see what kind of relationships between books and borrowers we might find. When asked if we could have access to the database, [Tim Dolin][] graciously agreed and enlisted [Jason Ensor][] to help with our technical questions. \n\n[meta-index]: http://conflate.net/inductio/2008/02/a-meta-index-of-data-sets/\n[tim dolin]: http://www.humanities.curtin.edu.au/staff.cfm/t.dolin\n[jason ensor]: http://www.humanities.curtin.edu.au/staff.cfm/j.ensor \n\nBooks and Borrowers\n------------------------\nAfter an initial inspection, my first thought was to try to visualise the similarity of the books in the database as measured by the number of borrowers they have in common. \nThe full database contains 99,692 loans of 7,078 different books from 11 libraries by one of the 2,642 people. To make this more manageable, I focused on books that had at least 20 different borrowers and only considered people who had borrowed one of these books.\nThis distilled the database down to a simple table with each row representing one of 1,616 books and each column representing one of 2,473 people. \n\n<table>\n<caption>Table 1: A portion of the book and borrower table. A 1 indicates that the borrower (column)\nborrowed the book (row) at least once. A 0 indicates that the borrower never borrowed the book.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="4" class="title">Borrower ID</th></tr>\n<tr><th>1</th><th>2</th><th>...</th><th>2,473</th></tr>\n<tr><th>1</th><td>1</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>2</th><td>1</td><td>1</td><td>...</td><td>0</td></tr>\n<tr><th>3</th><td>0</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>1</td><td>1</td><td>...</td><td>1</td></tr>\n</table>\n\nConceptually, each cell in the table contains a 1 if the person associated with the cell's column borrowed the book associated with the cell's row. If there was no such loan between a given book and borrower the corresponding cell contains a 0. For example, Table 1 shows that book 2 was borrowed (at least once) by borrower 1 but never by borrower 2,473.\n\nBook Similarity\n-----------------\nThe table view of the books and their borrowers does not readily lend itself to insight. The approach we took to get a better picture of this information was to plot each book as a point on a graph so that similar books are placed closer together than dissimilar books. To do this a notion of what "similar books" is required.\n\nMathematically, row [tex]i[/tex] of Table 1 can be represented as a vector [tex]mathbf{b}_i[/tex] of 1s and 0s. The value of the cell in the [tex]j[/tex]<sup>th</sup> column of that row will be denoted [tex]b_{i,j}[/tex]. For example, the 2<sup>nd</sup> row in the table can be written as the vector [tex]mathbf{b}_2 = (1,1,ldots,0)[/tex] and the value in its first column is [tex]b_{2,1} = 1[/tex].\n\nA crude measure of the similarity between book 1 and book 2 can be computed from this table by counting how many borrowers they have in common. That is, the number of columns that have a `1` in the row for book 1 and the row for book 2. \n \nIn terms of the vector representation, this similarity measure is simply the "[inner product][]" between [tex]mathbf{b}_1[/tex] and [tex]mathbf{b}_2[/tex] and is written [tex]left<mathbf{b}_1,mathbf{b}_2right> = b_{1,1}b_{2,1} + cdots + b_{1,N}b_{2,N}[/tex] where N = 2,473 is the total number of borrowers.\n\n[inner product]: http://en.wikipedia.org/wiki/Inner_product_space\n\nIt turns out that simply counting the number of borrowers two books is not a great measure of similarity. The problem is that two very popular books, each with 100 borrowers, that only share 10% of their borrowers would be considered as similar as two books, each with 10 readers, that share all of their borrowers. An easy way to correct this is to "normalise" the borrower counts by making sure the similarity of a book with itself is always equal to 1. A common way of doing this is by dividing the inner product of two books by the "size" of each of the vectors for those books. \n\nMathematically, we will denote the size of a book vector [tex]mathbf{b}_i[/tex] as [tex]|mathbf{b}_i| = sqrt{left<mathbf{b}_i,mathbf{b}_iright>}[/tex]. The similarity between two books then becomes:\n\n<center>\n[tex]displaystyle\n    text{sim}(mathbf{b}_i,mathbf{b}_j) \n     = frac{left<mathbf{b}_i,mathbf{b}_jright>}{|mathbf{b}_i||mathbf{b}_j|}\n[/tex]\n</center>\n\nPrincipal Component Analysis\n---------------------------------\nNow that we have a similarity measure between books the idea is to create a plot of points -- one per book -- so that similar books are placed close together and dissimilar books are kept far apart. \n\nA standard technique for doing this is [Principal Component Analysis][pca]. Intuitively, this technique aims to find a way of reducing the number of coordinates in each book vector  in such a way that when the similarity between two books is computed using these smaller vectors it is as close as possible to the original similarity. That is, PCA creates a new table that represents books in terms of only two columns.\n\n[pca]: http://en.wikipedia.org/wiki/Principal_components_analysis\n\n<table>\n<caption>Table 2: A portion of the book table after PCA. The values in the two new columns (PCA IDs) can be used to plot the books.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="2" class="title">PCA ID</th></tr>\n<tr>                 <th>1</th><th>2</th></tr>\n<tr><th>1</th><td>-8.2</td><td>2.3</td></tr>\n<tr><th>2</th><td>0.4</td><td>-4.3</td></tr>\n<tr><th>3</th><td>-1.3</td><td>-3.7</td></tr>\n<tr><th>...</th><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>2.2</td><td>-5.6</td></tr>\n</table>\n\nTable 2 gives an example of the book table after PCA that reduces the book vectors (rows) from 2,473 to two entries. The PCA columns cannot be as easily interpreted as the borrowers columns in Table 1 but the values in the columns are such that the similarity of the books in Table 2 are roughly as similar as if the values in Table 1 were used. That is, if [tex]mathbf{c}_1 = (-8.2,2.3)[/tex] and [tex]mathbf{c}_2=(0.4,-4.3)[/tex] are the vectors\nfor the first two rows of Table 2 then [tex]text{sim}(mathbf{c}_1,mathbf{c}_2)[/tex]\nwould be close to [tex]text{sim}(mathbf{b}_1,mathbf{b}_2)[/tex], the similarity of the\nfirst two rows in Table 1.[^1]\n\n[^1]: Technically, the guarantee of the "closeness" of the similarity measures only holds on average, that is, over all possible pairs of books. There is no guarantee any particular pair's\nsimilarity is estimated well.\n\nVisualising the Data\n----------------------\nFigure 1 shows a plot of the PCA reduced book data. Each circle represents one of the 1,616 books, plotted according to the coordinates in a table like Table 2. The size of each circle indicates how many borrowers each book had and its colour indicates which library the book belongs to.[^2]\n\n[^2]: A book can belong to more than one library. In this case one library is chosen at random to determine a circle's colour.\n\n<div class="image">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/06/all_libraries.png" alt="Plot of the books across all libraries in the ACRP database" width="550" class="aligncenter wp-image-43" />\n<p>Figure 1: A PCA plot of all the books in the ACRP database coloured according to which library they belong to. The size of each circle indicates the number of borrowers of the corresponding book.\n</div>\n\nOne immediate observation is that books are clustered according to which library they belong to. This is not too surprising since the books in a library limit what borrowers from that library can read. This means it is likely that two voracious readers that frequent the same library will read the same books. This, in turn, will mean the similarity of two books from a library will be higher than books from different libraries as there are very few borrowers that use more than one library.\n\nDrilling Down and Interacting\n---------------------------------\nTo get a better picture of the data, we decided to focus on books from a single library to avoid this clustering. The library we focused on was the [Lambton][] Miners' and Mechanics' Institute in New South Wales. This library had the largest number of loans (20,253) and so was most likely to have interesting similarity data.\n\n[lambton]: http://en.wikipedia.org/wiki/Lambton,_New_South_Wales\n\nThere are a total of 789 books in the Lambton institute and 469 borrowers of those books. A separate PCA reduction was performed on this restricted part of the database to create a plot of only the Lambton books.\n\nTo make it easier to explore this data, I wrote a simple tool that allows a viewer to interact with the PCA plot. A screenshot from this tool is shown in Figure 2. Once again, larger circles represent books with a larger number of borrowers. \n\nClicking on the figure will open a new window and, after a short delay, the tool will run. The same page can also be accessed from [this link][applet]. \n\n[applet]: /inductio/wp-content/public/acrp/\n\n<div class="image">\n<a href='http://conflate.net/inductio/wp-content/public/acrp/' target="_"><img src="http://conflate.net/inductio/wp-content/uploads/2008/06/acrp.png" alt="Click to open visualisation applet" width="550" class="aligncenter wp-image-41" /></a>\n<p>Figure 2: A screenshot of the ACRP visualisation tool showing books from the Lambton Institute. Click the image to run the tool in a new window.</p>\n</div>\n\nInstructions describing how to use the tool can be found below it. \nIn a nutshell: hovering over a circle will reveal the title of the book corresponding to that circle; clicking on a circle will draw lines to its most similar neighbours; altering the "Borrowers" bar will only show books with at least that many borrowers; and altering the "Similarity" bar will only draw lines to books with at least that proportion of books in common.\n\nFuture Work and Distant Reading\n-------------------------------------\nJulieanne and I are still at the early stages of our research using the ACRP database. The use of PCA for visualisation was a first step in our pursuit of what [Franco Moretti][] calls "distant reading" -- looking at books as objects and how they are read rather than the "close reading" of the text of individual books. \n\n[Franco Moretti]: http://en.wikipedia.org/wiki/Franco_Moretti \n\nNow that we have this tool, we are able to quickly explore relationships between these books based on the reading habits of Australians at the turn of the century. Of course, there are many caveats that apply to any patterns we might see in these plots. For instance, the similarity between books is only based on habits of a small number of readers and will be influenced by the peculiarities of the libraries and the books they choose to buy. For this reason, these plots are not intended to provide conclusive answers to questions we might. \n\nInstead we hope that exploring the ACRP database in this way will lead us to interesting questions about particular pairs or groups of books that can be followed up by a more thorough analysis of their readers, their text as well as other historical and cultural factors about them.\n\nData and Code\n----------------\nFor the technically minded, I have made the code I used to do the visualisation is available on [GitHub][]. It is a combination of [SQL][] for data preprocessing, [R][] for the PCA reduction and [Processing][] for creating the visualisation tool. You will also find a number of images and some notes at the same location.\n\n[github]: http://github.com/mreid/acrp/tree/master \n[SQL]: http://en.wikipedia.org/wiki/SQL\n[R]: http://www.r-project.org/\n[Processing]: http://processing.org/\n\nAccess to the data that the code acts upon is not mine to give, so the code is primarily to show how I did the visualisation rather than a way to let others analyse the data. If the founders of the [ACRP][] project decide to release the data to the public at a later date I will link to it from here.\n\n
170	Visualising 19th Century Reading in Australia	40-revision-4	2008-12-09 12:05:28	2008-12-09 12:05:28	A description of a visualisation of some 19th century Australian borrowing records from the Australian Common Readers Project.	inherit	----\n_Update - 9 Dec 2008_: Julieanne and I presented a much improved version of this visualisation at the [Resourceful Reading][] conference held at the University of Sydney on the 5th of December. Those looking for the application I presented there: stay tuned, I will post the updated version here shortly.\n----\n[Resourceful Reading]: http://conferences.arts.usyd.edu.au/index.php?cf=20\n\nI've recently spent a bit of time collaborating with my wife on a research project. Research collaboration by couples is not new but given that Julieanne is a [lecturer in the English program][j] and I'm part of the [computer sciences laboratory][csl], this piece of joint research is a little unusual. \n\nThe rest of this post describes the intersection of our interests --- data from the Australian Common Reader Project --- and the visualisation tool I wrote to explore it. The tool itself is based on a simple application of linear Principal Component Analysis (PCA). I'll attempt to explain it here in such a way that readers who have not studied this technique might still be able to make use of the tool.\n\n[j]: http://cass.anu.edu.au/humanities/school_sites/staff.php\n[csl]: http://csl.cecs.anu.edu.au/\n\nThe Australian Common Reader Project\n--------------------------------------------\nOne of Julieanne's research interests is the Australian audience of the late 19th and early 20th centuries. As part of her PhD, she made use of an amazing database that is part of the [Australian Common Reader Project][acrp] --- a project that has collected and entered library borrowing records from Australian libraries along with annotations about when books were borrowed, their genres, borrower occupations, author information, <i>etc</i>. This sort of information makes it possible for Australian literature and cultural studies academics to ask empirical questions about Australian readers' relationship with books and periodicals. \n\n[acrp]: http://www.api-network.com/hosted/acrp/\n\nEver on the lookout for [interesting data sets][meta-index], I suggested that we apply some basic data analysis tools to the database to see what kind of relationships between books and borrowers we might find. When asked if we could have access to the database, [Tim Dolin][] graciously agreed and enlisted [Jason Ensor][] to help with our technical questions. \n\n[meta-index]: http://conflate.net/inductio/2008/02/a-meta-index-of-data-sets/\n[tim dolin]: http://www.humanities.curtin.edu.au/staff.cfm/t.dolin\n[jason ensor]: http://www.humanities.curtin.edu.au/staff.cfm/j.ensor \n\nBooks and Borrowers\n------------------------\nAfter an initial inspection, my first thought was to try to visualise the similarity of the books in the database as measured by the number of borrowers they have in common. \nThe full database contains 99,692 loans of 7,078 different books from 11 libraries by one of the 2,642 people. To make this more manageable, I focused on books that had at least 20 different borrowers and only considered people who had borrowed one of these books.\nThis distilled the database down to a simple table with each row representing one of 1,616 books and each column representing one of 2,473 people. \n\n<table>\n<caption>Table 1: A portion of the book and borrower table. A 1 indicates that the borrower (column)\nborrowed the book (row) at least once. A 0 indicates that the borrower never borrowed the book.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="4" class="title">Borrower ID</th></tr>\n<tr><th>1</th><th>2</th><th>...</th><th>2,473</th></tr>\n<tr><th>1</th><td>1</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>2</th><td>1</td><td>1</td><td>...</td><td>0</td></tr>\n<tr><th>3</th><td>0</td><td>0</td><td>...</td><td>1</td></tr>\n<tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>1</td><td>1</td><td>...</td><td>1</td></tr>\n</table>\n\nConceptually, each cell in the table contains a 1 if the person associated with the cell's column borrowed the book associated with the cell's row. If there was no such loan between a given book and borrower the corresponding cell contains a 0. For example, Table 1 shows that book 2 was borrowed (at least once) by borrower 1 but never by borrower 2,473.\n\nBook Similarity\n-----------------\nThe table view of the books and their borrowers does not readily lend itself to insight. The approach we took to get a better picture of this information was to plot each book as a point on a graph so that similar books are placed closer together than dissimilar books. To do this a notion of what "similar books" is required.\n\nMathematically, row [tex]i[/tex] of Table 1 can be represented as a vector [tex]mathbf{b}_i[/tex] of 1s and 0s. The value of the cell in the [tex]j[/tex]<sup>th</sup> column of that row will be denoted [tex]b_{i,j}[/tex]. For example, the 2<sup>nd</sup> row in the table can be written as the vector [tex]mathbf{b}_2 = (1,1,ldots,0)[/tex] and the value in its first column is [tex]b_{2,1} = 1[/tex].\n\nA crude measure of the similarity between book 1 and book 2 can be computed from this table by counting how many borrowers they have in common. That is, the number of columns that have a `1` in the row for book 1 and the row for book 2. \n \nIn terms of the vector representation, this similarity measure is simply the "[inner product][]" between [tex]mathbf{b}_1[/tex] and [tex]mathbf{b}_2[/tex] and is written [tex]left<mathbf{b}_1,mathbf{b}_2right> = b_{1,1}b_{2,1} + cdots + b_{1,N}b_{2,N}[/tex] where N = 2,473 is the total number of borrowers.\n\n[inner product]: http://en.wikipedia.org/wiki/Inner_product_space\n\nIt turns out that simply counting the number of borrowers two books is not a great measure of similarity. The problem is that two very popular books, each with 100 borrowers, that only share 10% of their borrowers would be considered as similar as two books, each with 10 readers, that share all of their borrowers. An easy way to correct this is to "normalise" the borrower counts by making sure the similarity of a book with itself is always equal to 1. A common way of doing this is by dividing the inner product of two books by the "size" of each of the vectors for those books. \n\nMathematically, we will denote the size of a book vector [tex]mathbf{b}_i[/tex] as [tex]|mathbf{b}_i| = sqrt{left<mathbf{b}_i,mathbf{b}_iright>}[/tex]. The similarity between two books then becomes:\n\n<center>\n[tex]displaystyle\n    text{sim}(mathbf{b}_i,mathbf{b}_j) \n     = frac{left<mathbf{b}_i,mathbf{b}_jright>}{|mathbf{b}_i||mathbf{b}_j|}\n[/tex]\n</center>\n\nPrincipal Component Analysis\n---------------------------------\nNow that we have a similarity measure between books the idea is to create a plot of points -- one per book -- so that similar books are placed close together and dissimilar books are kept far apart. \n\nA standard technique for doing this is [Principal Component Analysis][pca]. Intuitively, this technique aims to find a way of reducing the number of coordinates in each book vector  in such a way that when the similarity between two books is computed using these smaller vectors it is as close as possible to the original similarity. That is, PCA creates a new table that represents books in terms of only two columns.\n\n[pca]: http://en.wikipedia.org/wiki/Principal_components_analysis\n\n<table>\n<caption>Table 2: A portion of the book table after PCA. The values in the two new columns (PCA IDs) can be used to plot the books.\n</caption>\n<tr><th rowspan="2" class="title">Book<br/>ID</th><th colspan="2" class="title">PCA ID</th></tr>\n<tr>                 <th>1</th><th>2</th></tr>\n<tr><th>1</th><td>-8.2</td><td>2.3</td></tr>\n<tr><th>2</th><td>0.4</td><td>-4.3</td></tr>\n<tr><th>3</th><td>-1.3</td><td>-3.7</td></tr>\n<tr><th>...</th><td>...</td><td>...</td></tr>\n<tr><th>1,616</th><td>2.2</td><td>-5.6</td></tr>\n</table>\n\nTable 2 gives an example of the book table after PCA that reduces the book vectors (rows) from 2,473 to two entries. The PCA columns cannot be as easily interpreted as the borrowers columns in Table 1 but the values in the columns are such that the similarity of the books in Table 2 are roughly as similar as if the values in Table 1 were used. That is, if [tex]mathbf{c}_1 = (-8.2,2.3)[/tex] and [tex]mathbf{c}_2=(0.4,-4.3)[/tex] are the vectors\nfor the first two rows of Table 2 then [tex]text{sim}(mathbf{c}_1,mathbf{c}_2)[/tex]\nwould be close to [tex]text{sim}(mathbf{b}_1,mathbf{b}_2)[/tex], the similarity of the\nfirst two rows in Table 1.[^1]\n\n[^1]: Technically, the guarantee of the "closeness" of the similarity measures only holds on average, that is, over all possible pairs of books. There is no guarantee any particular pair's\nsimilarity is estimated well.\n\nVisualising the Data\n----------------------\nFigure 1 shows a plot of the PCA reduced book data. Each circle represents one of the 1,616 books, plotted according to the coordinates in a table like Table 2. The size of each circle indicates how many borrowers each book had and its colour indicates which library the book belongs to.[^2]\n\n[^2]: A book can belong to more than one library. In this case one library is chosen at random to determine a circle's colour.\n\n<div class="image">\n<img src="http://conflate.net/inductio/wp-content/uploads/2008/06/all_libraries.png" alt="Plot of the books across all libraries in the ACRP database" width="550" class="aligncenter wp-image-43" />\n<p>Figure 1: A PCA plot of all the books in the ACRP database coloured according to which library they belong to. The size of each circle indicates the number of borrowers of the corresponding book.\n</div>\n\nOne immediate observation is that books are clustered according to which library they belong to. This is not too surprising since the books in a library limit what borrowers from that library can read. This means it is likely that two voracious readers that frequent the same library will read the same books. This, in turn, will mean the similarity of two books from a library will be higher than books from different libraries as there are very few borrowers that use more than one library.\n\nDrilling Down and Interacting\n---------------------------------\nTo get a better picture of the data, we decided to focus on books from a single library to avoid this clustering. The library we focused on was the [Lambton][] Miners' and Mechanics' Institute in New South Wales. This library had the largest number of loans (20,253) and so was most likely to have interesting similarity data.\n\n[lambton]: http://en.wikipedia.org/wiki/Lambton,_New_South_Wales\n\nThere are a total of 789 books in the Lambton institute and 469 borrowers of those books. A separate PCA reduction was performed on this restricted part of the database to create a plot of only the Lambton books.\n\nTo make it easier to explore this data, I wrote a simple tool that allows a viewer to interact with the PCA plot. A screenshot from this tool is shown in Figure 2. Once again, larger circles represent books with a larger number of borrowers. \n\nClicking on the figure will open a new window and, after a short delay, the tool will run. The same page can also be accessed from [this link][applet]. \n\n[applet]: /inductio/wp-content/public/acrp/\n\n<div class="image">\n<a href='http://conflate.net/inductio/wp-content/public/acrp/' target="_"><img src="http://conflate.net/inductio/wp-content/uploads/2008/06/acrp.png" alt="Click to open visualisation applet" width="550" class="aligncenter wp-image-41" /></a>\n<p>Figure 2: A screenshot of the ACRP visualisation tool showing books from the Lambton Institute. Click the image to run the tool in a new window.</p>\n</div>\n\nInstructions describing how to use the tool can be found below it. \nIn a nutshell: hovering over a circle will reveal the title of the book corresponding to that circle; clicking on a circle will draw lines to its most similar neighbours; altering the "Borrowers" bar will only show books with at least that many borrowers; and altering the "Similarity" bar will only draw lines to books with at least that proportion of books in common.\n\nFuture Work and Distant Reading\n-------------------------------------\nJulieanne and I are still at the early stages of our research using the ACRP database. The use of PCA for visualisation was a first step in our pursuit of what [Franco Moretti][] calls "distant reading" -- looking at books as objects and how they are read rather than the "close reading" of the text of individual books. \n\n[Franco Moretti]: http://en.wikipedia.org/wiki/Franco_Moretti \n\nNow that we have this tool, we are able to quickly explore relationships between these books based on the reading habits of Australians at the turn of the century. Of course, there are many caveats that apply to any patterns we might see in these plots. For instance, the similarity between books is only based on habits of a small number of readers and will be influenced by the peculiarities of the libraries and the books they choose to buy. For this reason, these plots are not intended to provide conclusive answers to questions we might. \n\nInstead we hope that exploring the ACRP database in this way will lead us to interesting questions about particular pairs or groups of books that can be followed up by a more thorough analysis of their readers, their text as well as other historical and cultural factors about them.\n\nData and Code\n----------------\nFor the technically minded, I have made the code I used to do the visualisation is available on [GitHub][]. It is a combination of [SQL][] for data preprocessing, [R][] for the PCA reduction and [Processing][] for creating the visualisation tool. You will also find a number of images and some notes at the same location.\n\n[github]: http://github.com/mreid/acrp/tree/master \n[SQL]: http://en.wikipedia.org/wiki/SQL\n[R]: http://www.r-project.org/\n[Processing]: http://processing.org/\n\nAccess to the data that the code acts upon is not mine to give, so the code is primarily to show how I did the visualisation rather than a way to let others analyse the data. If the founders of the [ACRP][] project decide to release the data to the public at a later date I will link to it from here.\n\n
171	ML and Stats People on Twitter 		2008-12-19 04:11:45	0000-00-00 00:00:00		draft	I started using the social, "micro-blogging" service [Twitter][] in February this year simply because I had been seeing so much commentary about it  both good and bad. Since then, I've posted [800+ updates], amassed over 100 [followers][] and [follow][] nearly that many myself.\n\n[twitter]: http://twitter.com/\n[follow]: http://twitter.com/mdreid/friends\n[followers]: http://twitter.com/mdreid/followers\n\nWhat has surprised me about Twitter is how many people I have found on there who are active, or at least interested, in machine learning and statistics.\n\nCollection of people research in or around statistics and machine learning on twitter:\n\n@arthegall\n@mja\n@nealrichter\n@dwf\n@brendan642\n@dtunkelang\n@ealdent\n@mikiobraun\n@lemire\n@SoloGen\n@markusweimer\n@pongba\n@moorejh\n@peteskomoroch\n@smolix\n@DataJunkie\n@filterfish\n@ansate
172	ML and Stats People on Twitter 	171-revision	2008-12-17 21:45:40	2008-12-17 21:45:40		inherit	Collection of people research in or around statistics and machine learning on twitter:\n\n@arthegall\n@mja\n@nealrichter\n@dwf\n@brendan642\n@dtunkelang\n@ealdent\n@mikiobraun\n@lemire\n@SoloGen\n@markusweimer\n@pongba\n@moorejh\n@peteskomoroch\n@smolix\n@
173	ML and Stats People on Twitter 	171-revision-2	2008-12-19 02:21:26	2008-12-19 02:21:26		inherit	I started using the social, "micro-blogging" service [Twitter][] in February this year simply because I had been seeing so much commentary about it  both good and bad. Since then, I've posted [800+ updates], amassed over 100 followers and follow \n\n[twitter]: http://twitter.com/\n\nCollection of people research in or around statistics and machine learning on twitter:\n\n@arthegall\n@mja\n@nealrichter\n@dwf\n@brendan642\n@dtunkelang\n@ealdent\n@mikiobraun\n@lemire\n@SoloGen\n@markusweimer\n@pongba\n@moorejh\n@peteskomoroch\n@smolix\n@DataJunkie\n@filterfish\n@ansate
174	ML and Stats People on Twitter 	171-revision-3	2008-12-19 02:55:29	2008-12-19 02:55:29		inherit	I started using the social, "micro-blogging" service [Twitter][] in February this year simply because I had been seeing so much commentary about it  both good and bad. Since then, I've posted [800+ updates], amassed over 100 [followers][] and [follow][] nearly that many myself.\n\n[twitter]: http://twitter.com/\n[follow]: http://twitter.com/mdreid/friends\n[followers]: http://twitter.com/mdreid/followers\n\nWhat has surprised me about Twitter is how many people I have found on there who are active, or at least interested, in machine learning and statistics.\n\nCollection of people research in or around statistics and machine learning on twitter:\n\n@arthegall\n@mja\n@nealrichter\n@dwf\n@brendan642\n@dtunkelang\n@ealdent\n@mikiobraun\n@lemire\n@SoloGen\n@markusweimer\n@pongba\n@moorejh\n@peteskomoroch\n@smolix\n@DataJunkie\n@filterfish\n@ansate
